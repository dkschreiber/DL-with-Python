<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>14</title>
    
  <link href="../../stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="../../page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
  <h1 class="tochead" id="heading_id_2"><a id="pgfId-998407"></a><a id="pgfId-1040896"></a>14 Conclusions</h1>

  <p class="co-summary-head"><a id="pgfId-1011754"></a>This chapter covers</p>

  <ul class="calibre10">
    <li class="co-summary-bullet"><a class="calibre11" id="pgfId-1011760"></a>Important takeaways from this book</li>

    <li class="co-summary-bullet"><a class="calibre11" id="pgfId-1011774"></a>The limitations of deep learning</li>

    <li class="co-summary-bullet"><a class="calibre11" id="pgfId-1011784"></a>Possible future directions for deep learning, machine learning, and AI</li>

    <li class="co-summary-bullet"><a class="calibre11" id="pgfId-1011794"></a>Resources for further learning and applying your skills in practice</li>
  </ul>

  <p class="body"><a id="pgfId-1011804"></a>You’ve almost reached the end of this book. This last chapter will summarize and review core concepts while also expanding your horizons beyond what you’ve learned so far. Becoming an effective AI practitioner is a journey, and finishing this book is merely your first step on it. I want to make sure you realize this and are properly equipped to take the next steps of this journey on your own.</p>

  <p class="body"><a id="pgfId-1011826"></a>We’ll start with a bird’s-eye view of what you should take away from this book. This should refresh your memory regarding some of the concepts you’ve learned. Next, I’ll present an overview of some key limitations of deep learning. To use a tool appropriately, you should not only understand what it <i class="fm-italics">can</i> do but also be aware of what it <i class="fm-italics">can’t</i> do. Finally, I’ll offer some speculative thoughts about the future evolution of deep learning, machine learning, and AI. This should be especially interesting to you if you’d like to get into fundamental research. The chapter ends with a short list of resources and strategies for further learning about machine learning and staying up to date with new advances.</p>

  <h2 class="fm-head" id="heading_id_3"><a id="pgfId-1011835"></a>14.1 Key concepts in review</h2>

  <p class="body"><a id="pgfId-1011845"></a>This section briefly synthesizes key takeaways from this book. If you ever need a quick refresher to help you recall what you’ve learned, you can read these few pages.</p>

  <h3 class="fm-head1" id="heading_id_4"><a id="pgfId-1011854"></a>14.1.1 Various approaches to AI</h3>

  <p class="body"><a id="pgfId-1011864"></a><a id="marker-1011865"></a>First of all, deep learning isn’t synonymous with AI, or even with machine learning:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1011873"></a><i class="fm-italics1">Artificial intelligence</i> (AI) is an <a class="calibre11" id="marker-1011890"></a>ancient, broad field that can generally be understood as “all attempts to automate human cognitive processes.” This can range from the very basic, such as an Excel spreadsheet, to the very advanced, like a humanoid robot that can walk and talk.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1011936"></a><i class="fm-italics1">Machine learning</i> is a specific <a class="calibre11" id="marker-1011913"></a>subfield of AI that aims at automatically developing programs (called <i class="fm-italics1">models</i>) purely from exposure to training data. This process of turning data into a program is called <i class="fm-italics1">learning</i>. Although machine learning has been around for a long time, it only started to take off in the 1990s, before becoming the dominant form of AI in the 2000s.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012033"></a><i class="fm-italics1">Deep learning</i> is one of many branches of machine learning, where the models are long chains of geometric transformations, applied one after the other. These operations are structured into modules <a class="calibre11" id="marker-1011964"></a>called <i class="fm-italics1">layers</i>: deep learning models are typically stacks of layers—or, more generally, graphs of layers. These layers are parameterized by <i class="fm-italics1">weights</i>, which are <a class="calibre11" id="marker-1011990"></a>the parameters learned during training. The <i class="fm-italics1">knowledge</i> of a model is stored in its weights, and the process of learning consists of finding “good values” for these weights—values that minimize a <i class="fm-italics1">loss function</i>. Because the <a class="calibre11" id="marker-1012016"></a>chain of geometric transformations considered is differentiable, updating the weights to minimize the loss function is done efficiently <a class="calibre11" id="marker-1012022"></a>via <i class="fm-italics1">gradient descent</i>.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1012042"></a>Even though deep learning is just one among many approaches to machine learning, it isn’t on an equal footing with the others. Deep learning is a breakout success. Here’s why. <a id="marker-1012044"></a></p>

  <h3 class="fm-head1" id="heading_id_5"><a id="pgfId-1012051"></a>14.1.2 What makes deep learning special within the field of machine learning</h3>

  <p class="body"><a id="pgfId-1012061"></a>In the span of only a few years, deep learning has achieved tremendous breakthroughs across a wide range of tasks that have been historically perceived as extremely difficult for computers, especially in the area of machine perception: extracting useful information from images, videos, sound, and more. Given sufficient training data (in particular, training data appropriately labeled by humans), deep learning makes it possible to extract from perceptual data almost anything a human could. Hence, it’s sometimes said that deep learning has “solved perception”—although that’s true only for a fairly narrow definition of perception.</p>

  <p class="body"><a id="pgfId-1012070"></a>Due to its unprecedented technical successes, deep learning has singlehandedly brought about the third and by far the largest <i class="fm-italics">AI summer</i>: a period <a id="marker-1012081"></a>of intense interest, investment, and hype in the field of AI. As this book is being written, we’re in the middle of it. Whether this period will end in the near future, and what happens after it ends, are topics of debate. One thing is certain: in stark contrast with previous AI summers, deep learning has provided enormous business value to both large and small technology companies, enabling human-level speech recognition, smart assistants, human-level image classification, vastly improved machine translation, and more. The hype may (and likely will) recede, but the sustained economic and technological impact of deep learning will remain. In that sense, deep learning could be analogous to the internet: it may be overly hyped up for a few years, but in the longer term it will still be a major revolution that will transform our economy and our lives.</p>

  <p class="body"><a id="pgfId-1012091"></a>I’m particularly optimistic about deep learning, because even if we were to make no further technological progress in the next decade, deploying existing algorithms to every applicable problem would be a game changer for most industries. Deep learning is nothing short of a revolution, and progress is currently happening at an incredibly fast rate, due to an exponential investment in resources and headcount. From where I stand, the future looks bright, although short-term expectations are somewhat overoptimistic; deploying deep learning to the full extent of its potential will likely take multiple decades.</p>

  <h3 class="fm-head1" id="heading_id_6"><a id="pgfId-1012100"></a>14.1.3 How to think about deep learning</h3>

  <p class="body"><a id="pgfId-1012110"></a>The most surprising thing about deep learning is how simple it is. Ten years ago, no one expected that we would achieve such amazing results on machine-perception problems by using simple parametric models trained with gradient descent. Now, it turns out that all you need is sufficiently large parametric models trained with gradient descent on sufficiently many examples. As Feynman once said about the universe, “It’s not complicated, it’s just a lot of it.”<a id="Id-1012116"></a><a href="../Text/14.htm#pgfId-1012116"><sup class="footnotenumber">1</sup></a></p>

  <p class="body"><a id="pgfId-1012171"></a>In deep learning, everything is a vector—that is to say, everything is a <i class="fm-italics">point</i> in a <i class="fm-italics">geometric space</i>. Model inputs (text, images, and so on) and targets are first <i class="fm-italics">vectorized</i>—turned into <a id="marker-1012160"></a>an initial input vector space and target vector space. Each layer in a deep learning model operates one simple geometric transformation on the data that goes through it. Together, the chain of layers in the model forms one complex geometric transformation, broken down into a series of simple ones. This complex transformation attempts to map the input space to the target space, one point at a time. This transformation is parameterized by the weights of the layers, which are iteratively updated based on how well the model is currently performing. A key characteristic of this geometric transformation is that it must be <i class="fm-italics">differentiable</i>, which is required in order for us to be able to learn its parameters via gradient descent. Intuitively, this means the geometric morphing from inputs to outputs must be smooth and continuous—a significant constraint.</p>

  <p class="body"><a id="pgfId-1012180"></a>The entire process of applying this complex geometric transformation to the input data can be visualized in 3D by imagining a person trying to uncrumple a paper ball: the crumpled paper ball is the manifold of the input data that the model starts with. Each movement operated by the person on the paper ball is similar to a simple geometric transformation operated by one layer. The full uncrumpling gesture sequence is the complex transformation of the entire model. Deep learning models are mathematical machines for uncrumpling complicated manifolds of high-dimensional data.</p>

  <p class="body"><a id="pgfId-1012186"></a>That’s the magic of deep learning: turning meaning into vectors, then into geometric spaces, and then incrementally learning complex geometric transformations that map one space to another. All you need are spaces of sufficiently high dimensionality in order to capture the full scope of the relationships found in the original data.</p>

  <p class="body"><a id="pgfId-1012274"></a>The whole process hinges on a single core idea: <i class="fm-italics">that meaning is derived from the pairwise relationship between things</i> (between words in a language, between pixels in an image, and so on) and that <i class="fm-italics">these relationships can be captured by a distance function</i>. But note that whether the brain also implements meaning via geometric spaces is an entirely separate question. Vector spaces are efficient to work with from a computational standpoint, but different data structures for intelligence can easily be envisioned—in particular, graphs. Neural networks initially emerged from the idea of using graphs as a way to encode meaning, which is why they’re named <i class="fm-italics">neural networks</i>; the surrounding <a id="marker-1012223"></a>field of research used to be called <i class="fm-italics">connectionism</i>. Nowadays the name “neural network” exists purely for historical reasons—it’s an extremely misleading name because they’re neither neural nor networks. In particular, neural networks have hardly anything to do with the brain. A more appropriate name would have been <i class="fm-italics">layered representations learning</i> or <i class="fm-italics">hierarchical representations learning</i>, or maybe even deep <i class="fm-italics">differentiable models</i> or <i class="fm-italics">chained geometric transforms</i>, to emphasize the fact that continuous geometric space manipulation is at their core.</p>

  <h3 class="fm-head1" id="heading_id_7"><a id="pgfId-1012286"></a>14.1.4 Key enabling technologies</h3>

  <p class="body"><a id="pgfId-1012296"></a>The technological revolution that’s currently unfolding didn’t start with any single breakthrough invention. Rather, like any other revolution, it’s the product of a vast accumulation of enabling factors—gradual at first, and then sudden. In the case of deep learning, we can point out the following key factors:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012305"></a><i class="fm-italics1">Incremental algorithmic innovations</i>—These first began appearing slowly over the span of two decades (starting with backpropagation), and then were developed increasingly faster as more research effort was poured into deep learning after 2012.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012326"></a><i class="fm-italics1">The availability of large amounts of perceptual data</i>—This was a requirement in order to realize that sufficiently large models trained on sufficiently large data are all we need. This is, in turn, a byproduct of the rise of the consumer internet and Moore’s law applied to storage media.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012343"></a><i class="fm-italics1">The availability of fast, highly parallel computation hardware at a low price</i>—Especially the GPUs produced by NVIDIA—first gaming GPUs and then chips designed from the ground up for deep learning. Early on, NVIDIA CEO Jensen Huang took note of the deep learning boom and decided to bet the company’s future on it, which paid off in a big way.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012360"></a><i class="fm-italics1">A complex stack of software layers that makes this computational power available to humans</i>—The CUDA language, frameworks like TensorFlow that do automatic differentiation, and Keras, which makes deep learning accessible to most people.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1012377"></a>In the future, deep learning will not be used only by specialists—researchers, graduate students, and engineers with an academic profile—it will be a tool in the toolbox of every developer, much like web technology today. Everyone needs to build intelligent apps: just as every business today needs a website, every product will need to intelligently make sense of user-generated data. Bringing about this future will require us to build tools that make deep learning radically easy to use and accessible to anyone with basic coding abilities. Keras has been the first major step in that direction.</p>

  <h3 class="fm-head1" id="heading_id_8"><a id="pgfId-1012386"></a>14.1.5 The universal machine learning workflow</h3>

  <p class="body"><a id="pgfId-1012396"></a><a id="marker-1012397"></a>Having access to an extremely powerful tool for creating models that map any input space to any target space is great, but the difficult part of the machine learning workflow is often everything that comes before designing and training such models (and, for production models, what comes after, as well). Understanding the problem domain so as to be able to determine what to attempt to predict, given what data, and how to measure success, is a prerequisite for any successful application of machine learning, and it isn’t something that advanced tools like Keras and TensorFlow can help you with. As a reminder, here’s a quick summary of the typical machine learning workflow as described in chapter 6:</p>

  <ol class="calibre14">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012405"></a>Define the problem: What data is available, and what are you trying to predict? Will you need to collect more data or hire people to manually label a dataset?</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012419"></a>Identify a way to reliably measure success on your goal. For simple tasks this may be prediction accuracy, but in many cases it will require sophisticated, domain-specific metrics.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012429"></a>Prepare the validation process that you’ll use to evaluate your models. In particular, you should define a training set, a validation set, and a test set. The validation- and test-set labels shouldn’t leak into the training data: for instance, with temporal prediction, the validation and test data should be posterior to the training data.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012439"></a>Vectorize the data by turning it into vectors and preprocessing it in a way that makes it more easily approachable by a neural network (normalization and so on).</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012449"></a>Develop a first model that beats a trivial common-sense baseline, thus demonstrating that machine learning can work on your problem. This may not always be the case!</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012459"></a>Gradually refine your model architecture by tuning hyperparameters and adding regularization. Make changes based on performance on the validation data only, not the test data or the training data. Remember that you should get your model to overfit (thus identifying a model capacity level that’s greater than you need) and only then begin to add regularization or downsize your model. Beware of validation-set overfitting when tuning hyperparameters—the fact that your hyperparameters may end up being overspecialized to the validation set. Avoiding this is the purpose of having a separate test set.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012469"></a>Deploy your final model in production—as a web API, as part of a JavaScript or C++ application, on an embedded device, etc. Keep monitoring its performance on real-world data, and use your findings to refine the next iteration of the model!<a class="calibre11" id="marker-1012475"></a></p>
    </li>
  </ol>

  <h3 class="fm-head1" id="heading_id_9"><a id="pgfId-1012482"></a>14.1.6 Key network architectures</h3>

  <p class="body"><a id="pgfId-1012570"></a>The four families of network architectures that you should be familiar <a id="marker-1012497"></a>with <a id="marker-1012503"></a>are <i class="fm-italics">densely connected networks</i>, <i class="fm-italics">convolutional networks</i>, <i class="fm-italics">recurrent networks</i>, and <i class="fm-italics">Transformers</i>. Each type <a id="marker-1012549"></a>of model is meant for a specific input modality. A network architecture encodes <i class="fm-italics">assumptions</i> about the structure of the data: a <i class="fm-italics">hypothesis space</i> within which <a id="marker-1012575"></a>the search for a good model will proceed. Whether a given architecture will work on a given problem depends entirely on the match between the structure of the data and the assumptions of the network architecture.</p>

  <p class="body"><a id="pgfId-1012585"></a>These different network types can easily be combined to achieve larger multimodal models, much as you combine LEGO bricks. In a way, deep learning layers are LEGO bricks for information processing. Here’s a quick overview of the mapping between input modalities and appropriate network architectures:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012613"></a><i class="fm-italics1">Vector data</i>—Densely connected models (<code class="fm-code-in-text">Dense</code> layers).</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012622"></a><i class="fm-italics1">Image data</i>—2D convnets.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012639"></a><i class="fm-italics1">Sequence data</i>—RNNs for timeseries, or Transformers for discrete sequences (such as sequences of words). 1D convnets can also be used for translation-invariant, continuous sequence data, such as birdsong waveforms.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012656"></a><i class="fm-italics1">Video data</i>—Either 3D convnets (if you need to capture motion effects), or a combination of a frame-level 2D convnet for feature extraction followed by a sequence-processing model.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012673"></a><i class="fm-italics1">Volumetric data</i>—3D convnets.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1012690"></a>Now, let’s quickly review the specificities of each network architecture.</p>

  <p class="fm-head2"><a id="pgfId-1012696"></a>Densely connected networks</p>

  <p class="body"><a id="pgfId-1012755"></a><a id="marker-1012707"></a>A densely connected network is a stack of <code class="fm-code-in-text">Dense</code> layers meant <a id="marker-1012724"></a>to process vector data (where each sample is a vector of numerical or categorical attributes). Such networks assume no specific structure in the input features: they’re called <i class="fm-italics">densely connected</i> because the units of a <code class="fm-code-in-text">Dense</code> layer are connected to every other unit. The layer attempts to map relationships between any two input features; this is unlike a 2D convolution layer, for instance, which only looks at <i class="fm-italics">local</i> relationships.</p>

  <p class="body"><a id="pgfId-1012764"></a>Densely connected networks are most commonly used for categorical data (for example, where the input features are lists of attributes), such as the Boston Housing Price dataset used in chapter 4. They’re also used as the final classification or regression stage of most networks. For instance, the convnets covered in chapter 8 typically end with one or two <code class="fm-code-in-text">Dense</code> layers, and so do the recurrent networks in chapter 10.</p>

  <p class="body"><a id="pgfId-1012827"></a>Remember, to perform <i class="fm-italics">binary classification</i>, end your <a id="marker-1012790"></a>stack of layers with a <code class="fm-code-in-text">Dense</code> layer with a single unit <a id="marker-1012806"></a>and a <code class="fm-code-in-text">sigmoid</code> activation, and use <code class="fm-code-in-text">binary_crossentropy</code> as the <a id="marker-1012832"></a>loss. Your targets should be either 0 or 1:</p>
  <pre class="programlisting"><a id="pgfId-1056849"></a><b class="fm-codebrown">from</b> tensorflow <b class="fm-codebrown">import</b> keras 
<a id="pgfId-1056850"></a><b class="fm-codebrown">from</b> tensorflow.keras <b class="fm-codebrown">import</b> layers
<a id="pgfId-1056851"></a>inputs = keras.Input(shape=(num_input_features,))
<a id="pgfId-1056852"></a>x = layers.Dense(<span class="fm-codeblue">32</span>, activation=<span class="fm-codegreen">"relu"</span>)(inputs)
<a id="pgfId-1056853"></a>x = layers.Dense(<span class="fm-codeblue">32</span>, activation=<span class="fm-codegreen">"relu"</span>)(x)
<a id="pgfId-1056854"></a>outputs = layers.Dense(<span class="fm-codeblue">1</span>, activation=<span class="fm-codegreen">"sigmoid"</span>)(x)
<a id="pgfId-1056855"></a>model = keras.Model(inputs, outputs)
<a id="pgfId-1012892"></a>model.compile(optimizer=<span class="fm-codegreen">"rmsprop"</span>, loss=<span class="fm-codegreen">"binary_crossentropy"</span>)</pre>

  <p class="body"><a id="pgfId-1012962"></a>To perform <i class="fm-italics">single-label categorical classification</i> (where each <a id="marker-1012909"></a>sample has exactly one class, no more), end your stack of layers with a <code class="fm-code-in-text">Dense</code> layer with a number of units equal to the number of <a id="marker-1012925"></a>classes, and a <code class="fm-code-in-text">softmax</code> activation. If your targets are one-hot encoded, use <code class="fm-code-in-text">categorical_crossentropy</code> as the <a id="marker-1012951"></a>loss; if they’re integers, use <code class="fm-code-in-text">sparse_categorical_ crossentropy</code>:</p>
  <pre class="programlisting"><a id="pgfId-1056872"></a>inputs = keras.Input(shape=(num_input_features,))
<a id="pgfId-1056873"></a>x = layers.Dense(<span class="fm-codeblue">32</span>, activation=<span class="fm-codegreen">"relu"</span>)(inputs)
<a id="pgfId-1056874"></a>x = layers.Dense(<span class="fm-codeblue">32</span>, activation=<span class="fm-codegreen">"relu"</span>)(x)
<a id="pgfId-1056875"></a>outputs = layers.Dense(num_classes, activation=<span class="fm-codegreen">"softmax"</span>)(x)
<a id="pgfId-1056876"></a>model = keras.Model(inputs, outputs)
<a id="pgfId-1013009"></a>model.compile(optimizer=<span class="fm-codegreen">"rmsprop"</span>, loss=<span class="fm-codegreen">"categorical_crossentropy"</span>)</pre>

  <p class="body"><a id="pgfId-1013057"></a>To perform <i class="fm-italics">multilabel categorical classification</i> (where each <a id="marker-1013026"></a>sample can have several classes), end your stack of layers with a <code class="fm-code-in-text">Dense</code> layer with a number of units equal to the number of classes, and a <code class="fm-code-in-text">sigmoid</code> activation, and use <code class="fm-code-in-text">binary_crossentropy</code> as the loss. Your targets should be multi-hot encoded:</p>
  <pre class="programlisting"><a id="pgfId-1056889"></a>inputs = keras.Input(shape=(num_input_features,))
<a id="pgfId-1056890"></a>x = layers.Dense(<span class="fm-codeblue">32</span>, activation=<span class="fm-codegreen">"relu"</span>)(inputs)
<a id="pgfId-1056891"></a>x = layers.Dense(<span class="fm-codeblue">32</span>, activation=<span class="fm-codegreen">"relu"</span>)(x)
<a id="pgfId-1056892"></a>outputs = layers.Dense(num_classes, activation=<span class="fm-codegreen">"sigmoid"</span>)(x)
<a id="pgfId-1056893"></a>model = keras.Model(inputs, outputs)
<a id="pgfId-1013104"></a>model.compile(optimizer=<span class="fm-codegreen">"rmsprop"</span>, loss=<span class="fm-codegreen">"binary_crossentropy"</span>)</pre>

  <p class="body"><a id="pgfId-1013148"></a>To perform <i class="fm-italics">regression</i> toward a <a id="marker-1013121"></a>vector of continuous values, end your stack of layers with a <code class="fm-code-in-text">Dense</code> layer with a number of units equal to the number of values you’re trying to predict (often a single one, such as the price of a house), and no activation. Various losses can be used for <a id="marker-1013137"></a>regression—most commonly <code class="fm-code-in-text">mean_squared_error</code> (MSE):<a id="marker-1013153"></a></p>
  <pre class="programlisting"><a id="pgfId-1056947"></a>inputs = keras.Input(shape=(num_input_features,))
<a id="pgfId-1056948"></a>x = layers.Dense(<span class="fm-codeblue">32</span>, activation=<span class="fm-codegreen">"relu"</span>)(inputs)
<a id="pgfId-1056949"></a>x = layers.Dense(<span class="fm-codeblue">32</span>, activation=<span class="fm-codegreen">"relu"</span>)(x)
<a id="pgfId-1056950"></a>outputs layers.Dense(num_values)(x)
<a id="pgfId-1056951"></a>model = keras.Model(inputs, outputs)
<a id="pgfId-1013200"></a>model.compile(optimizer=<span class="fm-codegreen">"rmsprop"</span>, loss=<span class="fm-codegreen">"mse"</span>)</pre>

  <p class="fm-head2"><a id="pgfId-1013206"></a>Convnets</p>

  <p class="body"><a id="pgfId-1013319"></a><a id="marker-1013217"></a>Convolution layers look at spatially local patterns by applying the same geometric transformation to different spatial locations (<i class="fm-italics">patches</i>) in an input tensor. This results in representations that are <i class="fm-italics">translation invariant</i>, making convolution layers highly data efficient and modular. This idea is applicable to spaces of any dimensionality: 1D (continuous sequences), 2D (images), 3D (volumes), and so on. You can use the <code class="fm-code-in-text">Conv1D</code> layer to <a id="marker-1013254"></a>process sequences, the <code class="fm-code-in-text">Conv2D</code> layer to <a id="marker-1013270"></a>process images, and the <code class="fm-code-in-text">Conv3D</code> layers to <a id="marker-1013286"></a>process volumes. As a leaner, more efficient alternative to convolution layers, you can <a id="marker-1013292"></a>also <a id="marker-1013298"></a>use <i class="fm-italics">depthwise separable convolution</i> layers, such as <code class="fm-code-in-text">SeparableConv2D</code>.</p>

  <p class="body"><a id="pgfId-1013368"></a><i class="fm-italics">Convnets</i>, or <i class="fm-italics">convolutional networks</i>, consist of stacks of convolution and max-pooling layers. The pooling layers let you spatially downsample the data, which is required to keep feature maps to a reasonable size as the number of features grows, and to allow subsequent convolution layers to “see” a greater spatial extent of the inputs. Convnets are often ended with either a <code class="fm-code-in-text">Flatten</code> operation or <a id="marker-1013357"></a>a global pooling layer, turning spatial feature maps into vectors, followed by <code class="fm-code-in-text">Dense</code> layers to achieve classification or regression.</p>

  <p class="body"><a id="pgfId-1013377"></a>Here’s a typical image-classification network (categorical classification, in this case), leveraging <code class="fm-code-in-text">SeparableConv2D</code> layers:</p>
  <pre class="programlisting"><a id="pgfId-1056970"></a>inputs = keras.Input(shape=(height, width, channels))
<a id="pgfId-1056971"></a>x = layers.SeparableConv2D(<span class="fm-codeblue">32</span>, <span class="fm-codeblue">3</span>, activation=<span class="fm-codegreen">"relu"</span>)(inputs)
<a id="pgfId-1056972"></a>x = layers.SeparableConv2D(<span class="fm-codeblue">64</span>, <span class="fm-codeblue">3</span>, activation=<span class="fm-codegreen">"relu"</span>)(x)
<a id="pgfId-1056973"></a>x = layers.MaxPooling2D(<span class="fm-codeblue">2</span>)(x)
<a id="pgfId-1056974"></a>x = layers.SeparableConv2D(<span class="fm-codeblue">64</span>, <span class="fm-codeblue">3</span>, activation=<span class="fm-codegreen">"relu"</span>)(x)
<a id="pgfId-1056975"></a>x = layers.SeparableConv2D(<span class="fm-codeblue">128</span>, <span class="fm-codeblue">3</span>, activation=<span class="fm-codegreen">"relu"</span>)(x)
<a id="pgfId-1056976"></a>x = layers.MaxPooling2D(<span class="fm-codeblue">2</span>)(x)
<a id="pgfId-1056977"></a>x = layers.SeparableConv2D(<span class="fm-codeblue">64</span>, <span class="fm-codeblue">3</span>, activation=<span class="fm-codegreen">"relu"</span>)(x)
<a id="pgfId-1056978"></a>x = layers.SeparableConv2D(<span class="fm-codeblue">128</span>, <span class="fm-codeblue">3</span>, activation=<span class="fm-codegreen">"relu"</span>)(x)
<a id="pgfId-1056979"></a>x = layers.GlobalAveragePooling2D()(x)
<a id="pgfId-1056980"></a>x = layers.Dense(<span class="fm-codeblue">32</span>, activation=<span class="fm-codegreen">"relu"</span>)(x)
<a id="pgfId-1056981"></a>outputs = layers.Dense(num_classes, activation=<span class="fm-codegreen">"softmax"</span>)(x)
<a id="pgfId-1056982"></a>model = keras.Model(inputs, outputs)
<a id="pgfId-1013478"></a>model.compile(optimizer=<span class="fm-codegreen">"rmsprop"</span>, loss=<span class="fm-codegreen">"categorical_crossentropy"</span>)</pre>

  <p class="body"><a id="pgfId-1013506"></a>When building a very deep convnet, it’s common to add <i class="fm-italics">batch normalization</i> layers as <a id="marker-1013495"></a>well as <i class="fm-italics">residual connections</i>—two architecture <a id="marker-1013511"></a>patterns that help gradient information flow smoothly through the network. <a id="marker-1013517"></a></p>

  <p class="fm-head2"><a id="pgfId-1013526"></a>RNNs</p>

  <p class="body"><a id="pgfId-1013545"></a><a id="marker-1013537"></a><i class="fm-italics">Recurrent neural networks</i> (RNNs) work by processing sequences of inputs one timestep at a time, and maintaining a state throughout (a state is typically a vector or set of vectors). They should be used preferentially over 1D convnets in the case of sequences where patterns of interest aren’t invariant by temporal translation (for instance, timeseries data where the recent past is more important than the distant past).</p>

  <p class="body"><a id="pgfId-1013639"></a>Three RNN layers are <a id="marker-1013556"></a>available <a id="marker-1013562"></a>in Keras: <code class="fm-code-in-text">SimpleRNN</code>, <code class="fm-code-in-text">GRU</code>, and <code class="fm-code-in-text">LSTM</code>. For most <a id="marker-1013598"></a>practical purposes, you should use either <code class="fm-code-in-text">GRU</code> or <code class="fm-code-in-text">LSTM</code>. <code class="fm-code-in-text">LSTM</code> is the more powerful of the two but is also more expensive; you can think of <code class="fm-code-in-text">GRU</code> as a simpler, cheaper alternative to it.</p>

  <p class="body"><a id="pgfId-1013648"></a>In order to stack multiple RNN layers on top of each other, each layer prior to the last layer in the stack should return the full sequence of its outputs (each input timestep will correspond to an output timestep). If you aren’t stacking any further RNN layers, it’s common to return only the last output, which contains information about the entire sequence.</p>

  <p class="body"><a id="pgfId-1013654"></a>Following is a single RNN layer for binary classification of vector sequences:</p>
  <pre class="programlisting"><a id="pgfId-1057008"></a>inputs = keras.Input(shape=(num_timesteps, num_features))
<a id="pgfId-1057009"></a>x = layers.LSTM(<span class="fm-codeblue">32</span>)(inputs)
<a id="pgfId-1057010"></a>outputs = layers.Dense(num_classes, activation=<span class="fm-codegreen">"sigmoid"</span>)(x)
<a id="pgfId-1057011"></a>model = keras.Model(inputs, outputs)
<a id="pgfId-1013692"></a>model.compile(optimizer=<span class="fm-codegreen">"rmsprop"</span>, loss=<span class="fm-codegreen">"binary_crossentropy"</span>)</pre>

  <p class="body"><a id="pgfId-1013698"></a>And this is a stacked RNN for binary classification of vector sequences:<a id="marker-1013700"></a></p>
  <pre class="programlisting"><a id="pgfId-1057028"></a>inputs = keras.Input(shape=(num_timesteps, num_features))
<a id="pgfId-1057029"></a>x = layers.LSTM(<span class="fm-codeblue">32</span>, return_sequences=<code class="fm-codegreen">True</code>)(inputs)
<a id="pgfId-1057030"></a>x = layers.LSTM(<span class="fm-codeblue">32</span>, return_sequences=<code class="fm-codegreen">True</code>)(x)
<a id="pgfId-1057031"></a>x = layers.LSTM(<span class="fm-codeblue">32</span>)(x)
<a id="pgfId-1057032"></a>outputs = layers.Dense(num_classes, activation=<span class="fm-codegreen">"sigmoid"</span>)(x)
<a id="pgfId-1057033"></a>model = keras.Model(inputs, outputs)
<a id="pgfId-1013753"></a>model.compile(optimizer=<span class="fm-codegreen">"rmsprop"</span>, loss=<span class="fm-codegreen">"binary_crossentropy"</span>)</pre>

  <p class="fm-head2"><a id="pgfId-1013759"></a>Transformers</p>

  <p class="body"><a id="pgfId-1013802"></a><a id="marker-1013770"></a>A Transformer looks at a set of vectors (such as word vectors), and leverages <i class="fm-italics">neural attention</i> to transform each vector into a representation that is aware of the <i class="fm-italics">context</i> provided by the other vectors in the set. When the set in question is an ordered sequence, you can also leverage <i class="fm-italics">positional encoding</i> to create <a id="marker-1013807"></a>Transformers that can take into account both global context and word order, capable of processing long text paragraphs much more effectively than RNNs or 1D convnets.</p>

  <p class="body"><a id="pgfId-1013817"></a>Transformers can be used for any set-processing or sequence-processing task, including text classification, but they excel especially at <i class="fm-italics">sequence-to-sequence learning</i>, such as <a id="marker-1013828"></a>translating paragraphs in a source language into a target language.</p>

  <p class="body"><a id="pgfId-1013838"></a>A sequence-to-sequence Transformer is made up of two parts:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1013844"></a>A <code class="fm-code-in-text">TransformerEncoder</code> that turns <a class="calibre11" id="marker-1013863"></a>an input vector sequence into a context-aware, order-aware output vector sequence</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1013899"></a>A <code class="fm-code-in-text">TransformerDecoder</code> that takes <a class="calibre11" id="marker-1013888"></a>the output of the <code class="fm-code-in-text">TransformerEncoder</code>, as well as a target sequence, and predicts what should come next in the target sequence</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1013908"></a>If you’re only processing a single sequence (or set) of vectors, you’d be only using the <code class="fm-code-in-text">TransformerEncoder</code>.</p>

  <p class="body"><a id="pgfId-1013923"></a>Following is a sequence-to-sequence Transformer for mapping a source sequence to a target sequence (this setup could be used for machine translation or question answering, for instance):</p>
  <pre class="programlisting"><a id="pgfId-1057051"></a>encoder_inputs = keras.Input(shape=(sequence_length,), dtype=<span class="fm-codegreen">"int64"</span>)      <span class="fm-combinumeral">❶</span>
<a id="pgfId-1057052"></a>x = PositionalEmbedding(
<a id="pgfId-1058930"></a>    sequence_length, vocab_size, embed_dim)(encoder_inputs)
<a id="pgfId-1057053"></a>encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)
<a id="pgfId-1057054"></a>decoder_inputs = keras.Input(shape=(<code class="fm-codegreen">None</code>,), dtype=<span class="fm-codegreen">"int64"</span>)                 <span class="fm-combinumeral">❷</span>
<a id="pgfId-1057055"></a>x = PositionalEmbedding(
<a id="pgfId-1058939"></a>    sequence_length, vocab_size, embed_dim)(decoder_inputs)
<a id="pgfId-1057056"></a>x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)
<a id="pgfId-1057057"></a>decoder_outputs = layers.Dense(vocab_size, activation=<span class="fm-codegreen">"softmax"</span>)(x)        <span class="fm-combinumeral">❸</span>
<a id="pgfId-1057058"></a>transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)
<a id="pgfId-1057059"></a>transformer.compile(optimizer=<span class="fm-codegreen">"rmsprop"</span>, loss=<span class="fm-codegreen">"categorical_crossentropy"</span>)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1059531"></a><span class="fm-combinumeral">❶</span> Source sequence</p>

  <p class="fm-code-annotation"><a id="pgfId-1059567"></a><span class="fm-combinumeral">❷</span> Target sequence so far</p>

  <p class="fm-code-annotation"><a id="pgfId-1059532"></a><span class="fm-combinumeral">❸</span> Target sequence one step in the future</p>

  <p class="body"><a id="pgfId-1014061"></a>And this is a lone <code class="fm-code-in-text">TransformerEncoder</code> for binary classification of integer sequences:</p>
  <pre class="programlisting"><a id="pgfId-1057162"></a>inputs = keras.Input(shape=(sequence_length,), dtype=<span class="fm-codegreen">"int64"</span>)
<a id="pgfId-1057163"></a>x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)
<a id="pgfId-1057164"></a>x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)
<a id="pgfId-1057165"></a>x = layers.GlobalMaxPooling1D()(x)
<a id="pgfId-1057166"></a>outputs = layers.Dense(<span class="fm-codeblue">1</span>, activation=<span class="fm-codegreen">"sigmoid"</span>)(x)
<a id="pgfId-1057167"></a>model = keras.Model(inputs, outputs)
<a id="pgfId-1014120"></a>model.compile(optimizer=<span class="fm-codegreen">"rmsprop"</span>, loss=<span class="fm-codegreen">"binary_crossentropy"</span>)</pre>

  <p class="body"><a id="pgfId-1014152"></a>Full implementations of the <code class="fm-code-in-text">TransformerEncoder</code>, the <code class="fm-code-in-text">TransformerDecoder</code>, and the <code class="fm-code-in-text">PositionalEmbedding</code> layer are <a id="marker-1014157"></a>provided in chapter 11. <a id="marker-1014163"></a></p>

  <h3 class="fm-head1" id="heading_id_10"><a id="pgfId-1014174"></a>14.1.7 The space of possibilities</h3>

  <p class="body"><a id="pgfId-1014184"></a>What will you build with these techniques? Remember, building deep learning models is like playing with LEGO bricks: layers can be plugged together to map essentially anything to anything, given that you have appropriate training data available and that the mapping is achievable via a continuous geometric transformation of reasonable complexity. The space of possibilities is infinite. This section offers a few examples to inspire you to think beyond the basic classification and regression tasks that have traditionally been the bread and butter of machine learning.</p>

  <p class="body"><a id="pgfId-1014193"></a>I’ve sorted my suggested applications by input and output modalities in the following list. Note that quite a few of them stretch the limits of what is possible—although a model could be trained on all of these tasks, in some cases such a model probably wouldn’t generalize far from its training data. Sections 14.2 through 14.4 will address how these limitations could be lifted in the future:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1014199"></a>Mapping vector data to vector data:</p>

      <ul class="calibre16">
        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014213"></a><i class="fm-italics1">Predictive healthcare</i>—Mapping patient medical records to predictions of patient outcomes</p>
        </li>

        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014238"></a><i class="fm-italics1">Behavioral targeting</i>—Mapping a set of website attributes with data on how long a user will spend on the website</p>
        </li>

        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014255"></a><i class="fm-italics1">Product quality control</i>—Mapping a set of attributes relative to an instance of a manufactured product with the probability that the product will fail by next year</p>
        </li>
      </ul>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1014272"></a>Mapping image data to vector data:</p>

      <ul class="calibre16">
        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014282"></a><i class="fm-italics1">Medical assistant</i>—Mapping slides of medical images to a prediction about the presence of a tumor</p>
        </li>

        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014307"></a><i class="fm-italics1">Self-driving vehicle</i>—Mapping car dashcam video frames to steering wheel angle commands and gas and braking commands</p>
        </li>

        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014324"></a><i class="fm-italics1">Board game AI</i>—Mapping Go or chess boards to the next player move</p>
        </li>

        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014341"></a><i class="fm-italics1">Diet helper</i>—Mapping pictures of a dish to its calorie count</p>
        </li>

        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014358"></a><i class="fm-italics1">Age prediction</i>—Mapping selfies to the age of the person</p>
        </li>
      </ul>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1014375"></a>Mapping timeseries data to vector data:</p>

      <ul class="calibre16">
        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014385"></a><i class="fm-italics1">Weather prediction</i>—Mapping timeseries of weather data in a grid of locations to the temperature in a specific place one week later</p>
        </li>

        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014410"></a><i class="fm-italics1">Brain-computer interfaces</i>—Mapping timeseries of magnetoencephalogram (MEG) data to computer commands</p>
        </li>

        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014427"></a><i class="fm-italics1">Behavioral targeting</i>—Mapping timeseries of user interactions on a website to the probability that a user will buy something</p>
        </li>
      </ul>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1014444"></a>Mapping text to text:</p>

      <ul class="calibre16">
        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014454"></a><i class="fm-italics1">Machine translation</i>— Mapping a paragraph in one language to a translated version in a different language</p>
        </li>

        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014479"></a><i class="fm-italics1">Smart reply</i>—Mapping emails to possible one-line replies</p>
        </li>

        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014496"></a><i class="fm-italics1">Question answering</i>—Mapping general-knowledge questions to answers</p>
        </li>

        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014513"></a><i class="fm-italics1">Summarization</i>—Mapping a long article to a short summary of the article</p>
        </li>
      </ul>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1014530"></a>Mapping images to text:</p>

      <ul class="calibre16">
        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014540"></a><i class="fm-italics1">Text transcription</i>— Mapping images that contain a text element to the corresponding text string</p>
        </li>

        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014565"></a><i class="fm-italics1">Captioning</i>—Mapping images to short captions describing the contents of the images</p>
        </li>
      </ul>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1014582"></a>Mapping text to images:</p>

      <ul class="calibre16">
        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014592"></a><i class="fm-italics1">Conditioned image generation</i>—Mapping a short text description to images matching the description</p>
        </li>

        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014617"></a><i class="fm-italics1">Logo generation/selection</i>—Mapping the name and description of a company to a logo suggestion</p>
        </li>
      </ul>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1014634"></a>Mapping images to images:</p>

      <ul class="calibre16">
        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014644"></a><i class="fm-italics1">Super-resolution</i>—Mapping downsized images to higher-resolution versions of the same images</p>
        </li>

        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014669"></a><i class="fm-italics1">Visual depth sensing</i>—Mapping images of indoor environments to maps of depth predictions</p>
        </li>
      </ul>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1014686"></a>Mapping images and text to text:</p>

      <ul class="calibre16">
        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014696"></a><i class="fm-italics1">Visual QA</i>—Mapping images and natural language questions about the contents of images to natural language answers</p>
        </li>
      </ul>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1014721"></a>Mapping video and text to text:</p>

      <ul class="calibre16">
        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014731"></a><i class="fm-italics1">Video QA</i>—Mapping short videos and natural language questions about the contents of videos to natural language answers</p>
        </li>
      </ul>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1014780"></a><i class="fm-italics">Almost</i> anything is possible, but not quite <i class="fm-italics">anything</i>. You’ll see in the next section what we <i class="fm-italics">can’t</i> do with deep learning.</p>

  <h2 class="fm-head" id="heading_id_11"><a id="pgfId-1014794"></a>14.2 The limitations of deep learning</h2>

  <p class="body"><a id="pgfId-1014804"></a>The space of applications that can be implemented with deep learning is infinite. And yet, many applications remain completely out of reach for current deep learning techniques—even given vast amounts of human-annotated data. Say, for instance, that you could assemble a dataset of hundreds of thousands—even millions—of English-language descriptions of the features of a software product, written by a product manager, as well as the corresponding source code developed by a team of engineers to meet these requirements. Even with this data, you could not train a deep learning model to read a product description and generate the appropriate codebase. That’s just one example among many. In general, anything that requires reasoning—like programming or applying the scientific method—long-term planning, and algorithmic data manipulation is out of reach for deep learning models, no matter how much data you throw at them. Even learning a simple sorting algorithm with a deep neural network is tremendously difficult.</p>

  <p class="body"><a id="pgfId-1014839"></a>This is because a deep learning model is just <i class="fm-italics">a chain of simple, continuous geometric transformations</i> mapping one vector space into another. All it can do is map one data manifold X into another manifold Y, assuming the existence of a learnable continuous transform from X to Y. A deep learning model can be interpreted as a kind of program, but, inversely, <i class="fm-italics">most programs can’t be expressed as deep learning models</i>. For most tasks, either there exists no corresponding neural network of reasonable size that solves the task or, even if one exists, it may not be <i class="fm-italics">learnable</i>: the corresponding geometric transform may be far too complex, or there may not be appropriate data available to learn it.</p>

  <p class="body"><a id="pgfId-1014848"></a>Scaling up current deep learning techniques by stacking more layers and using more training data can only superficially palliate some of these issues. It won’t solve the more fundamental problems that deep learning models are limited in what they can represent and that most of the programs you may wish to learn can’t be expressed as a continuous geometric morphing of a data manifold.</p>

  <h3 class="fm-head1" id="heading_id_12"><a id="pgfId-1014854"></a>14.2.1 The risk of anthropomorphizing machine learning models</h3>

  <p class="body"><a id="pgfId-1014877"></a><a id="marker-1057627"></a>One real risk with contemporary AI is misinterpreting what deep learning models do and overestimating their abilities. A fundamental feature of humans is our <i class="fm-italics">theory of mind</i>: our tendency <a id="marker-1057630"></a>to project intentions, beliefs, and knowledge on the things around us. Drawing a smiley face on a rock suddenly makes it “happy” in our minds. Applied to deep learning, this means that, for instance, when we’re able to somewhat successfully train a model to generate captions to describe pictures, we’re led to believe that the model “understands” the contents of the pictures and the captions it generates. Then we’re surprised when any slight departure from the sort of images present in the training data causes the model to generate completely absurd captions (see figure 14.1).</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/14-01.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1060364"></a>Figure 14.1 Failure of an image-captioning system based on deep learning</p>

  <p class="body"><a id="pgfId-1014902"></a>In particular, this is highlighted by <i class="fm-italics">adversarial examples</i>, which are <a id="marker-1014927"></a>samples fed to a deep learning network that are designed to trick the model into misclassifying them. You’re already aware that, for instance, it’s possible to do gradient ascent in input space to generate inputs that maximize the activation of some convnet filter—this is the basis of the filter-visualization technique introduced in chapter 9, as well as the DeepDream algorithm from chapter 12. Similarly, through gradient ascent, you can slightly modify an image in order to maximize the class prediction for a given class. By taking a picture of a panda and adding to it a gibbon gradient, we can get a neural network to classify the panda as a gibbon (see figure 14.2). This evidences both the brittleness of these models and the deep difference between their input-to-output mapping and our human perception.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/14-02.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1060407"></a>Figure 14.2 An adversarial example: imperceptible changes in an image can upend a model’s classification of an image.</p>

  <p class="body"><a id="pgfId-1014947"></a>In short, deep learning models don’t have any understanding of their input—at least, not in a human sense. Our own understanding of images, sounds, and language is grounded in our sensorimotor experience as humans. Machine learning models have no access to such experiences and thus can’t understand their inputs in a human-relatable way. By annotating large numbers of training examples to feed into our models, we get them to learn a geometric transform that maps data to human concepts on a specific set of examples, but this mapping is a simplistic sketch of the original model in our minds—the one developed from our experience as embodied agents. It’s like a dim image in a mirror (see figure 14.3). The models you create will take any shortcut available to fit their training data. For instance, image models tend to rely more on local textures than on a global understanding of the input images—a model trained on a dataset that features both leopards and sofas is likely to classify a leopard-pattern sofa as an actual leopard.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/14-03.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1060452"></a>Figure 14.3 Current machine learning models: like a dim image in a mirror</p>

  <p class="body"><a id="pgfId-1014977"></a>As a machine learning practitioner, always be mindful of this, and never fall into the trap of believing that neural networks understand the tasks they perform—they don’t, at least not in a way that would make sense to us. They were trained on a different, far narrower task than the one we wanted to teach them: that of mapping training inputs to training targets, point by point. Show them anything that deviates from their training data, and they will break in absurd ways. <a id="marker-1014993"></a></p>

  <h3 class="fm-head1" id="heading_id_13"><a id="pgfId-1015002"></a>14.2.2 Automatons vs. intelligent agents</h3>

  <p class="body"><a id="pgfId-1015019"></a><a id="marker-1015013"></a>There are fundamental differences between the straightforward geometric morphing from input to output that deep learning models do and the way humans think and learn. It isn’t just the fact that humans learn by themselves from embodied experience instead of being presented with explicit training examples. The human brain is an entirely different beast compared to a differentiable parametric function.</p>

  <p class="body"><a id="pgfId-1015040"></a>Let’s zoom out a little bit and ask, “what’s the purpose of intelligence?” Why did it arise in the first place? We can only speculate, but we can make fairly informed speculations. We can start by looking at brains—the organ that produces intelligence. Brains are an evolutionary adaption—a mechanism developed incrementally over hundreds of millions of years, via random trial-and-error guided by natural selection—that dramatically expanded the ability of organisms to adapt to their environment. Brains originally appeared more than half a billion years ago as a way to <i class="fm-italics">store and execute behavioral programs</i>. “Behavioral programs” are just sets of instructions that make an organism reactive to its environment: “if this happens, then do that.” They link the organism’s sensory inputs to its motor controls. In the beginning, brains would have served to hardcode behavioral programs (as neural connectivity patterns), which would allow an organism to react appropriately to its sensory input. This is the way insect brains still work—flies, ants, C. elegans (see figure 14.4), etc. Because the original “source code” of these programs was DNA, which would get decoded as neural connectivity patterns, evolution was suddenly able to <i class="fm-italics">search over behavior space</i> in a largely unbounded way—a major evolutionary shift.</p>

  <p class="body"><a id="pgfId-1015068"></a>Evolution was the programmer, and brains were computers carefully executing the code evolution gave them. Because neural connectivity is a very general computing substrate, the sensorimotor space of all brain-enabled species could suddenly start undergoing a dramatic expansion. Eyes, ears, mandibles, 4 legs, 24 legs—as long as you have a brain, evolution will kindly figure out for you behavioral programs that make good use of these. Brains can handle any modality—or combination of modalities—you throw at them.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/14-04.png"/></p>

  <p class="fm-figure-caption">Figure 14.4 The brain network of the C. elegans worm: a behavioral automaton "programmed" by natural evolution. Figure created by Emma Towlson (from Yan et al., "Network control principles predict neuron function in the Caenorhabditis elegans connectome," <i class="calibre19">Nature</i>, Oct. 2017).</p>

  <p class="body"><a id="pgfId-1015088"></a>Now, mind you, these early brains weren’t exactly intelligent per se. They were very much <i class="fm-italics">automatons</i>: they would merely execute behavioral programs hardcoded in the organism’s DNA. They could only be described as intelligent in the same sense that a thermostat is “intelligent.” Or a list-sorting program. Or . . . a trained deep neural network (of the artificial kind). This is an important distinction, so let’s look at it carefully: what’s the difference between automatons and actual intelligent agents?<a id="marker-1015099"></a></p>

  <h3 class="fm-head1" id="heading_id_14"><a id="pgfId-1015108"></a>14.2.3 Local generalization vs. extreme generalization</h3>

  <p class="body"><a id="pgfId-1015125"></a><a id="marker-1015119"></a>Seventeenth century French philosopher and scientist René Descartes wrote in 1637 an illuminating comment that perfectly captures this distinction—long before the rise of AI, and in fact, before the first mechanical computer (which his colleague Pascal would create five years later). Descartes tells us, in reference to automatons,</p>

  <p class="fm-quote"><i class="calibre8"><a class="calibre11" id="pgfId-1015130"></a>Even though such machines might do some things as well as we do them, or perhaps even better, they would inevitably fail in others, which would reveal they were acting not through understanding, but only from the disposition of their organs</i>.</p>

  <p class="fm-quote-source"><a class="calibre11" id="pgfId-1015140"></a>—René Descartes, <i class="fm-italics1">Discourse on the Method</i> (1637)</p>

  <p class="body"><a id="pgfId-1015171"></a>There it is. Intelligence is characterized by <i class="fm-italics">understanding</i>, and understanding is evidenced by <i class="fm-italics">generalization</i>—the ability <a id="marker-1015176"></a>to handle whatever novel situation may arise. How do you tell the difference between a student that has memorized the past three years of exam questions but has no understanding of the subject, and a student that actually understands the material? You give them a brand new problem. An automaton is static, crafted to accomplish specific things in a specific context—“if this, then that”—while an intelligent agent can adapt on the fly to novel, unexpected situations. When an automaton is exposed to something that doesn’t match what it is “programmed” to do (whether we’re talking about human-written programs, evolution-generated programs, or the implicit programming process of fitting a model on a training data set), it will fail. Meanwhile, intelligent agents, like humans, will use their understanding to find a way forward.</p>

  <p class="body"><a id="pgfId-1015212"></a>Humans are capable of far more than mapping immediate stimuli to immediate responses, as a deep net, or an insect, would. We maintain complex, abstract models of our current situation, of ourselves, and of other people, and we can use these models to anticipate different possible futures and perform long-term planning. You can merge together known concepts to represent something you’ve never experienced before—like imagining what you’d do if you won the lottery, or picturing how your friend would react if you discreetly replaced her keys with exact copies made of elastic rubber. This ability to handle novelty and what-ifs, to expand our mental model space far beyond what we can experience directly—to leverage <i class="fm-italics">abstraction</i> and <i class="fm-italics">reasoning</i>—is the defining characteristic of human cognition. I call it <i class="fm-italics">extreme generalization</i>: an ability <a id="marker-1015217"></a>to adapt to novel, never-before-experienced situations using little data or even no new data at all. This capability is key to the intelligence displayed by humans and advanced animals.</p>

  <p class="body"><a id="pgfId-1015250"></a>This stands in sharp contrast with what automaton-like systems do. A very rigid automaton wouldn’t feature any generalization at all—it would be incapable of handling anything that it wasn’t precisely told about in advance. A Python dict or a basic question-answering program implemented as hardcoded if-then-else statements would fall into this category. Deep nets do slightly better: they can successfully process inputs that deviate a bit from what they’re familiar with—which is precisely what makes them useful. Our cats vs. dogs model from chapter 8 could classify cat or dog pictures it had not seen before, as long as they were close enough to what it was trained on. However, deep nets are limited to what <a id="marker-1015229"></a>I call <i class="fm-italics">local generalization</i> (see figure 14.5): the mapping from inputs to outputs performed by a deep net quickly stops making sense as inputs start deviating from what the net saw at training time. Deep nets can only generalize to <i class="fm-italics">known unknowns</i>—to factors of variation that were anticipated during model development and that are extensively featured in the training data, such as different camera angles or lighting conditions for pet pictures. That’s because deep nets generalize via interpolation on a manifold (remember chapter 5): any factor of variation in their input space needs to be captured by the manifold they learn. That’s why basic data augmentation is so helpful in improving deep net generalization. Unlike humans, these models have no ability to improvise in the face of situations for which little or no data is available (like winning the lottery or being handed rubber keys) that only share abstract commonalities with past situations.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/14-05.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1060494"></a>Figure 14.5 Local generalization vs. extreme generalization</p>

  <p class="body"><a id="pgfId-1015269"></a>Consider, for instance, the problem of learning the appropriate launch parameters to get a rocket to land on the moon. If you used a deep net for this task and trained it using supervised learning or reinforcement learning, you’d have to feed it tens of thousands or even millions of launch trials: you’d need to expose it to a <i class="fm-italics">dense sampling</i> of the <a id="marker-1015294"></a>input space, in order for it to learn a reliable mapping from input space to output space. In contrast, as humans, we can use our power of abstraction to come up with physical models—rocket science—and derive an exact solution that will land the rocket on the moon in one or a few trials. Similarly, if you developed a deep net controlling a human body, and you wanted it to learn to safely navigate a city without getting hit by cars, the net would have to die many thousands of times in various situations until it could infer that cars are dangerous and develop appropriate avoidance behaviors. Dropped into a new city, the net would have to relearn most of what it knows. On the other hand, humans are able to learn safe behaviors without having to die even once—again, thanks to our power of abstract modeling of novel situations. <a id="marker-1015300"></a></p>

  <h3 class="fm-head1" id="heading_id_15"><a id="pgfId-1015309"></a>14.2.4 The purpose of intelligence</h3>

  <p class="body"><a id="pgfId-1015319"></a>This distinction between highly adaptable intelligent agents and rigid automatons leads us back to brain evolution. Why did brains—originally a mere medium for natural evolution to develop behavioral automatons—eventually turn intelligent? Like every significant evolutionary milestone, it happened because natural selection constraints encouraged it to happen.</p>

  <p class="body"><a id="pgfId-1015328"></a>Brains are responsible for behavior generation. If the set of situations an organism had to face was mostly static and known in advance, behavior generation would be an easy problem: evolution would just figure out the correct behaviors via random trial and error and hardcode them into the organism’s DNA. This first stage of brain evolution—brains as automatons—would already be optimal. However, crucially, as organism complexity—and alongside it, environmental complexity—kept increasing, the situations that animals had to deal with became much more dynamic and more unpredictable. A day in your life, if you look closely, is unlike any day you’ve ever experienced, and unlike any day ever experienced by any of your evolutionary ancestors. You need to be able to face unknown and surprising situations—constantly. There is no way for evolution to find and hardcode as DNA the sequence of behaviors you’ve been executing to successfully navigate your day since you woke up a few hours ago. It has to be generated on the fly—every day.</p>

  <p class="body"><a id="pgfId-1015334"></a>The brain, as a good behavior-generation engine, simply adapted to fit this need. It optimized for adaptability and generality, rather than merely optimizing for fitness to a fixed set of situations. This shift likely occurred multiple times throughout evolutionary history, resulting in highly intelligent animals in very distant evolutionary branches—apes, octopuses, ravens, and more. Intelligence is an answer to challenges presented by complex, dynamic ecosystems.</p>

  <p class="body"><a id="pgfId-1015340"></a>That’s the nature of intelligence: it is the ability to efficiently leverage the information at your disposal in order to produce successful behavior in the face of an uncertain, ever-changing future. What Descartes calls “understanding” is the key to this remarkable capability: the power to mine your past experience to develop modular, reusable abstractions that can be quickly repurposed to handle novel situations and achieve extreme generalization.</p>

  <h3 class="fm-head1" id="heading_id_16"><a id="pgfId-1015349"></a>14.2.5 Climbing the spectrum of generalization</h3>

  <p class="body"><a id="pgfId-1015372"></a><a id="marker-1015360"></a>As a crude caricature, you could summarize the evolutionary history of biological intelligence as a slow climb up the <i class="fm-italics">spectrum of generalization</i>. It started with automaton-like brains that could only perform local generalization. Over time, evolution started producing organisms capable of increasingly broader generalization that could thrive in ever-more complex and variable environments. Eventually, in the past few millions of years—an instant in evolutionary terms—certain hominin species started trending toward an implementation of biological intelligence capable of extreme generalization, precipitating the start of the Anthropocene and forever changing the history of life on earth.</p>

  <p class="body"><a id="pgfId-1015381"></a>The progress of AI over the past 70 years bears striking similarities to this evolution. Early AI systems were pure automatons, like the ELIZA chat program from the 1960s, or SHRDLU,<a id="Id-1015384"></a><a href="../Text/14.htm#pgfId-1015384"><sup class="footnotenumber">2</sup></a> a 1970 AI capable of manipulating simple objects from natural language commands. In the 1990s and 2000s, we saw the rise of machine learning systems capable of local generalization, which could deal with some level of uncertainty and novelty. In the 2010s, deep learning further expanded the local-generalization power of these systems by enabling engineers to leverage much larger datasets and much more expressive models.</p>

  <p class="body"><a id="pgfId-1015422"></a>Today, we may be on the cusp of the next evolutionary step. There is increasing interest in systems that could achieve <i class="fm-italics">broad generalization</i>, which I <a id="marker-1015411"></a>define as the ability to deal with <i class="fm-italics">unknown unknowns</i> within a single broad domain of tasks (including situations the system was not trained to handle and that its creators could not have anticipated). For instance, a self-driving car capable of safely dealing with any situation you throw at it, or a domestic robot that could pass the “Woz test of intelligence”—entering a random kitchen and making a cup of coffee.<a id="Id-1015428"></a><a href="../Text/14.htm#pgfId-1015428"><sup class="footnotenumber">3</sup></a> By combining deep learning and painstakingly handcrafted abstract models of the world, we’re already making visible progress toward these goals.</p>

  <p class="body"><a id="pgfId-1015458"></a>However, for the time being, AI remains limited <a id="marker-1053574"></a>to <i class="fm-italics">cognitive automation</i>: the “intelligence” label in “Artificial Intelligence” is a category error. It would be more accurate to call our field “Artificial Cognition,” with “Cognitive Automation” and “Artificial Intelligence” being two nearly independent subfields within it. In this subdivision, “Artificial Intelligence” would be a greenfield where almost everything remains to be discovered.</p>

  <p class="body"><a id="pgfId-1015467"></a>Now, I don’t mean to diminish the achievements of deep learning. Cognitive automation is incredibly useful, and the way deep learning models are capable of automating tasks from exposure to data alone represents an especially powerful form of cognitive automation, far more practical and versatile than explicit programming. Doing this well is a game-changer for essentially every industry. But it’s still a long way from human (or animal) intelligence. Our models, so far, can only perform local generalization: they map space X to space Y via a smooth geometric transform learned from a dense sampling of X-to-Y data points, and any disruption within spaces X or Y invalidates this mapping. They can only generalize to new situations that stay similar to past data, whereas human cognition is capable of extreme generalization, quickly adapting to radically novel situations and planning for long-term future situations. <a id="marker-1015469"></a></p>

  <h2 class="fm-head" id="heading_id_17"><a id="pgfId-1015480"></a>14.3 Setting the course toward greater generality in AI</h2>

  <p class="body"><a id="pgfId-1015510"></a><a id="marker-1059926"></a>To lift some of the limitations we have discussed and create AI that can compete with human brains, we need to move away from straightforward input-to-output mappings and on to <i class="fm-italics">reasoning</i> and <i class="fm-italics">abstraction</i>. In the following couple of sections, we’ll take a look at what the road ahead may look like.</p>

  <h3 class="fm-head1" id="heading_id_18"><a id="pgfId-1015519"></a>14.3.1 On the importance of setting the right objective: The shortcut rule</h3>

  <p class="body"><a id="pgfId-1015536"></a><a id="marker-1059929"></a><a id="marker-1059930"></a>Biological intelligence was the answer to a question asked by nature. Likewise, if we want to develop true artificial intelligence, first, we need to be asking the right questions.</p>

  <p class="body"><a id="pgfId-1015541"></a>An effect you see constantly in systems design is the <i class="fm-italics">shortcut rule</i>: if you focus on optimizing one success metric, you will achieve your goal, but at the expense of everything in the system that wasn’t covered by your success metric. You end up taking every available shortcut toward the goal. Your creations are shaped by the incentives you give yourself.</p>

  <p class="body"><a id="pgfId-1015556"></a>You see this often in machine learning competitions. In 2009, Netflix ran a challenge that promised a $1 million prize to the team that achieved the highest score on a movie recommendation task. It ended up never using the system created by the winning team, because it was way too complex and compute-intensive. The winners had optimized for prediction accuracy alone—what they were incentivized to achieve—at the expense of every other desirable characteristic of the system: inference cost, maintainability, and explainability. The shortcut rule holds true in most Kaggle competitions as well—the models produced by Kaggle winners can rarely, if ever, be used in production.</p>

  <p class="body"><a id="pgfId-1015562"></a>The shortcut rule has been everywhere in AI over the past few decades. In the 1970s, psychologist and computer science pioneer Allen Newell, concerned that his field wasn’t making any meaningful progress toward a proper theory of cognition, proposed a new grand goal for AI: chess-playing. The rationale was that playing chess, in humans, seemed to involve—perhaps even require—capabilities such as perception, reasoning and analysis, memory, study from books, and so on. Surely, if we could build a chess-playing machine, it would have to feature these attributes as well. Right?</p>

  <p class="body"><a id="pgfId-1015568"></a>Over two decades later, the dream came true: in 1997, IBM’s Deep Blue beat Gary Kasparov, the best chess player in the world. Researchers had then to contend with the fact that creating a chess-champion AI had taught them little about human intelligence. The Alpha–Beta algorithm at the heart of Deep Blue wasn’t a model of the human brain and couldn’t generalize to tasks other than similar board games. It turned out it was easier to build an AI that could only play chess than to build an artificial mind—so that’s the shortcut researchers took.</p>

  <p class="body"><a id="pgfId-1015574"></a>So far, the driving success metric of the field of AI has been to solve specific tasks, from chess to Go, from MNIST classification to ImageNet, from Atari Arcade games to StarCraft and DotA 2. Consequently, the history of the field has been defined by a series of “successes” where we figured out how to solve these tasks <i class="fm-italics">without featuring any intelligence</i>.</p>

  <p class="body"><a id="pgfId-1015589"></a>If that sounds like a surprising statement, keep in mind that human-like intelligence isn’t characterized by skill at any particular task—rather, it is the ability to adapt to novelty, to efficiently acquire new skills and master never-seen-before tasks. By fixing the task, you make it possible to provide an arbitrarily precise description of what needs to be done—either via hardcoding human-provided knowledge or by supplying humongous amounts of data. You make it possible for engineers to “buy” more skill for their AI by just adding data or adding hardcoded knowledge, without increasing the generalization power of the AI (see figure 14.6). If you have near-infinite training data, even a very crude algorithm like nearest-neighbor search can play video games with superhuman skill. Likewise if you have a near-infinite amount of human-written if-then-else statements. That is, until you make a small change to the rules of the game—the kind a human could adapt to instantly—that will require the non-intelligent system to be retrained or rebuilt from scratch.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/14-06.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1060541"></a>Figure 14.6 A low-generalization system can achieve arbitrary skill at a fixed task given unlimited task-specific information.</p>

  <p class="body"><a id="pgfId-1015605"></a>In short, by fixing the task, you remove the need to handle uncertainty and novelty, and since the nature of intelligence is the ability to handle uncertainty and novelty, you’re effectively removing the need for intelligence. And because it’s always easier to find a non-intelligent solution to a specific task than to solve the general problem of intelligence, that’s the shortcut you will take 100% of the time. Humans can use their general intelligence to acquire skills at any new task, but in reverse, there is no path from a collection of task-specific skills to general intelligence. <a id="marker-1015621"></a><a id="marker-1015624"></a></p>

  <h3 class="fm-head1" id="heading_id_19"><a id="pgfId-1015630"></a>14.3.2 A new target</h3>

  <p class="body"><a id="pgfId-1015650"></a><a id="marker-1015641"></a>To make artificial intelligence actually intelligent, and give it the ability to deal with the incredible variability and ever-changing nature of the real world, we first need to move away from seeking to achieve <i class="fm-italics">task-specific skill</i>, and instead, start targeting generalization power itself. We need new metrics of progress that will help us develop increasingly intelligent systems. Metrics that will point in the right direction and that will give us an actionable feedback signal. As long as we set our goal to be “create a model that solves task X,” the shortcut rule will apply, and we’ll end up with a model that does X, period.</p>

  <p class="body"><a id="pgfId-1015731"></a>In my view, intelligence can be precisely quantified as an <i class="fm-italics">efficiency ratio</i>: the conversion <a id="marker-1015670"></a>ratio between the <i class="fm-italics">amount of relevant information</i> you have available about the world (which could be either <i class="fm-italics">past experience</i> or innate <i class="fm-italics">prior knowledge</i>) and your <i class="fm-italics">future operating area</i>, the set of novel situations where you will be able to produce appropriate behavior (you can view this as your <i class="fm-italics">skillset</i>). A more intelligent agent will be able to handle a broader set of future tasks and situations using a smaller amount of past experience. To measure such a ratio, you just need to fix the information available to your system—its experience and its prior knowledge—and measure its performance on a set of reference situations or tasks that are known to be sufficiently different from what the system has had access to. Trying to maximize this ratio should lead you toward intelligence. Crucially, to avoid cheating, you’re going to need to make sure you test the system only on tasks it wasn’t programmed or trained to handle—in fact, you need tasks that the <i class="fm-italics">creators of the system could not have anticipated</i>.</p>

  <p class="body"><a id="pgfId-1015753"></a>In 2018 and 2019, I developed a benchmark dataset called <a id="marker-1015742"></a>the <i class="fm-italics">Abstraction and Reasoning Corpus (ARC)</i> <a id="Id-1015759"></a><a href="../Text/14.htm#pgfId-1015759"><sup class="footnotenumber">4</sup></a> that seeks to capture this definition of intelligence. ARC is meant to be approachable by both machines and humans, and it looks very similar to human IQ tests, such as Raven’s progressive matrices. At test time, you’ll see a series of “tasks.” Each task is explained via three or four “examples” that take the form of an input grid and a corresponding output grid (see figure 14.7). You’ll then be given a brand new input grid, and you’ll have three tries to produce the correct output grid before moving on to the next task.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/14-07.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1060586"></a>Figure 14.7 An ARC task: the nature of the task is demonstrated by a couple of input-output pair examples. Provided with a new input, you must construct the corresponding output.</p>

  <p class="body"><a id="pgfId-1015786"></a>Compared to IQ tests, two things are unique about ARC. First, ARC seeks to measure generalization power, by only testing you on tasks you’ve never seen before. That means that ARC is <i class="fm-italics">a game you can’t practice for</i>, at least in theory: the tasks you will get tested on will have their own unique logic that you will have to understand on the fly. You can’t just memorize specific strategies from past tasks.</p>

  <p class="body"><a id="pgfId-1015815"></a>In addition, ARC tries to control for the <i class="fm-italics">prior knowledge</i> that you bring to the test. You never approach a new problem entirely from scratch—you bring to it preexisting skills and information. ARC makes the assumption that all test takers should start from the set of knowledge priors, called “Core Knowledge priors,” that represent the “knowledge systems” that humans are born with. Unlike an IQ test, ARC tasks will never involve acquired knowledge, like English sentences, for instance.</p>

  <p class="body"><a id="pgfId-1015830"></a>Unsurprisingly, deep-learning-based methods (including models trained on extremely large amounts of external data, like GPT-3) have proven entirely unable to solve ARC tasks, because these tasks are non-interpolative, and thus are a poor fit for curve-fitting. Meanwhile, average humans have no issue solving these tasks on the first try, without any practice. When you see a situation like this, where humans as young as five are able to naturally perform something that seems to be completely out of reach for modern AI technology, that’s a clear signal that something interesting is going on—that we’re missing something.</p>

  <p class="body"><a id="pgfId-1015836"></a>What would it take to solve ARC? Hopefully, this challenge will get you thinking. That’s the entire point of ARC: to give you a goal of a different kind that will nudge you in a new direction—hopefully a productive direction. Now, let’s take a quick look at the key ingredients you’re going to need if you want to answer the call. <a id="marker-1015838"></a><a id="marker-1015841"></a></p>

  <h2 class="fm-head" id="heading_id_20"><a id="pgfId-1015847"></a>14.4 Implementing intelligence: The missing ingredients</h2>

  <p class="body"><a id="pgfId-1015857"></a>So far, you’ve learned that there’s a lot more to intelligence than the sort of latent manifold interpolation that deep learning does. But what, then, do we need to start building real intelligence? What are the core pieces that are currently eluding us?</p>

  <h3 class="fm-head1" id="heading_id_21"><a id="pgfId-1015863"></a>14.4.1 Intelligence as sensitivity to abstract analogies</h3>

  <p class="body"><a id="pgfId-1015883"></a>Intelligence is the ability to use your past experience (and innate prior knowledge) to face novel, unexpected future situations. Now, if the future you had to face was <i class="fm-italics">truly novel</i>—sharing no common ground with anything you’ve seen before—you’d be unable to react to it, no matter how intelligent you were.</p>

  <p class="body"><a id="pgfId-1015892"></a>Intelligence works because nothing is ever truly without precedent. When we encounter something new, we’re able to make sense of it by drawing analogies to our past experience, by articulating it in terms of the abstract concepts we’ve collected over time. A person from the 17th century seeing a jet plane for the first time might describe it as a large, loud metal bird that doesn’t flap its wings. A car? That’s a horseless carriage. If you’re trying to teach physics to a grade schooler, you can explain how electricity is like water in a pipe, or how space-time is like a rubber sheet getting distorted by heavy objects.</p>

  <p class="body"><a id="pgfId-1015898"></a>Besides such clear-cut, explicit analogies, we’re constantly making smaller, implicit analogies—every second, with every thought. Analogies are how we navigate life. Going to a new supermarket? You’ll find your way by relating it to similar stores you’ve been to. Talking to someone new? They’ll remind you of a few people you’ve met before. Even seemingly random patterns, like the shape of clouds, instantly evoke in us vivid images—an elephant, a ship, a fish.</p>

  <p class="body"><a id="pgfId-1015904"></a>These analogies aren’t just in our minds, either: physical reality itself is full of isomorphisms. Electromagnetism is analogous to gravity. Animals are all structurally similar to each other, due to shared origins. Silica crystals are similar to ice crystals. And so on.</p>

  <p class="body"><a id="pgfId-1015932"></a>I call this the <i class="fm-italics">kaleidoscope hypothesis</i>: our experience <a id="marker-1015921"></a>of the world seems to feature incredible complexity and never-ending novelty, but everything in this sea of complexity is similar to everything else. The number of <i class="fm-italics">unique atoms of meaning</i> that you need to describe the universe you live in is relatively small, and everything around you is a recombination of these atoms. A few seeds, endless variation—much like what goes on inside a kaleidoscope, where a few glass beads are reflected by a system of mirrors to produce rich, seemingly ever-changing patterns (see figure 14.8).</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/14-08.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1060628"></a>Figure 14.8 A kaleidoscope produces rich (yet repetitive) patterns from just a few beads of colored glass.</p>

  <p class="body"><a id="pgfId-1015951"></a>Generalization power—intelligence—is the ability to mine your experience to identify these atoms of meaning that can seemingly be reused across many different situations. Once extracted, they’re called <i class="fm-italics">abstractions</i>. Whenever you encounter a new situation, you make sense of it via your accumulated collection of abstractions. How do you identify reusable atoms of meaning? Simply by noticing when two things are similar—by noticing analogies. If something is repeated twice, then both instances must have a single origin, like in a kaleidoscope. Abstraction is the engine of intelligence, and analogy-making is the engine that produces abstraction.</p>

  <p class="body"><a id="pgfId-1015980"></a>In short, intelligence is literally sensitivity to abstract analogies, and that’s in fact all there is to it. If you have a high sensitivity to analogies, you will extract powerful abstractions from little experience, and you will be able to use these abstractions to operate in a maximally large area of future experience space. You will be maximally efficient in converting past experience into the ability to handle future novelty.</p>

  <h3 class="fm-head1" id="heading_id_22"><a id="pgfId-1015989"></a>14.4.2 The two poles of abstraction</h3>

  <p class="body"><a id="pgfId-1016022"></a><a id="marker-1016000"></a>If intelligence is sensitivity to analogies, then developing artificial intelligence should start with spelling out a step-by-step algorithm for analogy-making. Analogy-making starts with <i class="fm-italics">comparing things to one other</i>. Crucially, there are <i class="fm-italics">two distinct ways</i> to compare things, from which arise two different kinds of abstraction, two modes of thinking, each better suited to a different kind of problem. Together, these two poles of abstraction form the basis for all of our thoughts.</p>

  <p class="body"><a id="pgfId-1016112"></a>The first way to relate things to each <a id="marker-1016033"></a>other is <i class="fm-italics">similarity comparison</i>, which gives rise <a id="marker-1016049"></a>to <i class="fm-italics">value-centric analogies</i>. The second way <a id="marker-1016065"></a>is <i class="fm-italics">exact structural match</i>, which gives rise <a id="marker-1016081"></a>to <i class="fm-italics">program-centric analogies</i> (or structure-centric analogies). In both cases, you start from <i class="fm-italics">instances</i> of a thing, and you merge together related instances to produce an <i class="fm-italics">abstraction</i> that captures the common elements of the underlying instances. What varies is how you tell that two instances are related, and how you merge instances into abstractions. Let’s take a close look at each type.</p>

  <p class="fm-head2"><a id="pgfId-1016121"></a>Value-centric analogy</p>

  <p class="body"><a id="pgfId-1016176"></a><a id="marker-1016132"></a>Let’s say you come across a number of different beetles in your backyard, belonging to multiple species. You’ll notice similarities between them. Some will be more similar to one another, and some will be less similar: the notion of similarity is implicitly a smooth, continuous <i class="fm-italics">distance function</i> that defines <a id="marker-1016149"></a>a latent manifold where your instances live. Once you’ve seen enough beetles, you can start clustering more similar instances together and merging them into a set of <i class="fm-italics">prototypes</i> that captures <a id="marker-1016165"></a>the shared visual features of each cluster (see figure 14.9). This prototype is abstract: it doesn’t look like any specific instance you’ve seen, though it encodes properties that are common across all of them. When you encounter a new beetle, you won’t need to compare it to every single beetle you’ve seen before in order to know what to do with it. You can simply compare it to your handful of prototypes, so as to find the closest prototype—the beetle’s <i class="fm-italics">category</i>—and use it to make useful predictions: is the beetle likely to bite you? Will it eat your apples?</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/14-09.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1060670"></a>Figure 14.9 Value-centric analogy relates instances via a continuous notion of similarity to obtain abstract prototypes.</p>

  <p class="body"><a id="pgfId-1016195"></a>Does this sound familiar? It’s pretty much a description of what unsupervised machine learning (such as the K-means clustering algorithm) does. In general, all of modern machine learning, unsupervised or not, works by learning latent manifolds that describe a space of instances encoded via prototypes. (Remember the convnet features you visualized in chapter 9? They were visual prototypes.) Value-centric analogy is the kind of analogy-making that enables deep learning models to perform local generalization.</p>

  <p class="body"><a id="pgfId-1016248"></a>It’s also what many of your own cognitive abilities run on. As a human, you perform value-centric analogies all the time. It’s the type of abstraction <a id="marker-1016217"></a>that underlies <i class="fm-italics">pattern recognition</i>, <i class="fm-italics">perception</i>, and <i class="fm-italics">intuition</i>. If you can do a task without thinking about it, you’re relying heavily on value-centric analogies. If you’re watching a movie and you start subconsciously categorizing the different characters into “types,” that’s value-centric abstraction. <a id="marker-1016253"></a></p>

  <p class="fm-head2"><a id="pgfId-1016262"></a>Program-centric analogy</p>

  <p class="body"><a id="pgfId-1016279"></a><a id="marker-1016273"></a>Crucially, there’s more to cognition than the kind of immediate, approximative, intuitive categorization that value-centric analogy enables. There’s another type of abstraction-generation mechanism that’s slower, exact, deliberate: program-centric (or structure-centric) analogy.</p>

  <p class="body"><a id="pgfId-1016326"></a>In software engineering, you often write different functions or classes that seem to have a lot in common. When you notice these redundancies, you start asking, “could there be a more abstract function that performs the same job, that could be reused twice? Could there be an abstract base class that both of my classes could inherit from?” The definition of abstraction you’re using here corresponds to program-centric analogy. You’re not trying to compare your classes and functions by <i class="fm-italics">how similar</i> they look, the way you’d compare two human faces, via an implicit distance function. Rather, you’re interested in whether there are <i class="fm-italics">parts</i> of them that have <i class="fm-italics">exactly the same structure</i>. You’re looking for what is <a id="marker-1016315"></a>called a <i class="fm-italics">subgraph isomorphism</i> (see figure 14.10): programs can be represented as graphs of operators, and you’re trying to find subgraphs (program subsets) that are exactly shared across your different programs.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/14-10.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1060712"></a>Figure 14.10 Program-centric analogy identifies and isolates isomorphic substructures across different instances</p>

  <p class="body"><a id="pgfId-1016385"></a>This kind of analogy-making via exact structural match within different discrete structures isn’t at all exclusive to specialized fields like computer science or mathematics—you’re constantly using it without noticing. It underlies <i class="fm-italics">reasoning</i>, <i class="fm-italics">planning</i>, and the general concept of <i class="fm-italics">rigor</i> (as opposed to intuition). Any time you’re thinking about objects connected to each other by a discrete network of relationships (rather than a continuous similarity function), you’re leveraging program-centric analogies. <a id="marker-1016390"></a></p>

  <p class="fm-head2"><a id="pgfId-1016399"></a>Cognition as a combination of both kinds of abstraction</p>

  <p class="body"><a id="pgfId-1016416"></a><a id="marker-1016410"></a>Let’s compare these two poles of abstraction side by side (see table 14.1).</p>

  <p class="fm-table-caption"><a id="pgfId-1055042"></a>Table 14.1 The two poles of abstraction</p>

  <table border="1" class="contenttable" width="100%">
    <tr class="calibre20">
      <th class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1016468"></a>Value-centric abstraction</p>
      </th>

      <th class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1016470"></a>Program-centric abstraction</p>
      </th>
    </tr>

    <tr class="calibre20">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016433"></a>Relates things by distance</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016435"></a>Relates things by exact structural match</p>
      </td>
    </tr>

    <tr class="calibre20">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016505"></a>Continuous, grounded in geometry</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016508"></a>Discrete, grounded in topology</p>
      </td>
    </tr>

    <tr class="calibre20">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016522"></a>Produces abstractions by “averaging” instances into “prototypes”</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016525"></a>Produces abstractions by isolating isomorphic substructures across instances</p>
      </td>
    </tr>

    <tr class="calibre20">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016539"></a>Underlies perception and intuition</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016542"></a>Underlies reasoning and planning</p>
      </td>
    </tr>

    <tr class="calibre20">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016556"></a>Immediate, fuzzy, approximative</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016559"></a>Slow, exact, rigorous</p>
      </td>
    </tr>

    <tr class="calibre20">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016573"></a>Requires a lot of experience to produce reliable results</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016576"></a>Experience-efficient, can operate on as few as two instances</p>
      </td>
    </tr>
  </table>

  <p class="body"><a id="pgfId-1016592"></a>Everything we do, everything we think, is a combination of these two types of abstraction. You’d be hard-pressed to find tasks that only involve one of the two. Even a seemingly “pure perception” task, like recognizing objects in a scene, involves a fair amount of implicit reasoning about the relationships between the objects you’re looking at. And even a seemingly “pure reasoning” task, like finding the proof of a mathematical theorem, involves a good amount of intuition. When a mathematician puts their pen to the paper, they’ve already got a fuzzy vision of the direction in which they’re going. The discrete reasoning steps they take to get to the destination are guided by high-level intuition.</p>

  <p class="body"><a id="pgfId-1016598"></a>These two poles are complementary, and it’s their interleaving that enables extreme generalization. No mind could be complete without both of them. <a id="marker-1016600"></a><a id="marker-1016605"></a></p>

  <h3 class="fm-head1" id="heading_id_23"><a id="pgfId-1016613"></a>14.4.3 The missing half of the picture</h3>

  <p class="body"><a id="pgfId-1016623"></a>By this point, you should start seeing what’s missing from modern deep learning: it’s very good at encoding value-centric abstraction, but it has basically no ability to generate program-centric abstraction. Human-like intelligence is a tight interleaving of both types, so we’re literally missing half of what we need—arguably the most important half.</p>

  <p class="body"><a id="pgfId-1016632"></a>Now, here’s a caveat. So far, I’ve presented each type of abstraction as entirely separate from the other—opposite, even. In practice, however, they’re more of a spectrum: to an extent, you could do reasoning by embedding discrete programs in continuous manifolds—just like you may fit a polynomial function through any set of discrete points, as long as you have enough coefficients. And inversely, you could use discrete programs to emulate continuous distance functions—after all, when you’re doing linear algebra on a computer, you’re working with continuous spaces, entirely via discrete programs that operate on ones and zeros.</p>

  <p class="body"><a id="pgfId-1016638"></a>However, there are clearly types of problems that are better suited to one or the other. Try to train a deep learning model to sort a list of five numbers, for instance. With the right architecture, it’s not impossible, but it’s an exercise in frustration. You’ll need a massive amount of training data to make it happen—and even then, the model will still make occasional mistakes when presented with new numbers. And if you want to start sorting lists of 10 numbers instead, you’ll need to completely retrain the model on even more data. Meanwhile, writing a sorting algorithm in Python takes just a few lines—and the resulting program, once validated on a couple more examples, will work every time on lists of any size. That’s pretty strong generalization: going from a couple of demonstration examples and test examples to a program that can successfully process literally any list of numbers.</p>

  <p class="body"><a id="pgfId-1016644"></a>In reverse, perception problems are a terrible fit for discrete reasoning processes. Try to write a pure-Python program to classify MNIST digits without using any machine learning technique: you’re in for a ride. You’ll find yourself painstakingly coding functions that can detect the number of closed loops in a digit, the coordinates of the center of mass of a digit, and so on. After thousands of lines of code, you might achieve . . . 90% test accuracy. In this case, fitting a parametric model is much simpler; it can better utilize the large amount of data that’s available, and it achieves much more robust results. If you have lots of data and you’re faced with a problem where the manifold hypothesis applies, go with deep learning.</p>

  <p class="body"><a id="pgfId-1016650"></a>For this reason, it’s unlikely that we’ll see the rise of an approach that would reduce reasoning problems to manifold interpolation, or that would reduce perception problems to discrete reasoning. The way forward in AI is to develop a unified framework that incorporates <i class="fm-italics">both</i> types of abstract analogy-making. Let’s examine what that might look like.</p>

  <h2 class="fm-head" id="heading_id_24"><a id="pgfId-1016668"></a>14.5 The future of deep learning</h2>

  <p class="body"><a id="pgfId-1016678"></a>Given what we know of how deep nets work, their limitations, and what they’re currently missing, can we predict where things are headed in the medium term? Following are some purely personal thoughts. Note that I don’t have a crystal ball, so a lot of what I anticipate may fail to become reality. I’m sharing these predictions not because I expect them to be proven completely right in the future, but because they’re interesting and actionable in the present.</p>

  <p class="body"><a id="pgfId-1016687"></a>At a high level, these are the main directions in which I see promise:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1016693"></a><i class="fm-italics1">Models closer to general-purpose computer programs</i>, built on top of far richer primitives than the current differentiable layers. This is how we’ll get to reasoning and abstraction, the lack of which is the fundamental weakness of current models.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1016714"></a><i class="fm-italics1">A fusion between deep learning and discrete search over program spaces</i>, with the former providing perception and intuition capabilities, and the latter providing reasoning and planning capabilities.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1016731"></a><i class="fm-italics1">Greater, systematic reuse of previously learned features and architectures</i>, such as meta-learning systems using reusable and modular program subroutines.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1016748"></a>Additionally, note that these considerations aren’t specific to the sort of supervised learning that has been the bread and butter of deep learning so far—rather, they’re applicable to any form of machine learning, including unsupervised, self-supervised, and reinforcement learning. It isn’t fundamentally important where your labels come from or what your training loop looks like; these different branches of machine learning are different facets of the same construct. Let’s dive in.</p>

  <h3 class="fm-head1" id="heading_id_25"><a id="pgfId-1016754"></a>14.5.1 Models as programs</h3>

  <p class="body"><a id="pgfId-1016794"></a>As noted in the previous section, a necessary transformational development that we can expect in the field of machine learning is a move away from models that perform purely <i class="fm-italics">pattern recognition</i> and can only achieve <i class="fm-italics">local generalization</i>, toward models capable of abstraction and reasoning that can achieve <i class="fm-italics">extreme generalization</i>. Current AI programs that are capable of basic forms of reasoning are all hardcoded by human programmers: for instance, software that relies on search algorithms, graph manipulation, and formal logic.</p>

  <p class="body"><a id="pgfId-1016825"></a>That may be about to change, thanks to <i class="fm-italics">program synthesis</i>—a field <a id="marker-1055834"></a>that is very niche today, but I expect to take off in a big way over the next few decades. Program synthesis consists of automatically generating simple programs by using a search algorithm (possibly genetic search, as in <i class="fm-italics">genetic programming</i>) to explore a large space of possible programs (see figure 14.11). The search stops when a program is found that matches the required specifications, often provided as a set of input-output pairs. This is highly reminiscent of machine learning: given training data provided as input-output pairs, we find a program that matches inputs to outputs and can generalize to new inputs. The difference is that instead of learning parameter values in a hardcoded program (a neural network), we generate source code via a discrete search process (see table 14.2).</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/14-11.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1060754"></a>Figure 14.11 A schematic view of program synthesis: given a program specification and a set of building blocks, a search process assembles the building blocks into candidate programs, which are then tested against the specification. The search continues until a valid program is found.</p>

  <p class="fm-table-caption"><a id="pgfId-1055125"></a>Table 14.2 Machine learning vs. program synthesis</p>

  <table border="1" class="contenttable" width="100%">
    <tr class="calibre20">
      <th class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1016905"></a>Machine learning</p>
      </th>

      <th class="fm-contenttable1" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1016907"></a>Program synthesis</p>
      </th>
    </tr>

    <tr class="calibre20">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016870"></a>Model: differentiable parametric function</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016872"></a>Model: graph of operators from a programming language</p>
      </td>
    </tr>

    <tr class="calibre20">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016942"></a>Engine: gradient descent</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016945"></a>Engine: discrete search (such as genetic search)</p>
      </td>
    </tr>

    <tr class="calibre20">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016959"></a>Requires a lot of data to produce reliable results</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1016962"></a>Data-efficient, can work with a couple of training examples</p>
      </td>
    </tr>
  </table>

  <p class="body"><a id="pgfId-1016978"></a>Program synthesis is how we’re going to add program-centric abstraction capabilities to our AI systems. It’s the missing piece of the puzzle. I mentioned earlier that deep learning techniques were entirely unusable on ARC, a reasoning-focused intelligence test. Meanwhile, very crude program-synthesis approaches are already producing very promising results on this benchmark.</p>

  <h3 class="fm-head1" id="heading_id_26"><a id="pgfId-1016987"></a>14.5.2 Blending together deep learning and program synthesis</h3>

  <p class="body"><a id="pgfId-1017004"></a><a id="marker-1016998"></a>Of course, deep learning isn’t going anywhere. Program synthesis isn’t its replacement; it is its complement. It’s the hemisphere that has been so far missing from our artificial brains. We’re going to be leveraging both, in combination. There are two major ways this will take place:</p>

  <ol class="calibre14">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1017009"></a>Developing systems that integrate both deep learning modules and discrete algorithmic modules</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1017023"></a>Using deep learning to make the program search process itself more efficient</p>
    </li>
  </ol>

  <p class="body"><a id="pgfId-1017033"></a>Let’s review each of these possible avenues.</p>

  <p class="fm-head2"><a id="pgfId-1017039"></a>Integrating deep learning modules and algorithmic modules into hybrid systems</p>

  <p class="body"><a id="pgfId-1017049"></a><a id="marker-1058162"></a>Today, the most powerful AI systems are hybrid: they leverage both deep learning models and handcrafted symbol-manipulation programs. In DeepMind’s AlphaGo, for example, most of the intelligence on display is designed and hardcoded by human programmers (such as Monte Carlo Tree Search). Learning from data happens only in specialized submodules (value networks and policy networks). Or consider autonomous vehicles: a self-driving car is able to handle a large variety of situations because it maintains a model of the world around it—a literal 3D model—full of assumptions hardcoded by human engineers. This model is constantly updated via deep learning perception modules that interface it with the surroundings of the car.</p>

  <p class="body"><a id="pgfId-1017058"></a>For both of these systems—AlphaGo and self-driving vehicles—the combination of human-created discrete programs and learned continuous models is what unlocks a level of performance that would be impossible with either approach in isolation, such as an end-to-end deep net or a piece of software without ML elements. So far, the discrete algorithmic elements of such hybrid systems are painstakingly hardcoded by human engineers. But in the future, such systems may be fully learned, with no human involvement.</p>

  <p class="body"><a id="pgfId-1017134"></a>What will this look like? Consider a well-known type of network: RNNs. It’s important to note that RNNs have slightly fewer limitations than feedforward networks. That’s because RNNs are a bit more than mere geometric transformations: they’re geometric transformations <i class="fm-italics">repeatedly applied inside a</i> <code class="fm-code-in-text">for</code> <i class="fm-italics">loop</i>. The temporal <code class="fm-code-in-text">for</code> loop is <a id="marker-1017097"></a>itself hardcoded by human developers: it’s a built-in assumption of the network. Naturally, RNNs are still extremely limited in what they can represent, primarily because each step they perform is a differentiable geometric transformation, and they carry information from step to step via points in a continuous geometric space (state vectors). Now imagine a neural network that’s augmented in a similar way with programming primitives, but instead of a single hardcoded <code class="fm-code-in-text">for</code> loop with hardcoded continuous-space memory, the network includes a large set of programming primitives that the model is free to manipulate to expand its processing <a id="marker-1017113"></a>function, such as <code class="fm-code-in-text">if</code> branches, <code class="fm-code-in-text">while</code> statements, variable creation, disk storage for long-term memory, sorting operators, advanced data structures (such as lists, graphs, and hash tables), and many more. The space of programs that such a network could represent would be far broader than what can be represented with current deep learning models, and some of these programs could achieve superior generalization power. Importantly, such programs will not be differentiable end-to-end, though specific modules will remain differentiable and thus will need to be generated via a combination of discrete program search and gradient descent.</p>

  <p class="body"><a id="pgfId-1017143"></a>We’ll move away from having, on one hand, hardcoded algorithmic intelligence (handcrafted software) and, on the other hand, learned geometric intelligence (deep learning). Instead, we’ll have a blend of formal algorithmic modules that provide reasoning and abstraction capabilities, and geometric modules that provide informal intuition and pattern-recognition capabilities (see figure 14.12). The entire system will be learned with little or no human involvement. This should dramatically expand the scope of problems that can be solved with machine learning—the space of programs that we can generate automatically, given appropriate training data. Systems like AlphaGo—or even RNNs—can be seen as a prehistoric ancestor of such hybrid algorithmic-geometric models. <a id="marker-1017145"></a></p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/14-12.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1060796"></a>Figure 14.12 A learned program relying on both geometric primitives (pattern recognition, intuition) and algorithmic primitives (reasoning, search, memory)</p>

  <p class="fm-head2"><a id="pgfId-1017162"></a>Using deep learning to guide program search</p>

  <p class="body"><a id="pgfId-1017196"></a><a id="marker-1017187"></a>Today, program synthesis faces a major obstacle: it’s tremendously inefficient. To caricature, program synthesis works by trying every possible program in a search space until it finds one that matches the specification provided. As the complexity of the program specification increases, or as the vocabulary of primitives used to write programs expands, the program search process runs into what’s known as <i class="fm-italics">combinatorial explosion</i>, where the <a id="marker-1017201"></a>set of possible programs to consider grows very fast; in fact, much faster than merely exponentially fast. As a result, today, program synthesis can only be used to generate very short programs. You’re not going to be generating a new OS for your computer anytime soon.</p>

  <p class="body"><a id="pgfId-1017211"></a>To move forward, we’re going to need to make program synthesis efficient by bringing it closer to the way humans write software. When you open your editor to code up a script, you’re not thinking about every possible program you could potentially write. You only have in mind a handful of possible approaches: you can use your understanding of the problem and your past experience to drastically cut through the space of possible options to consider.</p>

  <p class="body"><a id="pgfId-1017243"></a>Deep learning can help program synthesis do the same: although each specific program we’d like to generate might be a fundamentally discrete object that performs non-interpolative data manipulation, evidence so far indicates that <i class="fm-italics">the space of all useful programs</i> may look a lot like a continuous manifold. That means that a deep learning model that has been trained on millions of successful program-generation episodes might start to develop solid <i class="fm-italics">intuition</i> about the <i class="fm-italics">path through program space</i> that the search process should take to go from a specification to the corresponding program—just like a software engineer might have immediate intuition about the overall architecture of the script they’re about to write, about the intermediate functions and classes they should use as stepping stones on the way to the goal.</p>

  <p class="body"><a id="pgfId-1017252"></a>Remember that human reasoning is heavily guided by value-centric abstraction, that is to say, by pattern recognition and intuition. Program synthesis should be too. I expect the general approach of guiding program search via learned heuristics to see increasing research interest over the next ten to twenty years. <a id="marker-1017254"></a><a id="marker-1017257"></a></p>

  <h3 class="fm-head1" id="heading_id_27"><a id="pgfId-1017265"></a>14.5.3 Lifelong learning and modular subroutine reuse</h3>

  <p class="body"><a id="pgfId-1017284"></a><a id="marker-1017276"></a><a id="marker-1017278"></a>If models become more complex and are built on top of richer algorithmic primitives, this increased complexity will require higher reuse between tasks, rather than training a new model from scratch every time we have a new task or a new dataset. Many datasets don’t contain enough information for us to develop a new, complex model from scratch, and it will be necessary to use information from previously encountered datasets (much as you don’t learn English from scratch every time you open a new book—that would be impossible). Training models from scratch on every new task is also inefficient due to the large overlap between the current tasks and previously encountered tasks.</p>

  <p class="body"><a id="pgfId-1017315"></a>A remarkable observation has been made repeatedly in recent years: training the <i class="fm-italics">same</i> model to do several loosely connected tasks at the same time results in a model that’s <i class="fm-italics">better at each task</i>. For instance, training the same neural machine-translation model to perform both English-to-German translation and French-to-Italian translation will result in a model that’s better at each language pair. Similarly, training an image-classification model jointly with an image-segmentation model, sharing the same convolutional base, results in a model that’s better at both tasks. This is fairly intuitive: there’s always <i class="fm-italics">some</i> information overlap between seemingly disconnected tasks, and a joint model has access to a greater amount of information about each individual task than a model trained on that specific task only.</p>

  <p class="body"><a id="pgfId-1017324"></a>Currently, when it comes to model reuse across tasks, we use pretrained weights for models that perform common functions, such as visual feature extraction. You saw this in action in chapter 9. In the future, I expect a generalized version of this to be commonplace: we’ll use not only previously learned features (submodel weights) but also model architectures and training procedures. As models become more like programs, we’ll begin to reuse <i class="fm-italics">program subroutines</i> like the <a id="marker-1017335"></a>functions and classes found in human programming languages.</p>

  <p class="body"><a id="pgfId-1017345"></a>Think of the process of software development today: once an engineer solves a specific problem (HTTP queries in Python, for instance), they package it as an abstract, reusable library. Engineers who face a similar problem in the future will be able to search for existing libraries, download one, and use it in their own project. In a similar way, in the future, meta-learning systems will be able to assemble new programs by sifting through a global library of high-level reusable blocks. When the system finds itself developing similar program subroutines for several different tasks, it can come up with an <i class="fm-italics">abstract</i>, reusable version of the subroutine and store it in the global library (see figure 14.13). These subroutines can be either geometric (deep learning modules with pretrained representations) or algorithmic (closer to the libraries that contemporary software engineers manipulate). <a id="marker-1017356"></a><a id="marker-1017359"></a></p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/14-13.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1060838"></a>Figure 14.13 A meta-learner capable of quickly developing task-specific models using reusable primitives (both algorithmic and geometric), thus achieving extreme generalization</p>

  <h3 class="fm-head1" id="heading_id_28"><a id="pgfId-1017377"></a>14.5.4 The long-term vision</h3>

  <p class="body"><a id="pgfId-1017401"></a>In short, here’s my long-term vision for machine learning:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1017410"></a>Models will be more like programs and will have capabilities that go far beyond the continuous geometric transformations of the input data we currently work with. These programs will arguably be much closer to the abstract mental models that humans maintain about their surroundings and themselves, and they will be capable of stronger generalization due to their rich algorithmic nature.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1017450"></a>In particular, models will blend <i class="fm-italics1">algorithmic modules</i> providing formal <a class="calibre11" id="marker-1017439"></a>reasoning, search, and abstraction capabilities with <i class="fm-italics1">geometric modules</i> providing informal <a class="calibre11" id="marker-1017455"></a>intuition and pattern-recognition capabilities. This will achieve a blend of value-centric and program-centric abstraction. AlphaGo or self-driving cars (systems that required a lot of manual software engineering and human-made design decisions) provide an early example of what such a blend of symbolic and geometric AI could look like.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1017465"></a>Such models will be <i class="fm-italics1">grown</i> automatically rather than hardcoded by human engineers, using modular parts stored in a global library of reusable subroutines—a library evolved by learning high-performing models on thousands of previous tasks and datasets. As frequent problem-solving patterns are identified by the meta-learning system, they will be turned into reusable subroutines—much like functions and classes in software engineering—and added to the global library.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1017484"></a>The process that searches over possible combinations of subroutines to grow new models will be a discrete search process (program synthesis), but it will be heavily guided by a form of <i class="fm-italics1">program-space intuition</i> provided by <a class="calibre11" id="marker-1017499"></a>deep learning.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1017509"></a>This global subroutine library and associated model-growing system will be able to achieve some form of human-like <i class="fm-italics1">extreme generalization</i>: given a new task or situation, the system will be able to assemble a new working model appropriate for the task using very little data, thanks to rich program-like primitives that generalize well and extensive experience with similar tasks. In the same way, humans can quickly learn to play a complex new video game if they have experience with many previous games, because the models derived from this previous experience are abstract and program-like, rather than a basic mapping between stimuli and action.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1017545"></a>As such, this perpetually learning model-growing system can be interpreted as <a class="calibre11" id="marker-1017534"></a>an <i class="fm-italics1">artificial general intelligence</i> (AGI). But don’t expect any singularitarian robot apocalypse to ensue: that’s pure fantasy, coming from a long series of profound misunderstandings of both intelligence and technology. Such a critique, however, doesn’t belong in this book.</p>
    </li>
  </ul>

  <h2 class="fm-head" id="heading_id_29"><a id="pgfId-1017559"></a>14.6 Staying up to date in a fast-moving field</h2>

  <p class="body"><a id="pgfId-1017569"></a>As final parting words, I want to give you some pointers about how to keep learning and updating your knowledge and skills after you’ve turned the last page of this book. The field of modern deep learning, as we know it today, is only a few years old, despite a long, slow prehistory stretching back decades. With an exponential increase in financial resources and research headcount since 2013, the field as a whole is now moving at a frenetic pace. What you’ve learned in this book won’t stay relevant forever, and it isn’t all you’ll need for the rest of your career.</p>

  <p class="body"><a id="pgfId-1017578"></a>Fortunately, there are plenty of free online resources that you can use to stay up to date and expand your horizons. Here are a few.</p>

  <h3 class="fm-head1" id="heading_id_30"><a id="pgfId-1017584"></a>14.6.1 Practice on real-world problems using Kaggle</h3>

  <p class="body"><a id="pgfId-1017601"></a><a id="marker-1017595"></a>An effective way to acquire real-world experience is to try your hand at machine learning competitions on Kaggle (<span class="fm-hyperlink"><a class="url" href="https://kaggle.com">https://kaggle.com</a></span>). The only real way to learn is through practice and actual coding—that’s the philosophy of this book, and Kaggle competitions are the natural continuation of this. On Kaggle, you’ll find an array of constantly renewed data science competitions, many of which involve deep learning, prepared by companies interested in obtaining novel solutions to some of their most challenging machine learning problems. Fairly large monetary prizes are offered to top entrants.</p>

  <p class="body"><a id="pgfId-1017607"></a>Most competitions are won using either the XGBoost <a id="marker-1017609"></a>library (for shallow machine learning) or Keras (for deep learning). So you’ll fit right in! By participating in a few competitions, maybe as part of a team, you’ll become more familiar with the practical side of some of the advanced best practices described in this book, especially hyperparameter tuning, avoiding validation-set overfitting, and model ensembling. <a id="marker-1017615"></a></p>

  <h3 class="fm-head1" id="heading_id_31"><a id="pgfId-1017624"></a>14.6.2 Read about the latest developments on arXiv</h3>

  <p class="body"><a id="pgfId-1017648"></a><a id="marker-1058248"></a>Deep learning research, in contrast with some other scientific fields, takes places completely in the open. Papers are made publicly and freely accessible as soon as they’re finalized, and a lot of related software is open source. arXiv (<span class="fm-hyperlink"><a class="url" href="https://arxiv.org">https://arxiv.org</a></span>)—pronounced “archive” (the X stands for the Greek <i class="fm-italics">chi</i>)—is an open-access preprint server for physics, mathematics, and computer science research papers. It has become the de facto way to stay up to date on the bleeding edge of machine learning and deep learning. The large majority of deep learning researchers upload any paper they write to arXiv shortly after completion. This allows them to plant a flag and claim a specific finding without waiting for a conference acceptance (which takes months), which is necessary given the fast pace of research and the intense competition in the field. It also allows the field to move extremely fast: all new findings are immediately available for all to see and to build on.</p>

  <p class="body"><a id="pgfId-1017657"></a>An important downside is that the sheer quantity of new papers posted every day on arXiv makes it impossible to even skim them all, and the fact that they aren’t peer-reviewed makes it difficult to identify those that are both important and high quality. It’s challenging, and becoming increasingly more so, to find the signal in the noise. But some tools can help: in particular, you can use Google Scholar (<span class="fm-hyperlink"><a class="url" href="https://scholar.google.com">https://scholar.google.com</a></span>) to keep track of publications by your favorite authors. <a id="marker-1057438"></a></p>

  <h3 class="fm-head1" id="heading_id_32"><a id="pgfId-1017669"></a>14.6.3 Explore the Keras ecosystem</h3>

  <p class="body"><a id="pgfId-1017690"></a><a id="marker-1017680"></a><a id="marker-1017684"></a><a id="marker-1017686"></a>With over one million users as of late 2021 and still growing, Keras has a large ecosystem of tutorials, guides, and related open source projects:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1017695"></a>Your main reference for working with Keras is the online documentation at <span class="fm-hyperlink1"><a class="url1" href="https://keras.io">https://keras.io</a></span>. In particular, you’ll find extensive developer guides at <span class="fm-hyperlink1"><a class="url1" href="https://keras.io/guides">https://keras.io/guides</a></span>, and you’ll find dozens of high-quality Keras code examples at <span class="fm-hyperlink1"><a class="url1" href="https://keras.io/examples">https://keras.io/examples</a></span>. Make sure to check them out!</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018043"></a>The Keras source code can be found at <span class="fm-hyperlink1"><a class="url1" href="https://github.com/keras-team/keras">https://github.com/keras-team/keras</a></span>.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1020393"></a>You can ask for help and join deep learning discussions on the Keras mailing list: <a class="url1" href="mailto:keras-users@googlegroups.com">keras-users@googlegroups.com</a>.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1020404"></a>You can follow me on Twitter: <span class="fm-hyperlink1">@fchollet</span>.<a class="calibre11" id="marker-1020419"></a></p>
    </li>
  </ul>

  <h2 class="fm-head" id="heading_id_33"><a id="pgfId-1020430"></a>Final words</h2>

  <p class="body"><a id="pgfId-1020440"></a>This is the end of <i class="fm-italics">Deep Learning with Python,</i> second edition. I hope you’ve learned a thing or two about machine learning, deep learning, Keras, and maybe even cognition in general. Learning is a lifelong journey, especially in the field of AI, where we have far more unknowns on our hands than certitudes. So please go on learning, questioning, and researching. Never stop! Because even given the progress made so far, most of the fundamental questions in AI remain unanswered. Many haven’t even been properly asked yet.</p>
  <hr class="calibre15"/>

  <p class="fm-footnote"><a href="../Text/14.htm#Id-1012116"><sup class="footnotenumber1">1</sup></a> <a id="pgfId-1012116"></a>Richard Feynman, interview, “The World from Another Point of View,” Yorkshire Television, 1972.</p>

  <p class="fm-footnote"><a href="../Text/14.htm#Id-1015384"><sup class="footnotenumber1">2</sup></a> <a id="pgfId-1015384"></a>Terry Winograd, “Procedures as a Representation for Data in a Computer Program for Understanding Natural Language” (1971).</p>

  <p class="fm-footnote"><a href="../Text/14.htm#Id-1015428"><sup class="footnotenumber1">3</sup></a> <a id="pgfId-1015428"></a>Fast Company, “Wozniak: Could a Computer Make a Cup of Coffee?” (March 2010), <span class="fm-hyperlink"><a class="url" href="http://mng.bz/pJMP">http://mng.bz/pJMP</a></span>.</p>

  <p class="fm-footnote"><a href="../Text/14.htm#Id-1015759"><sup class="footnotenumber1">4</sup></a> <a id="pgfId-1015759"></a>François Chollet, “On the Measure of Intelligence” (2019), <span class="fm-hyperlink"><a class="url" href="https://arxiv.org/abs/1911.01547">https://arxiv.org/abs/1911.01547</a></span>.</p>
</body>
</html>
