<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>2</title>
    
  <link href="../../stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="../../page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
  <h1 class="tochead" id="heading_id_2"><a id="pgfId-998407"></a><a id="pgfId-1026036"></a>2 The mathematical building blocks of neural networks</h1>

  <p class="co-summary-head"><a id="pgfId-1011754"></a>This chapter covers</p>

  <ul class="calibre10">
    <li class="co-summary-bullet"><a class="calibre11" id="pgfId-1011760"></a>A first example of a neural network</li>

    <li class="co-summary-bullet"><a class="calibre11" id="pgfId-1011774"></a>Tensors and tensor operations</li>

    <li class="co-summary-bullet"><a class="calibre11" id="pgfId-1011784"></a>How neural networks learn via backpropagation and gradient descent</li>
  </ul>

  <p class="body"><a id="pgfId-1011837"></a>Understanding deep learning requires familiarity with many simple <a id="marker-1011796"></a>mathematical concepts: <i class="fm-italics">tensors</i>, <i class="fm-italics">tensor operations</i>, <i class="fm-italics">differentiation</i>, <i class="fm-italics">gradient descent</i>, and so on. Our goal in this chapter will be to build up your intuition about these notions without getting overly technical. In particular, we’ll steer away from mathematical notation, which can introduce unnecessary barriers for those without any mathematics background and isn’t necessary to explain things well. The most precise, unambiguous description of a mathematical operation is its executable code.</p>

  <p class="body"><a id="pgfId-1011846"></a>To provide sufficient context for introducing tensors and gradient descent, we’ll begin the chapter with a practical example of a neural network. Then we’ll go over every new concept that’s been introduced, point by point. Keep in mind that these concepts will be essential for you to understand the practical examples in the following chapters!</p>

  <p class="body"><a id="pgfId-1011852"></a>After reading this chapter, you’ll have an intuitive understanding of the mathematical theory behind deep learning, and you’ll be ready to start diving into Keras and TensorFlow in chapter 3.</p>

  <h2 class="fm-head" id="heading_id_3"><a id="pgfId-1011858"></a>2.1 A first look at a neural network</h2>

  <p class="body"><a id="pgfId-1011868"></a><a id="marker-1011869"></a>Let’s look at a concrete example of a neural network that uses the Python library Keras to learn to classify handwritten digits. Unless you already have experience with Keras or similar libraries, you won’t understand everything about this first example right away. That’s fine. In the next chapter, we’ll review each element in the example and explain them in detail. So don’t worry if some steps seem arbitrary or look like magic to you! We’ve got to start somewhere.</p>

  <p class="body"><a id="pgfId-1011877"></a>The problem we’re trying to solve here is to classify grayscale images of handwritten digits (28 × 28 pixels) into their 10 categories (0 through 9). We’ll use the MNIST dataset, a classic in the machine learning community, which has been around almost as long as the field itself and has been intensively studied. It’s a set of 60,000 training images, plus 10,000 test images, assembled by the National Institute of Standards and Technology (the NIST in MNIST) in the 1980s. You can think of “solving” MNIST as the “Hello World” of deep learning—it’s what you do to verify that your algorithms are working as expected. As you become a machine learning practitioner, you’ll see MNIST come up over and over again in scientific papers, blog posts, and so on. You can see some MNIST samples in figure 2.1.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/02-01.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1041427"></a>Figure 2.1 MNIST sample digits</p>

  <p class="fm-callout"><a id="pgfId-1011893"></a><span class="fm-callout-head">Note</span> In machine learning, a <i class="fm-italics">category</i> in a <a id="marker-1011928"></a>classification problem is called a <i class="fm-italics">class</i>. Data points <a id="marker-1011944"></a>are called <i class="fm-italics">samples</i>. The class <a id="marker-1011960"></a>associated with a specific sample is <a id="marker-1011966"></a>called a <i class="fm-italics">label</i>.</p>

  <p class="body"><a id="pgfId-1011986"></a>You don’t need to try to reproduce this example on your machine just now. If you wish to, you’ll first need to set up a deep learning workspace, which is covered in chapter 3.</p>

  <p class="body"><a id="pgfId-1011992"></a>The MNIST dataset comes preloaded in Keras, in the form of a set of four NumPy arrays.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1012049"></a>Listing 2.1 Loading the MNIST dataset in Keras</p>
  <pre class="programlisting"><a id="pgfId-1032948"></a><b class="fm-codebrown">from</b> tensorflow.keras.datasets <b class="fm-codebrown">import</b> mnist
<a id="pgfId-1012088"></a>(train_images, train_labels), (test_images, test_labels) = mnist.load_data()</pre>

  <p class="body"><a id="pgfId-1012128"></a><code class="fm-code-in-text">train_images</code> and <code class="fm-code-in-text">train_labels</code> form the training set, the data that the model will learn from. The model will then be tested on the test set, <code class="fm-code-in-text">test_images</code> and <code class="fm-code-in-text">test_labels</code>. The images are encoded as NumPy arrays, and the labels are an array of digits, ranging from 0 to 9. The images and labels have a one-to-one correspondence.</p>

  <p class="body"><a id="pgfId-1012137"></a>Let’s look at the training data:</p>
  <pre class="programlisting"><a id="pgfId-1032961"></a>&gt;&gt;&gt; train_images.shape
<a id="pgfId-1032962"></a>(60000, 28, 28)
<a id="pgfId-1032963"></a>&gt;&gt;&gt; len(train_labels) 
<a id="pgfId-1032964"></a>60000 
<a id="pgfId-1032965"></a>&gt;&gt;&gt; train_labels
<a id="pgfId-1012181"></a>array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)</pre>

  <p class="body"><a id="pgfId-1012187"></a>And here’s the test data:</p>
  <pre class="programlisting"><a id="pgfId-1032978"></a>&gt;&gt;&gt; test_images.shape
<a id="pgfId-1032979"></a>(10000, 28, 28)
<a id="pgfId-1032980"></a>&gt;&gt;&gt; len(test_labels) 
<a id="pgfId-1032981"></a>10000 
<a id="pgfId-1032982"></a>&gt;&gt;&gt; test_labels
<a id="pgfId-1012231"></a>array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)</pre>

  <p class="body"><a id="pgfId-1012273"></a>The workflow will be as follows: First, we’ll feed the neural network the training data, <code class="fm-code-in-text">train_images</code> and <code class="fm-code-in-text">train_labels</code>. The network will then learn to associate images and labels. Finally, we’ll ask the network to produce predictions for <code class="fm-code-in-text">test_images</code>, and we’ll verify whether these predictions match the labels from <code class="fm-code-in-text">test_labels</code>.</p>

  <p class="body"><a id="pgfId-1012282"></a>Let’s build the network—again, remember that you aren’t expected to understand everything about this example yet.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1012339"></a>Listing 2.2 The network architecture</p>
  <pre class="programlisting"><a id="pgfId-1032995"></a><b class="fm-codebrown">from</b> tensorflow <b class="fm-codebrown">import</b> keras 
<a id="pgfId-1032996"></a><b class="fm-codebrown">from</b> tensorflow.keras <b class="fm-codebrown">import</b> layers
<a id="pgfId-1032997"></a>model = keras.Sequential([
<a id="pgfId-1032998"></a>    layers.Dense(<span class="fm-codeblue">512</span>, activation=<span class="fm-codegreen">"relu"</span>),
<a id="pgfId-1032999"></a>    layers.Dense(<span class="fm-codeblue">10</span>, activation=<span class="fm-codegreen">"softmax"</span>)
<a id="pgfId-1012402"></a>])</pre>

  <p class="body"><a id="pgfId-1012446"></a>The core building block of neural networks is the <i class="fm-italics">layer</i>. You can <a id="marker-1012419"></a>think of a layer as a filter for data: some data goes in, and it comes out in a more useful form. Specifically, layers extract <i class="fm-italics">representations</i> out of the data fed into them—hopefully, representations that are more meaningful for the problem at hand. Most of deep learning consists of chaining together simple layers that will implement a form of <a id="marker-1012435"></a>progressive <i class="fm-italics">data distillation</i>. A deep learning model is like a sieve for data processing, made of a succession of increasingly refined data filters—the layers.</p>

  <p class="body"><a id="pgfId-1012487"></a>Here, our model consists of a sequence of two <code class="fm-code-in-text">Dense</code> layers, which are <a id="marker-1012466"></a>densely connected (also called <i class="fm-italics">fully connected</i>) neural layers. The second (and last) layer is a 10-way <i class="fm-italics">softmax classification</i> layer, which means <a id="marker-1012492"></a>it will return an array of 10 probability scores (summing to 1). Each score will be the probability that the current digit image belongs to one of our 10 digit classes.</p>

  <p class="body"><a id="pgfId-1012515"></a>To make the model ready for training, we need to pick three more things as part of <a id="marker-1012504"></a>the <i class="fm-italics">compilation</i> step:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012524"></a><i class="fm-italics1">An optimizer</i>—The mechanism <a class="calibre11" id="marker-1012541"></a>through which the model will update itself based on the training data it sees, so as to improve its performance.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012551"></a><i class="fm-italics1">A loss function</i>—How the <a class="calibre11" id="marker-1012564"></a>model will be able to measure its performance on the training data, and thus how it will be able to steer itself in the right direction.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012574"></a><i class="fm-italics1">Metrics to monitor during training and testing</i>—Here, we’ll only care about accuracy (the fraction of the images that were correctly classified).</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1012591"></a>The exact purpose of the loss function and the optimizer will be made clear throughout the next two chapters.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1012648"></a>Listing 2.3 The compilation step</p>
  <pre class="programlisting"><a id="pgfId-1033012"></a>model.compile(optimizer=<span class="fm-codegreen">"rmsprop"</span>,
<a id="pgfId-1033013"></a>              loss=<span class="fm-codegreen">"sparse_categorical_crossentropy"</span>,
<a id="pgfId-1012693"></a>              metrics=[<span class="fm-codegreen">"accuracy"</span>])</pre>

  <p class="body"><a id="pgfId-1012755"></a>Before training, we’ll preprocess the data by reshaping it into the shape the model expects and scaling it so that all values are in the <code class="fm-code-in-text">[0,</code> <code class="fm-code-in-text">1]</code> interval. Previously, our training images were stored in an array of shape <code class="fm-code-in-text">(60000,</code> <code class="fm-code-in-text">28,</code> <code class="fm-code-in-text">28)</code> of type <code class="fm-code-in-text">uint8</code> with values in the <code class="fm-code-in-text">[0,</code> <code class="fm-code-in-text">255]</code> interval. We’ll transform it into a <code class="fm-code-in-text">float32</code> array of shape <code class="fm-code-in-text">(60000,</code> <code class="fm-code-in-text">28</code> <code class="fm-code-in-text">*</code> <code class="fm-code-in-text">28)</code> with values between 0 and 1.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1012815"></a>Listing 2.4 Preparing the image data</p>
  <pre class="programlisting"><a id="pgfId-1033056"></a>train_images = train_images.reshape((<span class="fm-codeblue">60000</span>, <span class="fm-codeblue">28</span> * <span class="fm-codeblue">28</span>))
<a id="pgfId-1033057"></a>train_images = train_images.astype(<span class="fm-codegreen">"float32"</span>) / <span class="fm-codeblue">255</span> 
<a id="pgfId-1033058"></a>test_images = test_images.reshape((<span class="fm-codeblue">10000</span>, <span class="fm-codeblue">28</span> * <span class="fm-codeblue">28</span>))
<a id="pgfId-1012866"></a>test_images = test_images.astype(<span class="fm-codegreen">"float32"</span>) / <span class="fm-codeblue">255</span></pre>

  <p class="body"><a id="pgfId-1012895"></a>We’re now ready to train the model, which in Keras is done via a call to <a id="marker-1033046"></a>the model’s <code class="fm-code-in-text">fit()</code> method—we <i class="fm-italics">fit</i> the model to its training data.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1012955"></a>Listing 2.5 “Fitting” the model</p>
  <pre class="programlisting"><a id="pgfId-1033073"></a>&gt;&gt;&gt; model.fit(train_images, train_labels, epochs=<span class="fm-codeblue">5</span>, batch_size=<span class="fm-codeblue">128</span>)
<a id="pgfId-1033074"></a>Epoch 1/5 
<a id="pgfId-1033075"></a>60000/60000 [===========================] - 5s - loss: 0.2524 - acc: 0.9273 
<a id="pgfId-1033076"></a>Epoch 2/5 
<a id="pgfId-1013012"></a>51328/60000 [=====================&gt;.....] - ETA: 1s - loss: 0.1035 - acc: 0.9692</pre>

  <p class="body"><a id="pgfId-1013018"></a>Two quantities are displayed during training: the loss of the model over the training data, and the accuracy of the model over the training data. We quickly reach an accuracy of 0.989 (98.9%) on the training data.</p>

  <p class="body"><a id="pgfId-1013024"></a>Now that we have a trained model, we can use it to predict class probabilities for <i class="fm-italics">new</i> digits—images that weren’t part of the training data, like those from the test set.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1013090"></a>Listing 2.6 Using the model to make predictions</p>
  <pre class="programlisting"><a id="pgfId-1036269"></a>&gt;&gt;&gt; test_digits = test_images[<span class="fm-codeblue">0</span>:<span class="fm-codeblue">10</span>]
<a id="pgfId-1036270"></a>&gt;&gt;&gt; predictions = model.predict(test_digits)
<a id="pgfId-1033098"></a>&gt;&gt;&gt; predictions[<span class="fm-codeblue">0</span>]
<a id="pgfId-1033099"></a>array([1.0726176e-10, 1.6918376e-10, 6.1314843e-08, 8.4106023e-06,
<a id="pgfId-1033100"></a>       2.9967067e-11, 3.0331331e-09, 8.3651971e-14, 9.9999106e-01,
<a id="pgfId-1013153"></a>       2.6657624e-08, 3.8127661e-07], dtype=float32)</pre>

  <p class="body"><a id="pgfId-1013185"></a>Each number of index <code class="fm-code-in-text">i</code> in that array corresponds to the probability that digit image <code class="fm-code-in-text">test_digits[0]</code> belongs to class <code class="fm-code-in-text">i</code>.</p>

  <p class="body"><a id="pgfId-1013194"></a>This first test digit has the highest probability score (0.99999106, almost 1) at index 7, so according to our model, it must be a 7:</p>
  <pre class="programlisting"><a id="pgfId-1033119"></a>&gt;&gt;&gt; predictions[<span class="fm-codeblue">0</span>].argmax() 
<a id="pgfId-1033120"></a>7 
<a id="pgfId-1033121"></a>&gt;&gt;&gt; predictions[<span class="fm-codeblue">0</span>][<span class="fm-codeblue">7</span>] 
<a id="pgfId-1013226"></a>0.99999106</pre>

  <p class="body"><a id="pgfId-1013232"></a>We can check that the test label agrees:</p>
  <pre class="programlisting"><a id="pgfId-1033138"></a>&gt;&gt;&gt; test_labels[<span class="fm-codeblue">0</span>] 
<a id="pgfId-1013252"></a>7 </pre>

  <p class="body"><a id="pgfId-1013258"></a>On average, how good is our model at classifying such never-before-seen digits? Let’s check by computing average accuracy over the entire test set.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1013315"></a>Listing 2.7 Evaluating the model on new data</p>
  <pre class="programlisting"><a id="pgfId-1036337"></a>&gt;&gt;&gt; test_loss, test_acc = model.evaluate(test_images, test_labels)
<a id="pgfId-1033154"></a>&gt;&gt;&gt; <b class="fm-codebrown">print</b>(f<span class="fm-codegreen">"test_acc: {test_acc}"</span>)
<a id="pgfId-1013360"></a>test_acc: 0.9785</pre>

  <p class="body"><a id="pgfId-1013366"></a>The test-set accuracy turns out to be 97.8%—that’s quite a bit lower than the training-set accuracy (98.9%). This gap between training accuracy and test accuracy is an example of <i class="fm-italics">overfitting</i>: the fact <a id="marker-1013377"></a>that machine learning models tend to perform worse on new data than on their training data. Overfitting is a central topic in chapter 3.</p>

  <p class="body"><a id="pgfId-1013387"></a>This concludes our first example—you just saw how you can build and train a neural network to classify handwritten digits in less than 15 lines of Python code. In this chapter and the next, we’ll go into detail about every moving piece we just previewed and clarify what’s going on behind the scenes. You’ll learn about tensors, the data-storing objects going into the model; tensor operations, which layers are made of; and gradient descent, which allows your model to learn from its training examples. <a id="marker-1013389"></a></p>

  <h2 class="fm-head" id="heading_id_4"><a id="pgfId-1013396"></a>2.2 Data representations for neural networks</h2>

  <p class="body"><a id="pgfId-1013419"></a><a id="marker-1013407"></a><a id="marker-1013409"></a>In the previous example, we started from data stored in multidimensional NumPy arrays, also called <i class="fm-italics">tensors</i>. In general, all current machine learning systems use tensors as their basic data structure. Tensors are fundamental to the field—so fundamental that TensorFlow was named after them. So what’s a tensor?</p>

  <p class="body"><a id="pgfId-1013451"></a>At its core, a tensor is a container for data—usually numerical data. So, it’s a container for numbers. You may be already familiar with matrices, which are rank-2 tensors: tensors are a generalization of matrices to an arbitrary number <a id="marker-1013430"></a>of <i class="fm-italics">dimensions</i> (note that in the context of tensors, a dimension is often called an <i class="fm-italics">axis</i>).</p>

  <h3 class="fm-head1" id="heading_id_5"><a id="pgfId-1013460"></a>2.2.1 Scalars (rank-0 tensors)</h3>

  <p class="body"><a id="pgfId-1013541"></a><a id="marker-1036186"></a><a id="marker-1036187"></a><a id="marker-1036188"></a>A tensor that contains only one number is called a <i class="fm-italics">scalar</i> (or scalar tensor, or rank-0 tensor, or 0D tensor). In NumPy, a <code class="fm-code-in-text">float32</code> or <code class="fm-code-in-text">float64</code> number is a scalar tensor (or scalar array). You can display the number of axes of a NumPy tensor via the <code class="fm-code-in-text">ndim</code> attribute; a scalar tensor <a id="marker-1036190"></a>has 0 axes (<code class="fm-code-in-text">ndim</code> <code class="fm-code-in-text">==</code> <code class="fm-code-in-text">0</code>). The number of axes of a tensor is also called its <i class="fm-italics">rank</i>. Here’s a NumPy scalar:<a id="marker-1036191"></a><a id="marker-1036192"></a><a id="marker-1036193"></a></p>
  <pre class="programlisting"><a id="pgfId-1036203"></a>&gt;&gt;&gt; <b class="fm-codebrown">import</b> numpy <b class="fm-codebrown">as</b> np
<a id="pgfId-1036204"></a>&gt;&gt;&gt; x = np.array(<span class="fm-codeblue">12</span>)
<a id="pgfId-1033175"></a>&gt;&gt;&gt; x
<a id="pgfId-1033176"></a>array(12)
<a id="pgfId-1033177"></a>&gt;&gt;&gt; x.ndim 
<a id="pgfId-1013595"></a>0</pre>

  <h3 class="fm-head1" id="heading_id_6"><a id="pgfId-1013601"></a>2.2.2 Vectors (rank-1 tensors)</h3>

  <p class="body"><a id="pgfId-1013620"></a><a id="marker-1013612"></a><a id="marker-1013614"></a><a id="marker-1013616"></a>An array of numbers is called a <i class="fm-italics">vector</i>, or rank-1 tensor, or 1D tensor. A rank-1 tensor is said to have exactly one axis. Following is a NumPy vector:</p>
  <pre class="programlisting"><a id="pgfId-1036217"></a>&gt;&gt;&gt; x = np.array([<span class="fm-codeblue">12</span>, <span class="fm-codeblue">3</span>, <span class="fm-codeblue">6</span>, <span class="fm-codeblue">14</span>, <span class="fm-codeblue">7</span>])
<a id="pgfId-1033195"></a>&gt;&gt;&gt; x
<a id="pgfId-1033196"></a>array([12, 3, 6, 14, 7])
<a id="pgfId-1033197"></a>&gt;&gt;&gt; x.ndim 
<a id="pgfId-1013657"></a>1</pre>

  <p class="body"><a id="pgfId-1013712"></a>This vector has five entries and so is called a <i class="fm-italics">5-dimensional vector</i>. Don’t confuse a 5D vector with a 5D tensor! A 5D vector has only one axis and has five dimensions along its axis, whereas a 5D tensor has five axes (and may have any number of dimensions along each axis). <i class="fm-italics">Dimensionality</i> can denote <a id="marker-1013691"></a>either the number of entries along a specific axis (as in the case of our 5D vector) or the number of axes in a tensor (such as a 5D tensor), which can be confusing at times. In the latter case, it’s technically more correct to talk about a <i class="fm-italics">tensor of rank 5</i> (the rank of a tensor being the number of axes), but the ambiguous notation <i class="fm-italics">5D tensor</i> is common regardless. <a id="marker-1013723"></a><a id="marker-1013726"></a><a id="marker-1013728"></a></p>

  <h3 class="fm-head1" id="heading_id_7"><a id="pgfId-1013734"></a>2.2.3 Matrices (rank-2 tensors)</h3>

  <p class="body"><a id="pgfId-1013779"></a><a id="marker-1013745"></a><a id="marker-1013747"></a><a id="marker-1013749"></a>An array of vectors is a <i class="fm-italics">matrix</i>, or rank-2 tensor, or 2D tensor. A matrix has two axes (often referred to as <i class="fm-italics">rows</i> and <i class="fm-italics">columns</i>). You can visually interpret a matrix as a rectangular grid of numbers. This is a NumPy matrix:</p>
  <pre class="programlisting"><a id="pgfId-1033212"></a>&gt;&gt;&gt; x = np.array([[<span class="fm-codeblue">5</span>, <span class="fm-codeblue">78</span>, <span class="fm-codeblue">2</span>, <span class="fm-codeblue">34</span>, <span class="fm-codeblue">0</span>],
<a id="pgfId-1033213"></a>                  [<span class="fm-codeblue">6</span>, <span class="fm-codeblue">79</span>, <span class="fm-codeblue">3</span>, <span class="fm-codeblue">35</span>, <span class="fm-codeblue">1</span>],
<a id="pgfId-1033214"></a>                  [<span class="fm-codeblue">7</span>, <span class="fm-codeblue">80</span>, <span class="fm-codeblue">4</span>, <span class="fm-codeblue">36</span>, <span class="fm-codeblue">2</span>]])
<a id="pgfId-1033215"></a>&gt;&gt;&gt; x.ndim 
<a id="pgfId-1013820"></a>2</pre>

  <p class="body"><a id="pgfId-1013884"></a>The entries from the first axis are called the <i class="fm-italics">rows</i>, and the <a id="marker-1013837"></a>entries from the second axis are called the <i class="fm-italics">columns</i>. In the <a id="marker-1013853"></a>previous example, <code class="fm-code-in-text">[5,</code> <code class="fm-code-in-text">78,</code> <code class="fm-code-in-text">2,</code> <code class="fm-code-in-text">34,</code> <code class="fm-code-in-text">0]</code> is the first row of <code class="fm-code-in-text">x</code>, and <code class="fm-code-in-text">[5,</code> <code class="fm-code-in-text">6,</code> <code class="fm-code-in-text">7]</code> is the first column. <a id="marker-1013889"></a><a id="marker-1013892"></a><a id="marker-1013894"></a></p>

  <h3 class="fm-head1" id="heading_id_8"><a id="pgfId-1013900"></a>2.2.4 Rank-3 and higher-rank tensors</h3>

  <p class="body"><a id="pgfId-1013917"></a><a id="marker-1013911"></a><a id="marker-1013913"></a>If you pack such matrices in a new array, you obtain a rank-3 tensor (or 3D tensor), which you can visually interpret as a cube of numbers. Following is a NumPy rank-3 tensor:</p>
  <pre class="programlisting"><a id="pgfId-1033234"></a>&gt;&gt;&gt; x = np.array([[[<span class="fm-codeblue">5</span>, <span class="fm-codeblue">78</span>, <span class="fm-codeblue">2</span>, <span class="fm-codeblue">34</span>, <span class="fm-codeblue">0</span>],
<a id="pgfId-1033235"></a>                   [<span class="fm-codeblue">6</span>, <span class="fm-codeblue">79</span>, <span class="fm-codeblue">3</span>, <span class="fm-codeblue">35</span>, <span class="fm-codeblue">1</span>],
<a id="pgfId-1033236"></a>                   [<span class="fm-codeblue">7</span>, <span class="fm-codeblue">80</span>, <span class="fm-codeblue">4</span>, <span class="fm-codeblue">36</span>, <span class="fm-codeblue">2</span>]],
<a id="pgfId-1033237"></a>                  [[<span class="fm-codeblue">5</span>, <span class="fm-codeblue">78</span>, <span class="fm-codeblue">2</span>, <span class="fm-codeblue">34</span>, <span class="fm-codeblue">0</span>],
<a id="pgfId-1033238"></a>                   [<span class="fm-codeblue">6</span>, <span class="fm-codeblue">79</span>, <span class="fm-codeblue">3</span>, <span class="fm-codeblue">35</span>, <span class="fm-codeblue">1</span>],
<a id="pgfId-1033239"></a>                   [<span class="fm-codeblue">7</span>, <span class="fm-codeblue">80</span>, <span class="fm-codeblue">4</span>, <span class="fm-codeblue">36</span>, <span class="fm-codeblue">2</span>]],
<a id="pgfId-1033240"></a>                  [[<span class="fm-codeblue">5</span>, <span class="fm-codeblue">78</span>, <span class="fm-codeblue">2</span>, <span class="fm-codeblue">34</span>, <span class="fm-codeblue">0</span>],
<a id="pgfId-1033241"></a>                   [<span class="fm-codeblue">6</span>, <span class="fm-codeblue">79</span>, <span class="fm-codeblue">3</span>, <span class="fm-codeblue">35</span>, <span class="fm-codeblue">1</span>],
<a id="pgfId-1033242"></a>                   [<span class="fm-codeblue">7</span>, <span class="fm-codeblue">80</span>, <span class="fm-codeblue">4</span>, <span class="fm-codeblue">36</span>, <span class="fm-codeblue">2</span>]]])
<a id="pgfId-1033243"></a>&gt;&gt;&gt; x.ndim 
<a id="pgfId-1013990"></a>3 </pre>

  <p class="body"><a id="pgfId-1013996"></a>By packing rank-3 tensors in an array, you can create a rank-4 tensor, and so on. In deep learning, you’ll generally manipulate tensors with ranks 0 to 4, although you may go up to 5 if you process video data. <a id="marker-1013998"></a><a id="marker-1014001"></a></p>

  <h3 class="fm-head1" id="heading_id_9"><a id="pgfId-1014007"></a>2.2.5 Key attributes</h3>

  <p class="body"><a id="pgfId-1014026"></a><a id="marker-1014018"></a><a id="marker-1014020"></a><a id="marker-1014022"></a>A tensor is defined by three key attributes:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1014059"></a><i class="fm-italics1">Number of axes (rank)</i>—For instance, a rank-3 tensor has <a class="calibre11" id="marker-1014048"></a>three axes, and a matrix has two axes. This is also called the tensor’s <code class="fm-code-in-text">ndim</code> in Python libraries such as NumPy <a class="calibre11" id="marker-1014070"></a>or TensorFlow.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1014128"></a><i class="fm-italics1">Shape</i>—This is a tuple of integers that describes how many dimensions the tensor has along each axis. For instance, the previous matrix example has shape <code class="fm-code-in-text">(3,</code> <code class="fm-code-in-text">5)</code>, and the rank-3 tensor example has shape <code class="fm-code-in-text">(3,</code> <code class="fm-code-in-text">3,</code> <code class="fm-code-in-text">5)</code>. A vector has a shape with a single element, such as <code class="fm-code-in-text">(5,)</code>, whereas a scalar has an empty shape, <code class="fm-code-in-text">()</code>.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1014210"></a><i class="fm-italics1">Data type (usually called</i> <code class="fm-code-in-text">dtype</code> <i class="fm-italics1">in Python libraries)</i>—This is <a class="calibre11" id="marker-1014159"></a>the type of the data contained in the tensor; for instance, a tensor’s type could be <code class="fm-code-in-text">float16</code>, <code class="fm-code-in-text">float32</code>, <code class="fm-code-in-text">float64</code>, <code class="fm-code-in-text">uint8</code>, and so on. In TensorFlow, you are also likely to come across <code class="fm-code-in-text">string</code> tensors.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1014219"></a>To make this more concrete, let’s look back at the data we processed in the MNIST example. First, we load the MNIST dataset:</p>
  <pre class="programlisting"><a id="pgfId-1033262"></a><b class="fm-codebrown">from</b> tensorflow.keras.datasets <b class="fm-codebrown">import</b> mnist
<a id="pgfId-1014239"></a>(train_images, train_labels), (test_images, test_labels) = mnist.load_data()</pre>

  <p class="body"><a id="pgfId-1014268"></a>Next, we display the number of axes of the <a id="marker-1014247"></a>tensor <code class="fm-code-in-text">train_images</code>, the <code class="fm-code-in-text">ndim</code> attribute:</p>
  <pre class="programlisting"><a id="pgfId-1033279"></a>&gt;&gt;&gt; train_images.ndim 
<a id="pgfId-1014291"></a>3 </pre>

  <p class="body"><a id="pgfId-1014297"></a>Here’s its shape:</p>
  <pre class="programlisting"><a id="pgfId-1033292"></a>&gt;&gt;&gt; train_images.shape
<a id="pgfId-1014317"></a>(60000, 28, 28)</pre>

  <p class="body"><a id="pgfId-1014336"></a>And this is its data <a id="marker-1014325"></a>type, the <code class="fm-code-in-text">dtype</code> attribute:</p>
  <pre class="programlisting"><a id="pgfId-1033307"></a>&gt;&gt;&gt; train_images.dtype
<a id="pgfId-1014359"></a>uint8</pre>

  <p class="body"><a id="pgfId-1014365"></a>So what we have here is a rank-3 tensor of 8-bit integers. More precisely, it’s an array of 60,000 matrices of 28 × 28 integers. Each such matrix is a grayscale image, with coefficients between 0 and 255.</p>

  <p class="body"><a id="pgfId-1014371"></a>Let’s display the fourth digit in this rank-3 tensor, using the Matplotlib library (a well-known Python data visualization library, which comes preinstalled in Colab); see figure 2.2.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/02-02.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1041487"></a>Figure 2.2 The fourth sample in our dataset</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1014428"></a>Listing 2.8 Displaying the fourth digit</p>
  <pre class="programlisting"><a id="pgfId-1033326"></a><b class="fm-codebrown">import</b> matplotlib.pyplot <b class="fm-codebrown">as</b> plt
<a id="pgfId-1033327"></a>digit = train_images[<span class="fm-codeblue">4</span>]
<a id="pgfId-1033328"></a>plt.imshow(digit, cmap=plt.cm.binary)
<a id="pgfId-1014479"></a>plt.show()</pre>

  <p class="body"><a id="pgfId-1014495"></a>Naturally, the corresponding label is the integer 9:<a id="marker-1014511"></a><a id="marker-1014514"></a><a id="marker-1014516"></a></p>
  <pre class="programlisting"><a id="pgfId-1033347"></a>&gt;&gt;&gt; train_labels[<span class="fm-codeblue">4</span>] 
<a id="pgfId-1014536"></a>9</pre>

  <h3 class="fm-head1" id="heading_id_10"><a id="pgfId-1014542"></a>2.2.6 Manipulating tensors in NumPy</h3>

  <p class="body"><a id="pgfId-1014583"></a><a id="marker-1014553"></a><a id="marker-1014555"></a><a id="marker-1014557"></a>In the previous example, we selected a specific digit alongside the first axis using the syntax <code class="fm-code-in-text">train_images[i]</code>. Selecting specific elements in a tensor is <a id="marker-1014572"></a>called <i class="fm-italics">tensor slicing</i>. Let’s look at the tensor-slicing operations you can do on NumPy arrays.</p>

  <p class="body"><a id="pgfId-1014592"></a>The following example selects digits #10 to #100 (#100 isn’t included) and puts them in an array of shape <code class="fm-code-in-text">(90,</code> <code class="fm-code-in-text">28,</code> <code class="fm-code-in-text">28)</code>:</p>
  <pre class="programlisting"><a id="pgfId-1036430"></a>&gt;&gt;&gt; my_slice = train_images[<span class="fm-codeblue">10</span>:<span class="fm-codeblue">100</span>]
<a id="pgfId-1033367"></a>&gt;&gt;&gt; my_slice.shape
<a id="pgfId-1014627"></a>(90, 28, 28)</pre>

  <p class="body"><a id="pgfId-1014633"></a>It’s equivalent to this more detailed notation, which specifies a start index and stop index for the slice along each tensor axis. Note that <code class="fm-code-in-text">:</code> is equivalent to selecting the entire axis:</p>
  <pre class="programlisting"><a id="pgfId-1033435"></a>&gt;&gt;&gt; my_slice = train_images[<span class="fm-codeblue">10</span>:<span class="fm-codeblue">100</span>, :, :]          <span class="fm-combinumeral">❶</span>
<a id="pgfId-1033436"></a>&gt;&gt;&gt; my_slice.shape
<a id="pgfId-1033437"></a>(90, 28, 28)
<a id="pgfId-1033438"></a>&gt;&gt;&gt; my_slice = train_images[<span class="fm-codeblue">10</span>:<span class="fm-codeblue">100</span>, <span class="fm-codeblue">0</span>:<span class="fm-codeblue">28</span>, <span class="fm-codeblue">0</span>:<span class="fm-codeblue">28</span>]    <span class="fm-combinumeral">❷</span>
<a id="pgfId-1033439"></a>&gt;&gt;&gt; my_slice.shape
<a id="pgfId-1033428"></a>(90, 28, 28)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1040229"></a><span class="fm-combinumeral">❶</span> Equivalent to the previous example</p>

  <p class="fm-code-annotation"><a id="pgfId-1040250"></a><span class="fm-combinumeral">❷</span> Also equivalent to the previous example</p>

  <p class="body"><a id="pgfId-1014740"></a>In general, you may select slices between any two indices along each tensor axis. For instance, in order to select 14 × 14 pixels in the bottom-right corner of all images, you would do this:</p>
  <pre class="programlisting"><a id="pgfId-1014746"></a>my_slice = train_images[:, <span class="fm-codeblue">14</span>:, <span class="fm-codeblue">14</span>:]</pre>

  <p class="body"><a id="pgfId-1014760"></a>It’s also possible to use negative indices. Much like negative indices in Python lists, they indicate a position relative to the end of the current axis. In order to crop the images to patches of 14 × 14 pixels centered in the middle, you’d do this:<a id="marker-1014762"></a><a id="marker-1014765"></a><a id="marker-1014767"></a></p>
  <pre class="programlisting"><a id="pgfId-1014773"></a>my_slice = train_images[:, <span class="fm-codeblue">7</span>:-<span class="fm-codeblue">7</span>, <span class="fm-codeblue">7</span>:-<span class="fm-codeblue">7</span>]</pre>

  <h3 class="fm-head1" id="heading_id_11"><a id="pgfId-1014787"></a>2.2.7 The notion of data batches</h3>

  <p class="body"><a id="pgfId-1014826"></a><a id="marker-1014798"></a><a id="marker-1014800"></a>In general, the first axis (axis 0, because indexing starts at 0) in all data tensors you’ll come across in deep learning will <a id="marker-1014805"></a>be the <i class="fm-italics">samples axis</i> (sometimes called the <i class="fm-italics">samples dimension</i>). In the MNIST example, “samples” are images of digits.</p>

  <p class="body"><a id="pgfId-1014835"></a>In addition, deep learning models don’t process an entire dataset at once; rather, they break the data into small batches. Concretely, here’s one batch of our MNIST digits, with a batch size of 128:</p>
  <pre class="programlisting"><a id="pgfId-1014841"></a>batch = train_images[:<span class="fm-codeblue">128</span>]</pre>

  <p class="body"><a id="pgfId-1014855"></a>And here’s the next batch:</p>
  <pre class="programlisting"><a id="pgfId-1014861"></a>batch = train_images[<span class="fm-codeblue">128</span>:<span class="fm-codeblue">256</span>]</pre>

  <p class="body"><a id="pgfId-1014875"></a>And the <i class="fm-italics">n</i>th batch:</p>
  <pre class="programlisting"><a id="pgfId-1033564"></a>n = <span class="fm-codeblue">3</span> 
<a id="pgfId-1014904"></a>batch = train_images[<span class="fm-codeblue">128</span> * n:<span class="fm-codeblue">128</span> * (n + <span class="fm-codeblue">1</span>)]</pre>

  <p class="body"><a id="pgfId-1014933"></a>When considering such a batch tensor, the first axis (axis 0) is called <a id="marker-1014912"></a>the <i class="fm-italics">batch axis</i> or <i class="fm-italics">batch dimension</i>. This is <a id="marker-1014938"></a>a term you’ll frequently encounter when using Keras and other deep learning libraries. <a id="marker-1014944"></a><a id="marker-1014947"></a></p>

  <h3 class="fm-head1" id="heading_id_12"><a id="pgfId-1014953"></a>2.2.8 Real-world examples of data tensors</h3>

  <p class="body"><a id="pgfId-1014970"></a><a id="marker-1014964"></a><a id="marker-1014966"></a>Let’s make data tensors more concrete with a few examples similar to what you’ll encounter later. The data you’ll manipulate will almost always fall into one of the following categories:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1015003"></a><i class="fm-italics1">Vector data</i>—Rank-2 tensors of <a class="calibre11" id="marker-1014992"></a>shape <code class="fm-code-in-text">(samples,</code> <code class="fm-code-in-text">features)</code>, where each sample is a vector of numerical attributes (“features”)</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1015052"></a><i class="fm-italics1">Timeseries data or sequence data</i>—Rank-3 tensors <a class="calibre11" id="marker-1015025"></a>of <a class="calibre11" id="marker-1015031"></a>shape <code class="fm-code-in-text">(samples,</code> <code class="fm-code-in-text">timesteps,</code> <code class="fm-code-in-text">features)</code>, where each sample is a sequence (of length <code class="fm-code-in-text">timesteps</code>) of feature vectors</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1015079"></a><i class="fm-italics1">Images</i>—Rank-4 tensors of shape <code class="fm-code-in-text">(samples,</code> <code class="fm-code-in-text">height,</code> <code class="fm-code-in-text">width,</code> <code class="fm-code-in-text">channels)</code>, where each sample is a 2D grid of pixels, and each pixel is represented by a vector of values (“channels”)</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1015116"></a><i class="fm-italics1">Video</i>—Rank-5 tensors of shape <code class="fm-code-in-text">(samples,</code> <code class="fm-code-in-text">frames,</code> <code class="fm-code-in-text">height,</code> <code class="fm-code-in-text">width,</code> <code class="fm-code-in-text">channels)</code>, where each sample is a sequence (of length <code class="fm-code-in-text">frames</code>) of images <a class="calibre11" id="marker-1027299"></a><a class="calibre11" id="marker-1027300"></a></p>
    </li>
  </ul>

  <h3 class="fm-head1" id="heading_id_13"><a id="pgfId-1015130"></a>2.2.9 Vector data</h3>

  <p class="body"><a id="pgfId-1015171"></a><a id="marker-1015141"></a><a id="marker-1015143"></a><a id="marker-1015145"></a>This is one of the most common cases. In such a dataset, each single data point can be encoded as a vector, and thus a batch of data will be encoded as a rank-2 tensor (that is, an array of vectors), where the first axis is the <i class="fm-italics">samples axis</i> and the second axis <a id="marker-1015160"></a>is the <i class="fm-italics">features axis</i>.</p>

  <p class="body"><a id="pgfId-1015180"></a>Let’s take a look at two examples:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1015186"></a>An actuarial dataset of people, where we consider each person’s age, gender, and income. Each person can be characterized as a vector of 3 values, and thus an entire dataset of 100,000 people can be stored in a rank-2 tensor of shape <code class="fm-code-in-text">(100000,</code> <code class="fm-code-in-text">3)</code>.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1015209"></a>A dataset of text documents, where we represent each document by the counts of how many times each word appears in it (out of a dictionary of 20,000 common words). Each document can be encoded as a vector of 20,000 values (one count per word in the dictionary), and thus an entire dataset of 500 documents can be stored in a tensor of shape <code class="fm-code-in-text">(500,</code> <code class="fm-code-in-text">20000)</code>. <a class="calibre11" id="marker-1015224"></a><a class="calibre11" id="marker-1015227"></a><a class="calibre11" id="marker-1015229"></a></p>
    </li>
  </ul>

  <h3 class="fm-head1" id="heading_id_14"><a id="pgfId-1015235"></a>2.2.10 Timeseries data or sequence data</h3>

  <p class="body"><a id="pgfId-1015254"></a><a id="marker-1015246"></a><a id="marker-1015248"></a><a id="marker-1015250"></a>Whenever time matters in your data (or the notion of sequence order), it makes sense to store it in a rank-3 tensor with an explicit time axis. Each sample can be encoded as a sequence of vectors (a rank-2 tensor), and thus a batch of data will be encoded as a rank-3 tensor (see figure 2.3).</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/02-03.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1041527"></a>Figure 2.3 A rank-3 timeseries data tensor</p>

  <p class="body"><a id="pgfId-1015269"></a>The time axis is always the second axis (axis of index 1) by convention. Let’s look at a few examples:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1015313"></a>A dataset of stock prices. Every minute, we store the current price of the stock, the highest price in the past minute, and the lowest price in the past minute. Thus, every minute is encoded as a 3D vector, an entire day of trading is encoded as a matrix of shape <code class="fm-code-in-text">(390,</code> <code class="fm-code-in-text">3)</code> (there are 390 minutes in a trading day), and 250 days’ worth of data can be stored in a rank-3 tensor of shape <code class="fm-code-in-text">(250,</code> <code class="fm-code-in-text">390,</code> <code class="fm-code-in-text">3)</code>. Here, each sample would be one day’s worth of data.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1015342"></a>A dataset of tweets, where we encode each tweet as a sequence of 280 characters out of an alphabet of 128 unique characters. In this setting, each character can be encoded as a binary vector of size 128 (an all-zeros vector except for a 1 entry at the index corresponding to the character). Then each tweet can be encoded as a rank-2 tensor of shape <code class="fm-code-in-text">(280,</code> <code class="fm-code-in-text">128)</code>, and a dataset of 1 million tweets can be stored in a tensor of shape <code class="fm-code-in-text">(1000000,</code> <code class="fm-code-in-text">280,</code> <code class="fm-code-in-text">128)</code>. <a class="calibre11" id="marker-1015347"></a><a class="calibre11" id="marker-1015350"></a><a class="calibre11" id="marker-1015352"></a></p>
    </li>
  </ul>

  <h3 class="fm-head1" id="heading_id_15"><a id="pgfId-1015358"></a>2.2.11 Image data</h3>

  <p class="body"><a id="pgfId-1015393"></a><a id="marker-1015369"></a><a id="marker-1015371"></a><a id="marker-1015373"></a>Images typically have three dimensions: height, width, and color depth. Although grayscale images (like our MNIST digits) have only a single color channel and could thus be stored in rank-2 tensors, by convention image tensors are always rank-3, with a one-dimensional color channel for grayscale images. A batch of 128 grayscale images of size 256 × 256 could thus be stored in a tensor of shape <code class="fm-code-in-text">(128,</code> <code class="fm-code-in-text">256,</code> <code class="fm-code-in-text">256,</code> <code class="fm-code-in-text">1)</code>, and a batch of 128 color images could be stored in a tensor of shape <code class="fm-code-in-text">(128,</code> <code class="fm-code-in-text">256,</code> <code class="fm-code-in-text">256,</code> <code class="fm-code-in-text">3)</code> (see figure 2.4).</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/02-04.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1041569"></a>Figure 2.4 A rank-4 image data tensor</p>

  <p class="body"><a id="pgfId-1015448"></a>There are two conventions for shapes of image tensors: the <i class="fm-italics">channels-last</i> convention (which is <a id="marker-1015437"></a>standard in TensorFlow) and the <i class="fm-italics">channels-first</i> convention (which is <a id="marker-1015453"></a>increasingly falling out of favor).</p>

  <p class="body"><a id="pgfId-1015499"></a>The channels-last convention places the color-depth axis at the end: <code class="fm-code-in-text">(samples,</code> <code class="fm-code-in-text">height,</code> <code class="fm-code-in-text">width,</code> <code class="fm-code-in-text">color_depth)</code>. Meanwhile, the channels-first convention places the color depth axis right after the batch axis: <code class="fm-code-in-text">(samples,</code> <code class="fm-code-in-text">color_depth,</code> <code class="fm-code-in-text">height,</code> <code class="fm-code-in-text">width)</code>. With the channels-first convention, the previous examples would become <code class="fm-code-in-text">(128,</code> <code class="fm-code-in-text">1,</code> <code class="fm-code-in-text">256,</code> <code class="fm-code-in-text">256)</code> and <code class="fm-code-in-text">(128,</code> <code class="fm-code-in-text">3,</code> <code class="fm-code-in-text">256,</code> <code class="fm-code-in-text">256)</code>. The Keras API provides support for both formats. <a id="marker-1015504"></a><a id="marker-1015507"></a><a id="marker-1015509"></a></p>

  <h3 class="fm-head1" id="heading_id_16"><a id="pgfId-1015515"></a>2.2.12 Video data</h3>

  <p class="body"><a id="pgfId-1015560"></a><a id="marker-1015526"></a><a id="marker-1015528"></a><a id="marker-1015530"></a>Video data is one of the few types of real-world data for which you’ll need rank-5 tensors. A video can be understood as a sequence of frames, each frame being a color image. Because each frame can be stored in a rank-3 tensor <code class="fm-code-in-text">(height,</code> <code class="fm-code-in-text">width,</code> <code class="fm-code-in-text">color_ depth)</code>, a sequence of frames can be stored in a rank-4 tensor <code class="fm-code-in-text">(frames,</code> <code class="fm-code-in-text">height,</code> <code class="fm-code-in-text">width,</code> <code class="fm-code-in-text">color_depth)</code>, and thus a batch of different videos can be stored in a rank-5 tensor of shape <code class="fm-code-in-text">(samples,</code> <code class="fm-code-in-text">frames,</code> <code class="fm-code-in-text">height,</code> <code class="fm-code-in-text">width,</code> <code class="fm-code-in-text">color_depth)</code>.</p>

  <p class="body"><a id="pgfId-1015605"></a>For instance, a 60-second, 144 × 256 YouTube video clip sampled at 4 frames per second would have 240 frames. A batch of four such video clips would be stored in a tensor of shape <code class="fm-code-in-text">(4,</code> <code class="fm-code-in-text">240,</code> <code class="fm-code-in-text">144,</code> <code class="fm-code-in-text">256,</code> <code class="fm-code-in-text">3)</code>. That’s a total of 106,168,320 values! If the <code class="fm-code-in-text">dtype</code> of the tensor was <code class="fm-code-in-text">float32</code>, each value would be stored in 32 bits, so the tensor would represent 405 MB. Heavy! Videos you encounter in real life are much lighter, because they aren’t stored in <code class="fm-code-in-text">float32</code>, and they’re typically compressed by a large factor (such as in the MPEG format). <a id="marker-1015610"></a><a id="marker-1015613"></a><a id="marker-1015615"></a><a id="marker-1015617"></a><a id="marker-1015619"></a></p>

  <h2 class="fm-head" id="heading_id_17"><a id="pgfId-1015625"></a>2.3 The gears of neural networks: Tensor operations</h2>

  <p class="body"><a id="pgfId-1015658"></a><a id="marker-1015636"></a><a id="marker-1015638"></a>Much as any computer program can be ultimately reduced to a small set of binary operations on binary inputs (AND, OR, NOR, and so on), all transformations learned by deep neural networks can be reduced to a handful of <i class="fm-italics">tensor operations</i> (or <i class="fm-italics">tensor functions</i>) applied to tensors of numeric data. For instance, it’s possible to add tensors, multiply tensors, and so on.</p>

  <p class="body"><a id="pgfId-1015667"></a>In our initial example, we built our model by stacking <code class="fm-code-in-text">Dense</code> layers on top of each other. A Keras layer instance looks like this:</p>
  <pre class="programlisting"><a id="pgfId-1015682"></a>keras.layers.Dense(<span class="fm-codeblue">512</span>, activation=<span class="fm-codegreen">"relu"</span>)</pre>

  <p class="body"><a id="pgfId-1015712"></a>This layer can be interpreted as a function, which takes as input a matrix and returns another matrix—a new representation for the input tensor. Specifically, the function is as follows (where <code class="fm-code-in-text">W</code> is a matrix and <code class="fm-code-in-text">b</code> is a vector, both attributes of the layer):</p>
  <pre class="programlisting"><a id="pgfId-1015721"></a>output = relu(dot(input, W) + b)</pre>

  <p class="body"><a id="pgfId-1015735"></a>Let’s unpack this. We have three tensor operations here:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1015765"></a>A dot product (<code class="fm-code-in-text">dot</code>) between the input tensor and a tensor named <code class="fm-code-in-text">W</code></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1015791"></a>An addition (<code class="fm-code-in-text">+</code>) between the resulting matrix and a vector <code class="fm-code-in-text">b</code></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1015827"></a>A <code class="fm-code-in-text">relu</code> operation: <code class="fm-code-in-text">relu(x)</code> is <code class="fm-code-in-text">max(x,</code> <code class="fm-code-in-text">0)</code>; “relu” stands for “rectified linear unit”</p>
    </li>
  </ul>

  <p class="fm-callout"><a id="pgfId-1015836"></a><span class="fm-callout-head">Note</span> Although this section deals entirely with linear algebra expressions, you won’t find any mathematical notation here. I’ve found that mathematical concepts can be more readily mastered by programmers with no mathematical background if they’re expressed as short Python snippets instead of mathematical equations. So we’ll use NumPy and TensorFlow code throughout.</p>

  <h3 class="fm-head1" id="heading_id_18"><a id="pgfId-1015852"></a>2.3.1 Element-wise operations</h3>

  <p class="body"><a id="pgfId-1015917"></a><a id="marker-1015863"></a><a id="marker-1015865"></a><a id="marker-1015867"></a>The <code class="fm-code-in-text">relu</code> operation and addition are element-wise operations: operations that are applied independently to each entry in the tensors being considered. This means these operations are highly amenable to massively parallel implementations (<i class="fm-italics">vectorized</i> implementations, a term that comes from the <i class="fm-italics">vector processor</i> supercomputer architecture from the 1970–90 period). If you want to write a naive Python implementation of an element-wise operation, you use a <code class="fm-code-in-text">for</code> loop, as in this naive implementation of an element-wise <code class="fm-code-in-text">relu</code> operation:</p>
  <pre class="programlisting"><a id="pgfId-1033609"></a><b class="fm-codebrown">def</b> naive_relu(x):
<a id="pgfId-1015940"></a>    <b class="fm-codebrown">assert</b> len(x.shape) == <span class="fm-codeblue">2</span>      <span class="fm-combinumeral">❶</span>
<a id="pgfId-1015952"></a>    x = x.copy()                  <span class="fm-combinumeral">❷</span>
<a id="pgfId-1033644"></a>    <b class="fm-codebrown">for</b> i <b class="fm-codebrown">in</b> range(x.shape[<span class="fm-codeblue">0</span>]):
<a id="pgfId-1033645"></a>        <b class="fm-codebrown">for</b> j <b class="fm-codebrown">in</b> range(x.shape[<span class="fm-codeblue">1</span>]):
<a id="pgfId-1033646"></a>            x[i, j] = max(x[i, j], <span class="fm-codeblue">0</span>)
<a id="pgfId-1015982"></a>    <b class="fm-codebrown">return</b> x</pre>

  <p class="fm-code-annotation"><a id="pgfId-1040123"></a><span class="fm-combinumeral">❶</span> x is a rank-2 NumPy tensor.</p>

  <p class="fm-code-annotation"><a id="pgfId-1040144"></a><span class="fm-combinumeral">❷</span> Avoid overwriting the input tensor.</p>

  <p class="body"><a id="pgfId-1016024"></a>You could do the same for addition:</p>
  <pre class="programlisting"><a id="pgfId-1033661"></a><b class="fm-codebrown">def</b> naive_add(x, y):
<a id="pgfId-1016044"></a>    <b class="fm-codebrown">assert</b> len(x.shape) == <span class="fm-codeblue">2</span>       <span class="fm-combinumeral">❶</span>
<a id="pgfId-1033678"></a>    <b class="fm-codebrown">assert</b> x.shape == y.shape
<a id="pgfId-1016062"></a>    x = x.copy()                   <span class="fm-combinumeral">❷</span>
<a id="pgfId-1033693"></a>    <b class="fm-codebrown">for</b> i <b class="fm-codebrown">in</b> range(x.shape[<span class="fm-codeblue">0</span>]):
<a id="pgfId-1033694"></a>        <b class="fm-codebrown">for</b> j <b class="fm-codebrown">in</b> range(x.shape[<span class="fm-codeblue">1</span>]):
<a id="pgfId-1033695"></a>            x[i, j] += y[i, j]
<a id="pgfId-1016092"></a>    <b class="fm-codebrown">return</b> x</pre>

  <p class="fm-code-annotation"><a id="pgfId-1039956"></a><span class="fm-combinumeral">❶</span> x and y are rank-2 NumPy tensors.</p>

  <p class="fm-code-annotation"><a id="pgfId-1039973"></a><span class="fm-combinumeral">❷</span> Avoid overwriting the input tensor.</p>

  <p class="body"><a id="pgfId-1016134"></a>On the same principle, you can do element-wise multiplication, subtraction, and so on.</p>

  <p class="body"><a id="pgfId-1016140"></a>In practice, when dealing with NumPy arrays, these operations are available as well-optimized built-in NumPy functions, which themselves delegate the heavy lifting to a Basic Linear Algebra <a id="marker-1016142"></a>Subprograms (BLAS) implementation. BLAS are low-level, highly parallel, efficient tensor-manipulation routines that are typically implemented in Fortran or C.</p>

  <p class="body"><a id="pgfId-1016152"></a>So, in NumPy, you can do the following element-wise operation, and it will be blazing fast:</p>
  <pre class="programlisting"><a id="pgfId-1033714"></a><b class="fm-codebrown">import</b> numpy <b class="fm-codebrown">as</b> np
<a id="pgfId-1016172"></a>z = x + y                <span class="fm-combinumeral">❶</span>
<a id="pgfId-1016184"></a>z = np.maximum(z, <span class="fm-codeblue">0.</span>)    <span class="fm-combinumeral">❷</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1039918"></a><span class="fm-combinumeral">❶</span> Element-wise addition</p>

  <p class="fm-code-annotation"><a id="pgfId-1039939"></a><span class="fm-combinumeral">❷</span> Element-wise relu</p>

  <p class="body"><a id="pgfId-1016232"></a>Let’s actually time the difference:</p>
  <pre class="programlisting"><a id="pgfId-1033745"></a><b class="fm-codebrown">import</b> time
<a id="pgfId-1033746"></a>  
<a id="pgfId-1033747"></a>x = np.random.random((<span class="fm-codeblue">20</span>, <span class="fm-codeblue">100</span>))
<a id="pgfId-1033748"></a>y = np.random.random((<span class="fm-codeblue">20</span>, <span class="fm-codeblue">100</span>))
<a id="pgfId-1033749"></a>  
<a id="pgfId-1033750"></a>t0 = time.time() 
<a id="pgfId-1033751"></a><b class="fm-codebrown">for</b> _ <b class="fm-codebrown">in</b> range(<span class="fm-codeblue">1000</span>):
<a id="pgfId-1033752"></a>    z = x + y
<a id="pgfId-1033753"></a>    z = np.maximum(z, <span class="fm-codeblue">0.</span>) 
<a id="pgfId-1016298"></a><b class="fm-codebrown">print</b>(<span class="fm-codegreen">"Took: {0:.2f} s"</span>.format(time.time() - t0))</pre>

  <p class="body"><a id="pgfId-1016304"></a>This takes 0.02 s. Meanwhile, the naive version takes a stunning 2.45 s:</p>
  <pre class="programlisting"><a id="pgfId-1033772"></a>t0 = time.time() 
<a id="pgfId-1033773"></a><b class="fm-codebrown">for</b> _ <b class="fm-codebrown">in</b> range(<span class="fm-codeblue">1000</span>):
<a id="pgfId-1033774"></a>    z = naive_add(x, y)
<a id="pgfId-1033775"></a>    z = naive_relu(z) 
<a id="pgfId-1016342"></a><b class="fm-codebrown">print</b>(<span class="fm-codegreen">"Took: {0:.2f} s"</span>.format(time.time() - t0))</pre>

  <p class="body"><a id="pgfId-1016348"></a>Likewise, when running TensorFlow code on a GPU, element-wise operations are executed via fully vectorized CUDA implementations that can best utilize the highly parallel GPU chip architecture. <a id="marker-1016350"></a><a id="marker-1016353"></a><a id="marker-1016355"></a></p>

  <h3 class="fm-head1" id="heading_id_19"><a id="pgfId-1016361"></a>2.3.2 Broadcasting</h3>

  <p class="body"><a id="pgfId-1016396"></a><a id="marker-1016372"></a><a id="marker-1016374"></a><a id="marker-1016376"></a>Our earlier naive implementation of <code class="fm-code-in-text">naive_add</code> only supports the addition of rank-2 tensors with identical shapes. But in the <code class="fm-code-in-text">Dense</code> layer introduced earlier, we added a rank-2 tensor with a vector. What happens with addition when the shapes of the two tensors being added differ?</p>

  <p class="body"><a id="pgfId-1016405"></a>When possible, and if there’s no ambiguity, the smaller tensor will be <i class="fm-italics">broadcast</i> to match the shape of the larger tensor. Broadcasting consists of two steps:</p>

  <ol class="calibre14">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1016444"></a>Axes (called <i class="fm-italics1">broadcast axes</i>) are added to the smaller tensor to match the <code class="fm-code-in-text">ndim</code> of the larger tensor.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1016453"></a>The smaller tensor is repeated alongside these new axes to match the full shape of the larger tensor.</p>
    </li>
  </ol>

  <p class="body"><a id="pgfId-1016499"></a>Let’s look at a concrete example. Consider <code class="fm-code-in-text">X</code> with shape <code class="fm-code-in-text">(32,</code> <code class="fm-code-in-text">10)</code> and <code class="fm-code-in-text">y</code> with shape <code class="fm-code-in-text">(10,)</code>:</p>
  <pre class="programlisting"><a id="pgfId-1033837"></a><b class="fm-codebrown">import</b> numpy <b class="fm-codebrown">as</b> np
<a id="pgfId-1016522"></a>X = np.random.random((<span class="fm-codeblue">32</span>, <span class="fm-codeblue">10</span>))     <span class="fm-combinumeral">❶</span>
<a id="pgfId-1016534"></a>y = np.random.random((<span class="fm-codeblue">10</span>,))        <span class="fm-combinumeral">❷</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1039748"></a><span class="fm-combinumeral">❶</span> X is a random matrix with shape (32, 10).</p>

  <p class="fm-code-annotation"><a id="pgfId-1039765"></a><span class="fm-combinumeral">❷</span> y is a random vector with shape (10,).</p>

  <p class="body"><a id="pgfId-1016598"></a>First, we add an empty first axis to <code class="fm-code-in-text">y</code>, whose shape becomes <code class="fm-code-in-text">(1,</code> <code class="fm-code-in-text">10)</code>:</p>
  <pre class="programlisting"><a id="pgfId-1016607"></a>y = np.expand_dims(y, axis=<span class="fm-codeblue">0</span>)    <span class="fm-combinumeral">❶</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1039731"></a><span class="fm-combinumeral">❶</span> The shape of y is now (1, 10).</p>

  <p class="body"><a id="pgfId-1016703"></a>Then, we repeat <code class="fm-code-in-text">y</code> 32 times alongside this new axis, so that we end up with a tensor <code class="fm-code-in-text">Y</code> with shape <code class="fm-code-in-text">(32,</code> <code class="fm-code-in-text">10)</code>, where <code class="fm-code-in-text">Y[i,</code> <code class="fm-code-in-text">:]</code> <code class="fm-code-in-text">==</code> <code class="fm-code-in-text">y</code> for <code class="fm-code-in-text">i</code> in <code class="fm-code-in-text">range(0,</code> <code class="fm-code-in-text">32)</code>:</p>
  <pre class="programlisting"><a id="pgfId-1016712"></a>Y = np.concatenate([y] * <span class="fm-codeblue">32</span>, axis=<span class="fm-codeblue">0</span>)     <span class="fm-combinumeral">❶</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1039710"></a><span class="fm-combinumeral">❶</span> Repeat y 32 times along axis 0 to obtain Y, which has shape (32, 10).</p>

  <p class="body"><a id="pgfId-1016768"></a>At this point, we can proceed to add <code class="fm-code-in-text">X</code> and <code class="fm-code-in-text">Y</code>, because they have the same shape.</p>

  <p class="body"><a id="pgfId-1016777"></a>In terms of implementation, no new rank-2 tensor is created, because that would be terribly inefficient. The repetition operation is entirely virtual: it happens at the algorithmic level rather than at the memory level. But thinking of the vector being repeated 10 times alongside a new axis is a helpful mental model. Here’s what a naive implementation would look like:</p>
  <pre class="programlisting"><a id="pgfId-1033903"></a><b class="fm-codebrown">def</b> naive_add_matrix_and_vector(x, y):
<a id="pgfId-1016797"></a>    <b class="fm-codebrown">assert</b> len(x.shape) == <span class="fm-codeblue">2</span>               <span class="fm-combinumeral">❶</span>
<a id="pgfId-1016809"></a>    <b class="fm-codebrown">assert</b> len(y.shape) == <span class="fm-codeblue">1</span>               <span class="fm-combinumeral">❷</span>
<a id="pgfId-1033937"></a>    <b class="fm-codebrown">assert</b> x.shape[<span class="fm-codeblue">1</span>] == y.shape[<span class="fm-codeblue">0</span>]
<a id="pgfId-1016827"></a>    x = x.copy()                           <span class="fm-combinumeral">❸</span>
<a id="pgfId-1033956"></a>    <b class="fm-codebrown">for</b> i <b class="fm-codebrown">in</b> range(x.shape[<span class="fm-codeblue">0</span>]):
<a id="pgfId-1033957"></a>        <b class="fm-codebrown">for</b> j <b class="fm-codebrown">in</b> range(x.shape[<span class="fm-codeblue">1</span>]):
<a id="pgfId-1033958"></a>            x[i, j] += y[j]
<a id="pgfId-1016857"></a>    <b class="fm-codebrown">return</b> x</pre>

  <p class="fm-code-annotation"><a id="pgfId-1039559"></a><span class="fm-combinumeral">❶</span> x is a rank-2 NumPy tensor.</p>

  <p class="fm-code-annotation"><a id="pgfId-1039576"></a><span class="fm-combinumeral">❷</span> y is a NumPy vector.</p>

  <p class="fm-code-annotation"><a id="pgfId-1039593"></a><span class="fm-combinumeral">❸</span> Avoid overwriting the input tensor.</p>

  <p class="body"><a id="pgfId-1016951"></a>With broadcasting, you can generally perform element-wise operations that take two inputs tensors if one tensor has shape <code class="fm-code-in-text">(a,</code> <code class="fm-code-in-text">b,</code> <code class="fm-code-in-text">...</code> <code class="fm-code-in-text">n,</code> <code class="fm-code-in-text">n</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">1,</code> <code class="fm-code-in-text">...</code> <code class="fm-code-in-text">m)</code> and the other has shape <code class="fm-code-in-text">(n,</code> <code class="fm-code-in-text">n</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">1,</code> <code class="fm-code-in-text">...</code> <code class="fm-code-in-text">m)</code>. The broadcasting will then automatically happen for axes <code class="fm-code-in-text">a</code> through <code class="fm-code-in-text">n</code> <code class="fm-code-in-text">-</code> <code class="fm-code-in-text">1</code>.</p>

  <p class="body"><a id="pgfId-1016960"></a>The following example applies the element-wise <code class="fm-code-in-text">maximum</code> operation to two tensors of different shapes via broadcasting:</p>
  <pre class="programlisting"><a id="pgfId-1033977"></a><b class="fm-codebrown">import</b> numpy <b class="fm-codebrown">as</b> np
<a id="pgfId-1016989"></a>x = np.random.random((<span class="fm-codeblue">64</span>, <span class="fm-codeblue">3</span>, <span class="fm-codeblue">32</span>, <span class="fm-codeblue">10</span>))     <span class="fm-combinumeral">❶</span>
<a id="pgfId-1017001"></a>y = np.random.random((<span class="fm-codeblue">32</span>, <span class="fm-codeblue">10</span>))            <span class="fm-combinumeral">❷</span>
<a id="pgfId-1017013"></a>z = np.maximum(x, y)                      <span class="fm-combinumeral">❸</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1039336"></a><span class="fm-combinumeral">❶</span> x is a random tensor with shape (64, 3, 32, 10).</p>

  <p class="fm-code-annotation"><a id="pgfId-1039357"></a><span class="fm-combinumeral">❷</span> y is a random tensor with shape (32, 10).</p>

  <p class="fm-code-annotation"><a id="pgfId-1039374"></a><span class="fm-combinumeral">❸</span> The output z has shape (64, 3, 32, 10) like x. <a id="marker-1039379"></a><a id="marker-1039380"></a><a id="marker-1039381"></a></p>

  <h3 class="fm-head1" id="heading_id_20"><a id="pgfId-1017084"></a>2.3.3 Tensor product</h3>

  <p class="body"><a id="pgfId-1017135"></a><a id="marker-1017095"></a><a id="marker-1017097"></a><a id="marker-1017099"></a>The <i class="fm-italics">tensor product</i>, or <i class="fm-italics">dot product</i> (not to be <a id="marker-1017124"></a>confused with an element-wise product, the <code class="fm-code-in-text">*</code> operator), is one of the most common, most useful tensor operations.</p>

  <p class="body"><a id="pgfId-1017157"></a>In NumPy, a tensor product is done using <a id="marker-1017146"></a>the <code class="fm-code-in-text">np.dot</code> function (because the mathematical notation for tensor product is usually a dot):</p>
  <pre class="programlisting"><a id="pgfId-1034026"></a>x = np.random.random((<span class="fm-codeblue">32</span>,))
<a id="pgfId-1034027"></a>y = np.random.random((<span class="fm-codeblue">32</span>,))
<a id="pgfId-1017186"></a>z = np.dot(x, y)</pre>

  <p class="body"><a id="pgfId-1017192"></a>In mathematical notation, you’d note the operation with a dot (•):</p>
  <pre class="programlisting"><a id="pgfId-1017198"></a>z = x • y</pre>

  <p class="body"><a id="pgfId-1017228"></a>Mathematically, what does the dot operation do? Let’s start with the dot product of two vectors, <code class="fm-code-in-text">x</code> and <code class="fm-code-in-text">y</code>. It’s computed as follows:</p>
  <pre class="programlisting"><a id="pgfId-1034046"></a><b class="fm-codebrown">def</b> naive_vector_dot(x, y):
<a id="pgfId-1017251"></a>    <b class="fm-codebrown">assert</b> len(x.shape) == <span class="fm-codeblue">1</span>         <span class="fm-combinumeral">❶</span>
<a id="pgfId-1017263"></a>    <b class="fm-codebrown">assert</b> len(y.shape) == <span class="fm-codeblue">1</span>         <span class="fm-combinumeral">❶</span>
<a id="pgfId-1034075"></a>    <b class="fm-codebrown">assert</b> x.shape[<span class="fm-codeblue">0</span>] == y.shape[<span class="fm-codeblue">0</span>]
<a id="pgfId-1034076"></a>    z = <span class="fm-codeblue">0.</span> 
<a id="pgfId-1034077"></a>    <b class="fm-codebrown">for</b> i <b class="fm-codebrown">in</b> range(x.shape[<span class="fm-codeblue">0</span>]):
<a id="pgfId-1034078"></a>        z += x[i] * y[i]
<a id="pgfId-1017299"></a>    <b class="fm-codebrown">return</b> z</pre>

  <p class="fm-code-annotation"><a id="pgfId-1039271"></a><span class="fm-combinumeral">❶</span> x and y are NumPy vectors.</p>

  <p class="body"><a id="pgfId-1017325"></a>You’ll have noticed that the dot product between two vectors is a scalar and that only vectors with the same number of elements are compatible for a dot product.</p>

  <p class="body"><a id="pgfId-1017367"></a>You can also take the dot product between a matrix <code class="fm-code-in-text">x</code> and a vector <code class="fm-code-in-text">y</code>, which returns a vector where the coefficients are the dot products between <code class="fm-code-in-text">y</code> and the rows of <code class="fm-code-in-text">x</code>. You implement it as follows:</p>
  <pre class="programlisting"><a id="pgfId-1034093"></a><b class="fm-codebrown">def</b> naive_matrix_vector_dot(x, y):
<a id="pgfId-1017390"></a>    <b class="fm-codebrown">assert</b> len(x.shape) == <span class="fm-codeblue">2</span>              <span class="fm-combinumeral">❶</span>
<a id="pgfId-1017402"></a>    <b class="fm-codebrown">assert</b> len(y.shape) == <span class="fm-codeblue">1</span>              <span class="fm-combinumeral">❷</span>
<a id="pgfId-1017414"></a>    <b class="fm-codebrown">assert</b> x.shape[<span class="fm-codeblue">1</span>] == y.shape[<span class="fm-codeblue">0</span>]       <span class="fm-combinumeral">❸</span>
<a id="pgfId-1017426"></a>    z = np.zeros(x.shape[<span class="fm-codeblue">0</span>])              <span class="fm-combinumeral">❹</span>
<a id="pgfId-1034152"></a>    <b class="fm-codebrown">for</b> i <b class="fm-codebrown">in</b> range(x.shape[<span class="fm-codeblue">0</span>]):
<a id="pgfId-1034153"></a>        <b class="fm-codebrown">for</b> j <b class="fm-codebrown">in</b> range(x.shape[<span class="fm-codeblue">1</span>]):
<a id="pgfId-1034154"></a>            z[i] += x[i, j] * y[j]
<a id="pgfId-1017456"></a>    <b class="fm-codebrown">return</b> z</pre>

  <p class="fm-code-annotation"><a id="pgfId-1039062"></a><span class="fm-combinumeral">❶</span> x is a NumPy matrix.</p>

  <p class="fm-code-annotation"><a id="pgfId-1039083"></a><span class="fm-combinumeral">❷</span> y is a NumPy vector.</p>

  <p class="fm-code-annotation"><a id="pgfId-1039100"></a><span class="fm-combinumeral">❸</span> The first dimension of x must be the same as the 0th dimension of y!</p>

  <p class="fm-code-annotation"><a id="pgfId-1039117"></a><span class="fm-combinumeral">❹</span> This operation returns a vector of 0s with the same shape as y.</p>

  <p class="body"><a id="pgfId-1017530"></a>You could also reuse the code we wrote previously, which highlights the relationship between a matrix-vector product and a vector product:</p>
  <pre class="programlisting"><a id="pgfId-1034169"></a><b class="fm-codebrown">def</b> naive_matrix_vector_dot(x, y):
<a id="pgfId-1034170"></a>    z = np.zeros(x.shape[<span class="fm-codeblue">0</span>])
<a id="pgfId-1034171"></a>    <b class="fm-codebrown">for</b> i <b class="fm-codebrown">in</b> range(x.shape[<span class="fm-codeblue">0</span>]):
<a id="pgfId-1034172"></a>        z[i] = naive_vector_dot(x[i, :], y)
<a id="pgfId-1017568"></a>    <b class="fm-codebrown">return</b> z</pre>

  <p class="body"><a id="pgfId-1017620"></a>Note that as soon as one of the two tensors has an <code class="fm-code-in-text">ndim</code> greater than 1, <code class="fm-code-in-text">dot</code> is no longer <i class="fm-italics">symmetric</i>, which is to say that <code class="fm-code-in-text">dot(x,</code> <code class="fm-code-in-text">y)</code> isn’t the same as <code class="fm-code-in-text">dot(y,</code> <code class="fm-code-in-text">x)</code>.</p>

  <p class="body"><a id="pgfId-1017695"></a>Of course, a dot product generalizes to tensors with an arbitrary number of axes. The most common applications may be the dot product between two matrices. You can take the dot product of two matrices <code class="fm-code-in-text">x</code> and <code class="fm-code-in-text">y</code> (<code class="fm-code-in-text">dot(x,</code> <code class="fm-code-in-text">y)</code>) if and only if <code class="fm-code-in-text">x.shape[1]</code> <code class="fm-code-in-text">==</code> <code class="fm-code-in-text">y.shape[0]</code>. The result is a matrix with shape <code class="fm-code-in-text">(x.shape[0],</code> <code class="fm-code-in-text">y.shape[1])</code>, where the coefficients are the vector products between the rows of <code class="fm-code-in-text">x</code> and the columns of <code class="fm-code-in-text">y</code>. Here’s the naive implementation:</p>
  <pre class="programlisting"><a id="pgfId-1034191"></a><b class="fm-codebrown">def</b> naive_matrix_dot(x, y):
<a id="pgfId-1017718"></a>    <b class="fm-codebrown">assert</b> len(x.shape) == <span class="fm-codeblue">2</span>                  <span class="fm-combinumeral">❶</span>
<a id="pgfId-1017730"></a>    <b class="fm-codebrown">assert</b> len(y.shape) == <span class="fm-codeblue">2</span>                  <span class="fm-combinumeral">❶</span>
<a id="pgfId-1017742"></a>    <b class="fm-codebrown">assert</b> x.shape[<span class="fm-codeblue">1</span>] == y.shape[<span class="fm-codeblue">0</span>]           <span class="fm-combinumeral">❷</span>
<a id="pgfId-1017754"></a>    z = np.zeros((x.shape[<span class="fm-codeblue">0</span>], y.shape[<span class="fm-codeblue">1</span>]))    <span class="fm-combinumeral">❸</span>
<a id="pgfId-1017766"></a>    <b class="fm-codebrown">for</b> i <b class="fm-codebrown">in</b> range(x.shape[<span class="fm-codeblue">0</span>]):               <span class="fm-combinumeral">❹</span>
<a id="pgfId-1017778"></a>        <b class="fm-codebrown">for</b> j <b class="fm-codebrown">in</b> range(y.shape[<span class="fm-codeblue">1</span>]):           <span class="fm-combinumeral">❺</span>
<a id="pgfId-1034291"></a>            row_x = x[i, :]
<a id="pgfId-1034292"></a>            column_y = y[:, j]
<a id="pgfId-1034293"></a>            z[i, j] = naive_vector_dot(row_x, column_y)
<a id="pgfId-1017808"></a>    <b class="fm-codebrown">return</b> z</pre>

  <p class="fm-code-annotation"><a id="pgfId-1038785"></a><span class="fm-combinumeral">❶</span> x and y are NumPy matrices.</p>

  <p class="fm-code-annotation"><a id="pgfId-1038802"></a><span class="fm-combinumeral">❷</span> The first dimension of x must be the same as the 0th dimension of y!</p>

  <p class="fm-code-annotation"><a id="pgfId-1038819"></a><span class="fm-combinumeral">❸</span> This operation returns a matrix of 0s with a specific shape.</p>

  <p class="fm-code-annotation"><a id="pgfId-1038836"></a><span class="fm-combinumeral">❹</span> Iterates over the rows of x . . .</p>

  <p class="fm-code-annotation"><a id="pgfId-1038853"></a><span class="fm-combinumeral">❺</span> . . . and over the columns of y.</p>

  <p class="body"><a id="pgfId-1017898"></a>To understand dot-product shape compatibility, it helps to visualize the input and output tensors by aligning them as shown in figure 2.5.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/02-05.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1041611"></a>Figure 2.5 Matrix dot-product box diagram</p>

  <p class="body"><a id="pgfId-1017994"></a>In the figure, <code class="fm-code-in-text">x</code>, <code class="fm-code-in-text">y</code>, and <code class="fm-code-in-text">z</code> are pictured as rectangles (literal boxes of coefficients). Because the rows of <code class="fm-code-in-text">x</code> and the columns of <code class="fm-code-in-text">y</code> must have the same size, it follows that the width of <code class="fm-code-in-text">x</code> must match the height of <code class="fm-code-in-text">y</code>. If you go on to develop new machine learning algorithms, you’ll likely be drawing such diagrams often.</p>

  <p class="body"><a id="pgfId-1018003"></a>More generally, you can take the dot product between higher-dimensional tensors, following the same rules for shape compatibility as outlined earlier for the 2D case:</p>
  <pre class="programlisting"><a id="pgfId-1018009"></a>(a, b, c, d) • (d,) <span class="segoe">→</span> (a, b, c)
<a id="pgfId-1018023"></a>(a, b, c, d) • (d, e) <span class="segoe">→</span> (a, b, c, e)</pre>

  <p class="body"><a id="pgfId-1018029"></a>And so on. <a id="marker-1018031"></a><a id="marker-1018034"></a><a id="marker-1018036"></a></p>

  <h3 class="fm-head1" id="heading_id_21"><a id="pgfId-1018042"></a>2.3.4 Tensor reshaping</h3>

  <p class="body"><a id="pgfId-1018077"></a><a id="marker-1018053"></a><a id="marker-1018055"></a><a id="marker-1018057"></a>A third type of tensor operation that’s essential to understand is <i class="fm-italics">tensor reshaping</i>. Although it wasn’t used in the <code class="fm-code-in-text">Dense</code> layers in our first neural network example, we used it when we preprocessed the digits data before feeding it into our model:</p>
  <pre class="programlisting"><a id="pgfId-1018086"></a>train_images = train_images.reshape((<span class="fm-codeblue">60000</span>, <span class="fm-codeblue">28</span> * <span class="fm-codeblue">28</span>))</pre>

  <p class="body"><a id="pgfId-1018100"></a>Reshaping a tensor means rearranging its rows and columns to match a target shape. Naturally, the reshaped tensor has the same total number of coefficients as the initial tensor. Reshaping is best understood via simple examples:</p>
  <pre class="programlisting"><a id="pgfId-1036478"></a>&gt;&gt;&gt; x = np.array([[<span class="fm-codeblue">0.</span>, <span class="fm-codeblue">1.</span>],
<a id="pgfId-1036479"></a>                  [<span class="fm-codeblue">2.</span>, <span class="fm-codeblue">3.</span>],
<a id="pgfId-1034332"></a>                  [<span class="fm-codeblue">4.</span>, <span class="fm-codeblue">5.</span>]])
<a id="pgfId-1034333"></a>&gt;&gt;&gt; x.shape
<a id="pgfId-1034334"></a>(3, 2)
<a id="pgfId-1034335"></a>&gt;&gt;&gt; x = x.reshape((<span class="fm-codeblue">6</span>, <span class="fm-codeblue">1</span>))
<a id="pgfId-1034336"></a>&gt;&gt;&gt; x
<a id="pgfId-1034337"></a>array([[ 0.],
<a id="pgfId-1034338"></a>       [ 1.],
<a id="pgfId-1034339"></a>       [ 2.],
<a id="pgfId-1034340"></a>       [ 3.],
<a id="pgfId-1034341"></a>       [ 4.],
<a id="pgfId-1034342"></a>       [ 5.]])
<a id="pgfId-1034343"></a> &gt;&gt;&gt; x = x.reshape((<span class="fm-codeblue">2</span>, <span class="fm-codeblue">3</span>))
<a id="pgfId-1034344"></a> &gt;&gt;&gt; x
<a id="pgfId-1034345"></a> array([[ 0.,  1.,  2.],
<a id="pgfId-1018210"></a>        [ 3.,  4.,  5.]])</pre>

  <p class="body"><a id="pgfId-1018252"></a>A special case of reshaping that’s commonly encountered is <i class="fm-italics">transposition</i>. <i class="fm-italics">Transposing</i> a matrix means exchanging its rows and its columns, so that <code class="fm-code-in-text">x[i,</code> <code class="fm-code-in-text">:]</code> becomes <code class="fm-code-in-text">x[:,</code> <code class="fm-code-in-text">i]</code>:</p>
  <pre class="programlisting"><a id="pgfId-1018261"></a>&gt;&gt;&gt; x = np.zeros((<span class="fm-codeblue">300</span>, <span class="fm-codeblue">20</span>))    <span class="fm-combinumeral">❶</span>
<a id="pgfId-1034380"></a>&gt;&gt;&gt; x = np.transpose(x)
<a id="pgfId-1034381"></a>&gt;&gt;&gt; x.shape
<a id="pgfId-1018293"></a>(20, 300)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1038735"></a><span class="fm-combinumeral">❶</span> Creates an all-zeros matrix of shape (300, 20) <a id="marker-1038740"></a><a id="marker-1038741"></a><a id="marker-1038742"></a></p>

  <h3 class="fm-head1" id="heading_id_22"><a id="pgfId-1018326"></a>2.3.5 Geometric interpretation of tensor operations</h3>

  <p class="body"><a id="pgfId-1018345"></a><a id="marker-1018337"></a><a id="marker-1018339"></a><a id="marker-1018341"></a>Because the contents of the tensors manipulated by tensor operations can be interpreted as coordinates of points in some geometric space, all tensor operations have a geometric interpretation. For instance, let’s consider addition. We’ll start with the following vector:</p>
  <pre class="programlisting"><a id="pgfId-1018350"></a>A = [<span class="fm-codeblue">0.5</span>, <span class="fm-codeblue">1</span>]</pre>

  <p class="body"><a id="pgfId-1018364"></a>It’s a point in a 2D space (see figure 2.6). It’s common to picture a vector as an arrow linking the origin to the point, as shown in figure 2.7.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/02-06.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1041693"></a>Figure 2.6  A point in a 2D space</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/02-07.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1041658"></a>Figure 2.7 A point in a 2D space pictured as an arrow</p>

  <p class="body"><a id="pgfId-1018440"></a>Let’s consider a new point, <code class="fm-code-in-text">B</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">[1,</code> <code class="fm-code-in-text">0.25]</code>, which we’ll add to the previous one. This is done geometrically by chaining together the vector arrows, with the resulting location being the vector representing the sum of the previous two vectors (see figure 2.8). As you can see, adding a vector B to a vector A represents the action of copying point A in a new location, whose distance and direction from the original point A is determined by the vector B. If you apply the same vector addition to a group of points in the plane (an “object”), you would be creating a copy of the entire object in a new location (see figure 2.9). Tensor addition thus represents the action <a id="marker-1042065"></a>of <i class="fm-italics">translating an object</i> (moving the object without distorting it) by a certain amount in a certain direction.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/02-08.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1041986"></a>Figure 2.8 Geometric interpretation of the sum of two vectors</p>

  <p class="body"><a id="pgfId-1018483"></a>In general, elementary geometric operations such as translation, rotation, scaling, skewing, and so on can be expressed as tensor operations. Here are a few examples:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018503"></a><i class="fm-italics1">Translation</i>: As you just saw, adding a vector to a point will move the point by a fixed amount in a fixed direction. Applied to a set of points (such as a 2D object), this is called a “translation” (see figure 2.9).</p>

      <p class="fm-figure"><img alt="" class="calibre13" src="../Images/02-09.png"/></p>

      <p class="fm-figure-caption"><a id="pgfId-1042142"></a>Figure 2.9 2D translation as a vector addition</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018542"></a><i class="fm-italics1">Rotation</i>: A counterclockwise rotation of a 2D vector by an angle theta (see figure 2.10) can be achieved via a dot product with a 2 × 2 matrix <code class="fm-code-in-text">R</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">[[cos(theta),</code> <code class="fm-code-in-text">-sin(theta)],</code> <code class="fm-code-in-text">[sin(theta),</code> <code class="fm-code-in-text">cos(theta)]]</code>.</p>

      <p class="fm-figure"><img alt="" class="calibre13" src="../Images/02-10.png"/></p>

      <p class="fm-figure-caption"><a id="pgfId-1042203"></a>Figure 2.10 2D rotation (counterclockwise) as a dot product</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018603"></a><i class="fm-italics1">Scaling</i>: A vertical and <a class="calibre11" id="marker-1040372"></a>horizontal scaling of the image (see figure 2.11) can be achieved via a dot product with a 2 × 2 matrix <code class="fm-code-in-text">S</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">[[horizontal_factor,</code> <code class="fm-code-in-text">0],</code> <code class="fm-code-in-text">[0,</code> <code class="fm-code-in-text">vertical_factor]]</code> (note that such a matrix is called a “diagonal matrix,” because it only has non-zero coefficients in its “diagonal,” going from the top left to the bottom right).</p>

      <p class="fm-figure"><img alt="" class="calibre13" src="../Images/02-11.png"/></p>

      <p class="fm-figure-caption"><a id="pgfId-1042249"></a>Figure 2.11 2D scaling as a dot product</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018674"></a><i class="fm-italics1">Linear transform</i>: A dot product <a class="calibre11" id="marker-1040380"></a>with an arbitrary matrix implements a linear transform. Note that <i class="fm-italics1">scaling</i> and <i class="fm-italics1">rotation</i>, listed previously, are by definition linear transforms.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018731"></a><i class="fm-italics1">Affine transform</i>: An affine <a class="calibre11" id="marker-1035890"></a>transform (see figure 2.12) is the combination of a linear transform (achieved via a dot product with some matrix) and a translation (achieved via a vector addition). As you have probably recognized, that’s exactly the <code class="fm-code-in-text">y</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">W</code> <code class="fm-code-in-text">•</code> <code class="fm-code-in-text">x</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">b</code> computation implemented by the <code class="fm-code-in-text">Dense</code> layer! A <code class="fm-code-in-text">Dense</code> layer without an activation function is an affine layer.</p>

      <p class="fm-figure"><img alt="" class="calibre13" src="../Images/02-12.png"/></p>

      <p class="fm-figure-caption"><a id="pgfId-1042295"></a>Figure 2.12 Affine transform in the plane</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018855"></a><i class="fm-italics1">Dense layer with</i> <code class="fm-code-in-text">relu</code> <i class="fm-italics1">activation</i>: An important observation about affine transforms is that if you apply many of them repeatedly, you still end up with an affine transform (so you could just have applied that one affine transform in the first place). Let’s try it with two: <code class="fm-code-in-text">affine2(affine1(x))</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">W2</code> <code class="fm-code-in-text">•</code> <code class="fm-code-in-text">(W1</code> <code class="fm-code-in-text">•</code> <code class="fm-code-in-text">x</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">b1)</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">b2</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">(W2</code> <code class="fm-code-in-text">•</code> <code class="fm-code-in-text">W1)</code> <code class="fm-code-in-text">•</code> <code class="fm-code-in-text">x</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">(W2</code> <code class="fm-code-in-text">•</code> <code class="fm-code-in-text">b1</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">b2)</code>. That’s an affine transform where the linear part is the matrix <code class="fm-code-in-text">W2</code> <code class="fm-code-in-text">•</code> <code class="fm-code-in-text">W1</code> and the translation part is the vector <code class="fm-code-in-text">W2</code> <code class="fm-code-in-text">•</code> <code class="fm-code-in-text">b1</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">b2</code>. As a consequence, a multilayer neural network made entirely of <code class="fm-code-in-text">Dense</code> layers without activations would be equivalent to a single <code class="fm-code-in-text">Dense</code> layer. This “deep” neural network would just be a linear model in disguise! This is why we need activation functions, like <code class="fm-code-in-text">relu</code> (seen in action in figure 2.13). Thanks to activation functions, a chain of <code class="fm-code-in-text">Dense</code> layers can be made to implement very complex, non-linear geometric transformations, resulting in very rich hypothesis spaces for your deep neural networks. We’ll cover this idea in more detail in the next chapter. <a class="calibre11" id="marker-1035877"></a><a class="calibre11" id="marker-1035878"></a><a class="calibre11" id="marker-1035879"></a></p>

      <p class="fm-figure"><img alt="" class="calibre13" src="../Images/02-13.png"/></p>

      <p class="fm-figure-caption"><a id="pgfId-1042344"></a>Figure 2.13 Affine transform followed by <code class="fm-code-in-text">relu</code> activation</p>
    </li>
  </ul>

  <h3 class="fm-head1" id="heading_id_23"><a id="pgfId-1018890"></a>2.3.6 A geometric interpretation of deep learning</h3>

  <p class="body"><a id="pgfId-1018929"></a><a id="marker-1018917"></a><a id="marker-1018919"></a><a id="marker-1018921"></a><a id="marker-1018923"></a><a id="marker-1018925"></a>You just learned that neural networks consist entirely of chains of tensor operations, and that these tensor operations are just simple geometric transformations of the input data. It follows that you can interpret a neural network as a very complex geometric transformation in a high-dimensional space, implemented via a series of simple steps.</p>

  <p class="body"><a id="pgfId-1018934"></a>In 3D, the following mental image may prove useful. Imagine two sheets of colored paper: one red and one blue. Put one on top of the other. Now crumple them together into a small ball. That crumpled paper ball is your input data, and each sheet of paper is a class of data in a classification problem. What a neural network is meant to do is figure out a transformation of the paper ball that would uncrumple it, so as to make the two classes cleanly separable again (see figure 2.14). With deep learning, this would be implemented as a series of simple transformations of the 3D space, such as those you could apply on the paper ball with your fingers, one movement at a time.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/02-14.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1042386"></a>Figure 2.14 Uncrumpling a complicated manifold of data</p>

  <p class="body"><a id="pgfId-1018977"></a>Uncrumpling paper balls is what machine learning is about: finding neat representations for complex, highly folded <a id="marker-1018966"></a>data <i class="fm-italics">manifolds</i> in high-dimensional spaces (a manifold is a continuous surface, like our crumpled sheet of paper). At this point, you should have a pretty good intuition as to why deep learning excels at this: it takes the approach of incrementally decomposing a complicated geometric transformation into a long chain of elementary ones, which is pretty much the strategy a human would follow to uncrumple a paper ball. Each layer in a deep network applies a transformation that disentangles the data a little, and a deep stack of layers makes tractable an extremely complicated disentanglement process. <a id="marker-1018985"></a><a id="marker-1018987"></a><a id="marker-1018989"></a><a id="marker-1018991"></a><a id="marker-1018993"></a></p>

  <h2 class="fm-head" id="heading_id_24"><a id="pgfId-1018999"></a>2.4 The engine of neural networks: Gradient-based optimization</h2>

  <p class="body"><a id="pgfId-1019016"></a><a id="marker-1019010"></a><a id="marker-1019012"></a>As you saw in the previous section, each neural layer from our first model example transforms its input data as follows:</p>
  <pre class="programlisting"><a id="pgfId-1019021"></a>output = relu(dot(input, W) + b)</pre>

  <p class="body"><a id="pgfId-1019103"></a>In this expression, <code class="fm-code-in-text">W</code> and <code class="fm-code-in-text">b</code> are tensors that are attributes of the <a id="marker-1019056"></a>layer. They’re called the <i class="fm-italics">weights</i> or <i class="fm-italics">trainable parameters</i> of the <a id="marker-1019082"></a>layer (the <code class="fm-code-in-text">kernel</code> and <code class="fm-code-in-text">bias</code> attributes, respectively). These weights contain the information learned by the model from exposure to training data.</p>

  <p class="body"><a id="pgfId-1019158"></a>Initially, these weight matrices are filled with small random values (a step called <i class="fm-italics">random initialization</i>). Of course, there’s no reason to expect that <code class="fm-code-in-text">relu(dot(input,</code> <code class="fm-code-in-text">W)</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">b)</code>, when <code class="fm-code-in-text">W</code> and <code class="fm-code-in-text">b</code> are random, will yield any useful representations. The resulting representations are meaningless—but they’re a starting point. What comes next is to gradually adjust these weights, based on a feedback signal. This gradual adjustment, also called <i class="fm-italics">training</i>, is the learning that machine learning is all about.</p>

  <p class="body"><a id="pgfId-1019167"></a>This happens within what’s called a <i class="fm-italics">training loop</i>, which works <a id="marker-1019178"></a>as follows. Repeat these steps in a loop, until the loss seems sufficiently low:</p>

  <ol class="calibre14">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1019212"></a>Draw a batch of training samples, <code class="fm-code-in-text">x</code>, and corresponding targets, <code class="fm-code-in-text">y_true</code>.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1019251"></a>Run the model on <code class="fm-code-in-text">x</code> (a step called the <i class="fm-italics1">forward pass</i>) to obtain predictions, <code class="fm-code-in-text">y_pred</code>.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1019280"></a>Compute the loss of the model on the batch, a measure of the mismatch between <code class="fm-code-in-text">y_pred</code> and <code class="fm-code-in-text">y_true</code>.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1019289"></a>Update all weights of the model in a way that slightly reduces the loss on this batch.</p>
    </li>
  </ol>

  <p class="body"><a id="pgfId-1019315"></a>You’ll eventually end up with a model that has a very low loss on its training data: a low mismatch between predictions, <code class="fm-code-in-text">y_pred</code>, and expected targets, <code class="fm-code-in-text">y_true</code>. The model has “learned” to map its inputs to correct targets. From afar, it may look like magic, but when you reduce it to elementary steps, it turns out to be simple.</p>

  <p class="body"><a id="pgfId-1019324"></a>Step 1 sounds easy enough—just I/O code. Steps 2 and 3 are merely the application of a handful of tensor operations, so you could implement these steps purely from what you learned in the previous section. The difficult part is step 4: updating the model’s weights. Given an individual weight coefficient in the model, how can you compute whether the coefficient should be increased or decreased, and by how much?</p>

  <p class="body"><a id="pgfId-1019330"></a>One naive solution would be to freeze all weights in the model except the one scalar coefficient being considered, and try different values for this coefficient. Let’s say the initial value of the coefficient is 0.3. After the forward pass on a batch of data, the loss of the model on the batch is 0.5. If you change the coefficient’s value to 0.35 and rerun the forward pass, the loss increases to 0.6. But if you lower the coefficient to 0.25, the loss falls to 0.4. In this case, it seems that updating the coefficient by –0.05 would contribute to minimizing the loss. This would have to be repeated for all coefficients in the model.</p>

  <p class="body"><a id="pgfId-1019349"></a>But such an approach would be horribly inefficient, because you’d need to compute two forward passes (which are expensive) for every individual coefficient (of which there are many, usually thousands and sometimes up to millions). Thankfully, there’s a much better <a id="marker-1019338"></a>approach: <i class="fm-italics">gradient descent</i>.</p>

  <p class="body"><a id="pgfId-1019444"></a>Gradient descent is the optimization technique that powers modern neural networks. Here’s the gist of it. All of the functions used in our models (such as <code class="fm-code-in-text">dot</code> or <code class="fm-code-in-text">+</code>) transform their input in a smooth and continuous way: if you look at <code class="fm-code-in-text">z</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">x</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">y</code>, for instance, a small change in <code class="fm-code-in-text">y</code> only results in a small change in <code class="fm-code-in-text">z</code>, and if you know the direction of the change in <code class="fm-code-in-text">y</code>, you can infer the direction of the change in <code class="fm-code-in-text">z</code>. Mathematically, you’d say these functions are <i class="fm-italics">differentiable</i>. If you chain together such functions, the bigger function you obtain is still differentiable. In particular, this applies to the function that maps the model’s coefficients to the loss of the model on a batch of data: a small change in the model’s coefficients results in a small, predictable change in the loss value. This enables you to use a mathematical operator called the <i class="fm-italics">gradient</i> to describe how the loss varies as you move the model’s coefficients in different directions. If you compute this gradient, you can use it to move the coefficients (all at once in a single update, rather than one at a time) in a direction that decreases the loss.</p>

  <p class="body"><a id="pgfId-1019469"></a>If you already know what <i class="fm-italics">differentiable</i> means and what a <i class="fm-italics">gradient</i> is, you can skip to section 2.4.3. Otherwise, the following two sections will help you understand these concepts.</p>

  <h3 class="fm-head1" id="heading_id_25"><a id="pgfId-1019478"></a>2.4.1 What’s a derivative?</h3>

  <p class="body"><a id="pgfId-1019523"></a><a id="marker-1019489"></a><a id="marker-1019491"></a><a id="marker-1019493"></a>Consider a continuous, smooth function <code class="fm-code-in-text">f(x)</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">y</code>, mapping a number, <code class="fm-code-in-text">x</code>, to a new number, <code class="fm-code-in-text">y</code>. We can use the function in figure 2.15 as an example.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/02-15.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1042428"></a>Figure 2.15 A continuous, smooth function</p>

  <p class="body"><a id="pgfId-1019632"></a>Because the function is <i class="fm-italics">continuous</i>, a small change in <code class="fm-code-in-text">x</code> can only result in a small change in <code class="fm-code-in-text">y</code>—that’s the intuition behind <i class="fm-italics">continuity</i>. Let’s say you increase <code class="fm-code-in-text">x</code> by a small factor, <code class="fm-code-in-text">epsilon_x</code>: this results in a small <code class="fm-code-in-text">epsilon_y</code> change to <code class="fm-code-in-text">y</code>, as shown in figure 2.16.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/02-16.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1042470"></a>Figure 2.16 With a continuous function, a small change in <code class="fm-code-in-text">x</code> results in a small change in <code class="fm-code-in-text">y</code>.</p>

  <p class="body"><a id="pgfId-1019750"></a>In addition, because the function is <i class="fm-italics">smooth</i> (its curve doesn’t have any abrupt angles), when <code class="fm-code-in-text">epsilon_x</code> is small enough, around a certain point <code class="fm-code-in-text">p</code>, it’s possible to approximate <code class="fm-code-in-text">f</code> as a linear function of slope <code class="fm-code-in-text">a</code>, so that <code class="fm-code-in-text">epsilon_y</code> becomes <code class="fm-code-in-text">a</code> <code class="fm-code-in-text">*</code> <code class="fm-code-in-text">epsilon_x</code>:</p>
  <pre class="programlisting"><a id="pgfId-1019759"></a>f(x + epsilon_x) = y + a * epsilon_x</pre>

  <p class="body"><a id="pgfId-1019789"></a>Obviously, this linear approximation is valid only when <code class="fm-code-in-text">x</code> is close enough to <code class="fm-code-in-text">p</code>.</p>

  <p class="body"><a id="pgfId-1019924"></a>The slope <code class="fm-code-in-text">a</code> is called the <i class="fm-italics">derivative</i> of <code class="fm-code-in-text">f</code> in <code class="fm-code-in-text">p</code>. If <code class="fm-code-in-text">a</code> is negative, it means a small increase in <code class="fm-code-in-text">x</code> around <code class="fm-code-in-text">p</code> will result in a decrease of <code class="fm-code-in-text">f(x)</code> (as shown in figure 2.17), and if <code class="fm-code-in-text">a</code> is positive, a small increase in <code class="fm-code-in-text">x</code> will result in an increase of <code class="fm-code-in-text">f(x)</code>. Further, the absolute value of <code class="fm-code-in-text">a</code> (the <i class="fm-italics">magnitude</i> of the derivative) tells you how quickly this increase or decrease will happen.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/02-17.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1042512"></a>Figure 2.17 Derivative of <code class="fm-code-in-text">f</code> in <code class="fm-code-in-text">p</code></p>

  <p class="body"><a id="pgfId-1020059"></a>For every differentiable function <code class="fm-code-in-text">f(x)</code> (<i class="fm-italics">differentiable</i> means “can be derived”: for example, smooth, continuous functions can be derived), there exists a derivative function <code class="fm-code-in-text">f'(x)</code>, that maps values of <code class="fm-code-in-text">x</code> to the slope of the local linear approximation of <code class="fm-code-in-text">f</code> in those points. For instance, the derivative of <code class="fm-code-in-text">cos(x)</code> is <code class="fm-code-in-text">-sin(x)</code>, the derivative of <code class="fm-code-in-text">f(x)</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">a</code> <code class="fm-code-in-text">*</code> <code class="fm-code-in-text">x</code> is <code class="fm-code-in-text">f'(x)</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">a</code>, and so on.</p>

  <p class="body"><a id="pgfId-1020180"></a>Being able to derive functions is a very powerful tool when it comes to <i class="fm-italics">optimization</i>, the task <a id="marker-1020079"></a>of finding values of <code class="fm-code-in-text">x</code> that minimize the value of <code class="fm-code-in-text">f(x)</code>. If you’re trying to update <code class="fm-code-in-text">x</code> by a factor <code class="fm-code-in-text">epsilon_x</code> in order to minimize <code class="fm-code-in-text">f(x)</code>, and you know the derivative of <code class="fm-code-in-text">f</code>, then your job is done: the derivative completely describes how <code class="fm-code-in-text">f(x)</code> evolves as you change <code class="fm-code-in-text">x</code>. If you want to reduce the value of <code class="fm-code-in-text">f(x)</code>, you just need to move <code class="fm-code-in-text">x</code> a little in the opposite direction from the derivative. <a id="marker-1020185"></a><a id="marker-1020188"></a></p>

  <h3 class="fm-head1" id="heading_id_26"><a id="pgfId-1020194"></a>2.4.2 Derivative of a tensor operation: The gradient</h3>

  <p class="body"><a id="pgfId-1020275"></a><a id="marker-1020205"></a><a id="marker-1020207"></a><a id="marker-1020209"></a>The function we were just looking at turned a scalar value <code class="fm-code-in-text">x</code> into another scalar value <code class="fm-code-in-text">y</code>: you could plot it as a curve in a 2D plane. Now imagine a function that turns a tuple of scalars <code class="fm-code-in-text">(x,</code> <code class="fm-code-in-text">y)</code> into a scalar value <code class="fm-code-in-text">z</code>: that would be a vector operation. You could plot it as a 2D <i class="fm-italics">surface</i> in a 3D space (indexed by coordinates <code class="fm-code-in-text">x,</code> <code class="fm-code-in-text">y,</code> <code class="fm-code-in-text">z</code>). Likewise, you can imagine functions that take matrices as inputs, functions that take rank-3 tensors as inputs, etc.</p>

  <p class="body"><a id="pgfId-1020316"></a>The concept of derivation can be applied to any such function, as long as the surfaces they describe are continuous and smooth. The derivative of a tensor operation (or tensor function) is called a <i class="fm-italics">gradient</i>. Gradients are just the generalization of the concept of derivatives to functions that take tensors as inputs. Remember how, for a scalar function, the derivative represents the <i class="fm-italics">local slope</i> of the <a id="marker-1020305"></a>curve of the function? In the same way, the gradient of a tensor function represents the <i class="fm-italics">curvature</i> of the multidimensional surface described by the function. It characterizes how the output of the function varies when its input parameters vary.</p>

  <p class="body"><a id="pgfId-1020331"></a>Let’s look at an example grounded in machine learning. Consider</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1020337"></a>An input vector, <code class="fm-code-in-text">x</code> (a sample in a dataset)</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1020360"></a>A matrix, <code class="fm-code-in-text">W</code> (the weights of a model)</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1020399"></a>A target, <code class="fm-code-in-text">y_true</code> (what the model should learn to associate to <code class="fm-code-in-text">x</code>)</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1020428"></a>A loss function, <code class="fm-code-in-text">loss</code> (meant to measure the gap between the model’s current predictions and <code class="fm-code-in-text">y_true</code>)</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1020473"></a>You can use <code class="fm-code-in-text">W</code> to compute a target candidate <code class="fm-code-in-text">y_pred</code>, and then compute the loss, or mismatch, between the target candidate <code class="fm-code-in-text">y_pred</code> and the target <code class="fm-code-in-text">y_true</code>:</p>
  <pre class="programlisting"><a id="pgfId-1020482"></a>y_pred = dot(W, x)                    <span class="fm-combinumeral">❶</span>
<a id="pgfId-1020502"></a>loss_value = loss(y_pred, y_true)     <span class="fm-combinumeral">❷</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1038534"></a><span class="fm-combinumeral">❶</span> We use the model weights, W, to make a prediction for x.</p>

  <p class="fm-code-annotation"><a id="pgfId-1038551"></a><span class="fm-combinumeral">❷</span> We estimate how far off the prediction was.</p>

  <p class="body"><a id="pgfId-1020566"></a>Now we’d like to use gradients to figure out how to update <code class="fm-code-in-text">W</code> so as to make <code class="fm-code-in-text">loss_value</code> smaller. How do we do that?</p>

  <p class="body"><a id="pgfId-1020601"></a>Given fixed inputs <code class="fm-code-in-text">x</code> and <code class="fm-code-in-text">y_true</code>, the preceding operations can be interpreted as a function mapping values of <code class="fm-code-in-text">W</code> (the model’s weights) to loss values:</p>
  <pre class="programlisting"><a id="pgfId-1020610"></a>loss_value = f(W)    <span class="fm-combinumeral">❶</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1038510"></a><span class="fm-combinumeral">❶</span> f describes the curve (or high-dimensional surface) formed by loss values when W varies.</p>

  <p class="body"><a id="pgfId-1020796"></a>Let’s say the current value of <code class="fm-code-in-text">W</code> is <code class="fm-code-in-text">W0</code>. Then the derivative of <code class="fm-code-in-text">f</code> at the point <code class="fm-code-in-text">W0</code> is a tensor <code class="fm-code-in-text">grad(loss_value,</code> <code class="fm-code-in-text">W0)</code>, with the same shape as <code class="fm-code-in-text">W</code>, where each coefficient <code class="fm-code-in-text">grad(loss_value,</code> <code class="fm-code-in-text">W0)[i,</code> <code class="fm-code-in-text">j]</code> indicates the direction and magnitude of the change in <code class="fm-code-in-text">loss_value</code> you observe when modifying <code class="fm-code-in-text">W0[i,</code> <code class="fm-code-in-text">j]</code>. That tensor <code class="fm-code-in-text">grad(loss_value,</code> <code class="fm-code-in-text">W0)</code> is the gradient of the function <code class="fm-code-in-text">f(W)</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">loss_value</code> in <code class="fm-code-in-text">W0</code>, also called “gradient of <code class="fm-code-in-text">loss_value</code> with respect to <code class="fm-code-in-text">W</code> around <code class="fm-code-in-text">W0</code>.”</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre11" id="pgfId-1020805"></a>Partial derivatives</p>

    <p class="fm-sidebar-text"><a id="pgfId-1020911"></a>The tensor operation <code class="fm-code-in-text1">grad(f(W),</code> <code class="fm-code-in-text1">W)</code> (which takes as input a matrix <code class="fm-code-in-text1">W</code>) can be expressed as a combination of scalar functions, <code class="fm-code-in-text1">grad_ij(f(W),</code> <code class="fm-code-in-text1">w_ij)</code>, each of which would return the derivative of <code class="fm-code-in-text1">loss_value</code> <code class="fm-code-in-text1">=</code> <code class="fm-code-in-text1">f(W)</code> with respect to the coefficient <code class="fm-code-in-text1">W[i,</code> <code class="fm-code-in-text1">j]</code> of <code class="fm-code-in-text1">W</code>, assuming all other coefficients are constant. <code class="fm-code-in-text1">grad_ij</code> is called the <i class="fm-italics">partial derivative</i> of <code class="fm-code-in-text1">f</code> with respect to <code class="fm-code-in-text1">W[i,</code> <code class="fm-code-in-text1">j]</code>.</p>
  </div>

  <p class="body"><a id="pgfId-1020996"></a>Concretely, what does <code class="fm-code-in-text">grad(loss_value,</code> <code class="fm-code-in-text">W0)</code> represent? You saw earlier that the derivative of a function <code class="fm-code-in-text">f(x)</code> of a single coefficient can be interpreted as the slope of the curve of <code class="fm-code-in-text">f</code>. Likewise, <code class="fm-code-in-text">grad(loss_value,</code> <code class="fm-code-in-text">W0)</code> can be interpreted as the tensor describing the <i class="fm-italics">direction of steepest ascent</i> of <code class="fm-code-in-text">loss_value</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">f(W)</code> around <code class="fm-code-in-text">W0</code>, as well as the slope of this ascent. Each partial derivative describes the slope of <code class="fm-code-in-text">f</code> in a specific direction.</p>

  <p class="body"><a id="pgfId-1021121"></a>For this reason, in much the same way that, for a function <code class="fm-code-in-text">f(x)</code>, you can reduce the value of <code class="fm-code-in-text">f(x)</code> by moving <code class="fm-code-in-text">x</code> a little in the opposite direction from the derivative, with a function <code class="fm-code-in-text">f(W)</code> of a tensor, you can reduce <code class="fm-code-in-text">loss_value</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">f(W)</code> by moving <code class="fm-code-in-text">W</code> in the opposite direction from the gradient: for example, <code class="fm-code-in-text">W1</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">W0</code> <code class="fm-code-in-text">-</code> <code class="fm-code-in-text">step</code> <code class="fm-code-in-text">*</code> <code class="fm-code-in-text">grad(f(W0),</code> <code class="fm-code-in-text">W0)</code> (where <code class="fm-code-in-text">step</code> is a small scaling factor). That means going against the direction of steepest ascent of <code class="fm-code-in-text">f</code>, which intuitively should put you lower on the curve. Note that the scaling factor <code class="fm-code-in-text">step</code> is needed because <code class="fm-code-in-text">grad(loss_value,</code> <code class="fm-code-in-text">W0)</code> only approximates the curvature when you’re close to <code class="fm-code-in-text">W0</code>, so you don’t want to get too far from <code class="fm-code-in-text">W0</code>. <a id="marker-1037088"></a><a id="marker-1037089"></a><a id="marker-1037090"></a><a id="marker-1037091"></a></p>

  <h3 class="fm-head1" id="heading_id_27"><a id="pgfId-1021139"></a>2.4.3 Stochastic gradient descent</h3>

  <p class="body"><a id="pgfId-1021158"></a><a id="marker-1021150"></a><a id="marker-1021152"></a><a id="marker-1021154"></a>Given a differentiable function, it’s theoretically possible to find its minimum analytically: it’s known that a function’s minimum is a point where the derivative is 0, so all you have to do is find all the points where the derivative goes to 0 and check for which of these points the function has the lowest value.</p>

  <p class="body"><a id="pgfId-1021219"></a>Applied to a neural network, that means finding analytically the combination of weight values that yields the smallest possible loss function. This can be done by solving the equation <code class="fm-code-in-text">grad(f(W),</code> <code class="fm-code-in-text">W)</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">0</code> for <code class="fm-code-in-text">W</code>. This is a polynomial equation of <code class="fm-code-in-text">N</code> variables, where <code class="fm-code-in-text">N</code> is the number of coefficients in the model. Although it would be possible to solve such an equation for <code class="fm-code-in-text">N</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">2</code> or <code class="fm-code-in-text">N</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">3</code>, doing so is intractable for real neural networks, where the number of parameters is never less than a few thousand and can often be several tens of millions.</p>

  <p class="body"><a id="pgfId-1021228"></a>Instead, you can use the four-step algorithm outlined at the beginning of this section: modify the parameters little by little based on the current loss value for a random batch of data. Because you’re dealing with a differentiable function, you can compute its gradient, which gives you an efficient way to implement step 4. If you update the weights in the opposite direction from the gradient, the loss will be a little less every time:</p>

  <ol class="calibre14">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1021258"></a>Draw a batch of training samples, <code class="fm-code-in-text">x</code>, and corresponding targets, <code class="fm-code-in-text">y_true</code>.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1021297"></a>Run the model on <code class="fm-code-in-text">x</code> to obtain predictions, <code class="fm-code-in-text">y_pred</code> (this is called the <i class="fm-italics1">forward pass</i>).</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1021326"></a>Compute the loss of the model on the batch, a measure of the mismatch between <code class="fm-code-in-text">y_pred</code> and <code class="fm-code-in-text">y_true</code>.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1021335"></a>Compute the gradient of the loss with regard to the model’s parameters (this is called the <i class="fm-italics1">backward pass</i>).</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1021390"></a>Move the parameters a little in the opposite direction from the gradient—for example, <code class="fm-code-in-text">W</code> <code class="fm-code-in-text">-=</code> <code class="fm-code-in-text">learning_rate</code> <code class="fm-code-in-text">*</code> <code class="fm-code-in-text">gradient</code>—thus reducing the loss on the batch <a class="calibre11" id="marker-1021369"></a>a bit. The <i class="fm-italics1">learning rate</i> (<code class="fm-code-in-text">learning_rate</code> here) would be a scalar factor modulating the “speed” of the gradient descent process.</p>
    </li>
  </ol>

  <p class="body"><a id="pgfId-1021442"></a>Easy enough! What we just described is <a id="marker-1021401"></a>called <i class="fm-italics">mini-batch stochastic gradient descent</i> (mini-batch SGD). The term <i class="fm-italics">stochastic</i> refers to the fact that each batch of data is drawn at random (<i class="fm-italics">stochastic</i> is a scientific synonym of <i class="fm-italics">random</i>). Figure 2.18 illustrates what happens in 1D, when the model has only one parameter and you have only one training sample.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/02-18.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1042554"></a>Figure 2.18 SGD down a 1D loss curve (one learnable parameter)</p>

  <p class="body"><a id="pgfId-1021498"></a>As you can see, intuitively it’s important to pick a reasonable value for the <code class="fm-code-in-text">learning_ rate</code> factor. If it’s too small, the descent down the curve will take many iterations, and it could get stuck in a local minimum. If <code class="fm-code-in-text">learning_rate</code> is too large, your updates may end up taking you to completely random locations on the curve.</p>

  <p class="body"><a id="pgfId-1021543"></a>Note that a variant of the mini-batch SGD algorithm would be to draw a single sample and target at each iteration, rather than drawing a batch of data. This would be <i class="fm-italics">true</i> SGD (as opposed to <i class="fm-italics">mini-batch</i> SGD). Alternatively, going to the opposite extreme, you could run every step on <i class="fm-italics">all</i> data available, which is called <i class="fm-italics">batch gradient descent</i>. Each update <a id="marker-1021548"></a>would then be more accurate, but far more expensive. The efficient compromise between these two extremes is to use mini-batches of reasonable size.</p>

  <p class="body"><a id="pgfId-1021558"></a>Although figure 2.18 illustrates gradient descent in a 1D parameter space, in practice you’ll use gradient descent in highly dimensional spaces: every weight coefficient in a neural network is a free dimension in the space, and there may be tens of thousands or even millions of them. To help you build intuition about loss surfaces, you can also visualize gradient descent along a 2D loss surface, as shown in figure 2.19. But you can’t possibly visualize what the actual process of training a neural network looks like—you can’t represent a 1,000,000-dimensional space in a way that makes sense to humans. As such, it’s good to keep in mind that the intuitions you develop through these low-dimensional representations may not always be accurate in practice. This has historically been a source of issues in the world of deep learning research.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/02-19.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1042599"></a>Figure 2.19 Gradient descent down a 2D loss surface (two learnable parameters)</p>

  <p class="body"><a id="pgfId-1021621"></a>Additionally, there exist multiple variants of SGD that differ by taking into account previous weight updates when computing the next weight update, rather than just looking at the current value of the gradients. There is, for instance, SGD with momentum, as well as Adagrad, RMSprop, and several others. Such variants are known <a id="marker-1021590"></a>as <i class="fm-italics">optimization methods</i> or <i class="fm-italics">optimizers</i>. In particular, the concept of <i class="fm-italics">momentum</i>, which is <a id="marker-1021626"></a>used in many of these variants, deserves your attention. Momentum addresses two issues with SGD: convergence speed and local minima. Consider figure 2.20, which shows the curve of a loss as a function of a model parameter.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/02-20.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1042641"></a>Figure 2.20 A local minimum and a global minimum</p>

  <p class="body"><a id="pgfId-1021646"></a>As you can see, around a certain parameter value, there is a <i class="fm-italics">local minimum</i>: around that point, moving left would result in the loss increasing, but so would moving right. If the parameter under consideration were being optimized via SGD with a small learning rate, the optimization process could get stuck at the local minimum instead of making its way to the global minimum.</p>

  <p class="body"><a id="pgfId-1021675"></a>You can avoid such issues by using momentum, which draws inspiration from physics. A useful mental image here is to think of the optimization process as a small ball rolling down the loss curve. If it has enough momentum, the ball won’t get stuck in a ravine and will end up at the global minimum. Momentum is implemented by moving the ball at each step based not only on the current slope value (current acceleration) but also on the current velocity (resulting from past acceleration). In practice, this means updating the parameter <code class="fm-code-in-text">w</code> based not only on the current gradient value but also on the previous parameter update, such as in this naive implementation:</p>
  <pre class="programlisting"><a id="pgfId-1034492"></a>past_velocity = <span class="fm-codeblue">0.</span> 
<a id="pgfId-1021704"></a>momentum = <span class="fm-codeblue">0.1</span>                <span class="fm-combinumeral">❶</span>
<a id="pgfId-1021716"></a><b class="fm-codebrown">while</b> loss &gt; <span class="fm-codeblue">0.01</span>:            <span class="fm-combinumeral">❷</span>
<a id="pgfId-1034521"></a>    w, loss, gradient = get_current_parameters()
<a id="pgfId-1034522"></a>    velocity = past_velocity * momentum - learning_rate * gradient
<a id="pgfId-1034523"></a>    w = w + momentum * velocity - learning_rate * gradient
<a id="pgfId-1034524"></a>    past_velocity = velocity
<a id="pgfId-1021752"></a>    update_parameter(w)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1038377"></a><span class="fm-combinumeral">❶</span> Constant momentum factor</p>

  <p class="fm-code-annotation"><a id="pgfId-1038405"></a><span class="fm-combinumeral">❷</span> Optimization loop <a id="marker-1038409"></a><a id="marker-1038410"></a><a id="marker-1038411"></a></p>

  <h3 class="fm-head1" id="heading_id_28"><a id="pgfId-1021801"></a>2.4.4 Chaining derivatives: The Backpropagation algorithm</h3>

  <p class="body"><a id="pgfId-1021826"></a><a id="marker-1021812"></a><a id="marker-1021814"></a><a id="marker-1021816"></a>In the preceding algorithm, we casually assumed that because a function is differentiable, we can easily compute its gradient. But is that true? How can we compute the gradient of complex expressions in practice? In the two-layer model we started the chapter with, how can we get the gradient of the loss with regard to the weights? That’s where the <i class="fm-italics">Backpropagation algorithm</i> comes in.</p>

  <p class="fm-head2"><a id="pgfId-1021835"></a>The chain rule</p>

  <p class="body"><a id="pgfId-1021968"></a><a id="marker-1021846"></a><a id="marker-1021848"></a><a id="marker-1021850"></a>Backpropagation is a way to use the derivatives of simple operations (such as addition, relu, or tensor product) to easily compute the gradient of arbitrarily complex combinations of these atomic operations. Crucially, a neural network consists of many tensor operations chained together, each of which has a simple, known derivative. For instance, the model defined in listing 2.2 can be expressed as a function parameterized by the variables <code class="fm-code-in-text">W1</code>, <code class="fm-code-in-text">b1</code>, <code class="fm-code-in-text">W2</code>, and <code class="fm-code-in-text">b2</code> (belonging to the first and second <code class="fm-code-in-text">Dense</code> layers respectively), involving the <a id="marker-1021905"></a>atomic operations <code class="fm-code-in-text">dot</code>, <code class="fm-code-in-text">relu</code>, <code class="fm-code-in-text">softmax</code>, and <code class="fm-code-in-text">+</code>, as well <a id="marker-1021951"></a>as our loss function <code class="fm-code-in-text">loss</code>, which are all easily differentiable:</p>
  <pre class="programlisting"><a id="pgfId-1021977"></a>loss_value = loss(y_true, softmax(dot(relu(dot(inputs, W1) + b1), W2) + b2))</pre>

  <p class="body"><a id="pgfId-1022004"></a>Calculus tells us that such a chain of functions can be derived using the following <a id="marker-1021993"></a>identity, called the <i class="fm-italics">chain rule</i>.</p>

  <p class="body"><a id="pgfId-1022068"></a>Consider two functions <code class="fm-code-in-text">f</code> and <code class="fm-code-in-text">g</code>, as well as the composed function <code class="fm-code-in-text">fg</code> such that <code class="fm-code-in-text">fg(x)</code> <code class="fm-code-in-text">==</code> <code class="fm-code-in-text">f(g(x))</code>:</p>
  <pre class="programlisting"><a id="pgfId-1034539"></a><b class="fm-codebrown">def</b> fg(x):
<a id="pgfId-1034540"></a>    x1 = g(x)
<a id="pgfId-1034541"></a>    y = f(x1)
<a id="pgfId-1022103"></a>    <b class="fm-codebrown">return</b> y</pre>

  <p class="body"><a id="pgfId-1022145"></a>Then the chain rule states that <code class="fm-code-in-text">grad(y,</code> <code class="fm-code-in-text">x)</code> <code class="fm-code-in-text">==</code> <code class="fm-code-in-text">grad(y,</code> <code class="fm-code-in-text">x1)</code> <code class="fm-code-in-text">*</code> <code class="fm-code-in-text">grad(x1,</code> <code class="fm-code-in-text">x)</code>. This enables you to compute the derivative of <code class="fm-code-in-text">fg</code> as long as you know the derivatives of <code class="fm-code-in-text">f</code> and <code class="fm-code-in-text">g</code>. The chain rule is named as it is because when you add more intermediate functions, it starts looking like a chain:</p>
  <pre class="programlisting"><a id="pgfId-1034560"></a><b class="fm-codebrown">def</b> fghj(x):
<a id="pgfId-1022168"></a>    x1 = j(x)
<a id="pgfId-1034577"></a>    x2 = h(x1)
<a id="pgfId-1034578"></a>    x3 = g(x2)
<a id="pgfId-1034579"></a>    y = f(x3)
<a id="pgfId-1022192"></a>    <b class="fm-codebrown">return</b> y
<a id="pgfId-1022203"></a> 
<a id="pgfId-1036647"></a>grad(y, x) == (grad(y, x3) * grad(x3, x2) *
<a id="pgfId-1022198"></a>               grad(x2, x1) * grad(x1, x))</pre>

  <p class="body"><a id="pgfId-1022222"></a>Applying the chain rule to the computation of the gradient values of a neural network gives rise to an algorithm <a id="marker-1022211"></a>called <i class="fm-italics">backpropagation</i>. Let’s see how that works, concretely. <a id="marker-1022227"></a><a id="marker-1022230"></a><a id="marker-1022232"></a></p>

  <p class="fm-head2"><a id="pgfId-1022238"></a>Automatic differentiation with computation graphs</p>

  <p class="body"><a id="pgfId-1022263"></a><a id="marker-1022249"></a><a id="marker-1022251"></a><a id="marker-1022253"></a>A useful way to think about backpropagation is in terms of <i class="fm-italics">computation graphs</i>. A computation graph is the data structure at the heart of TensorFlow and the deep learning revolution in general. It’s a directed acyclic graph of operations—in our case, tensor operations. For instance, figure 2.21 shows the graph representation of our first model.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/02-21.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1042683"></a>Figure 2.21 The computation graph representation of our two-layer model</p>

  <p class="body"><a id="pgfId-1022282"></a>Computation graphs have been an extremely successful abstraction in computer science because they enable us to <i class="fm-italics">treat computation as data</i>: a computable expression is encoded as a machine-readable data structure that can be used as the input or output of another program. For instance, you could imagine a program that receives a computation graph and returns a new computation graph that implements a large-scale distributed version of the same computation—this would mean that you could distribute any computation without having to write the distribution logic yourself. Or imagine a program that receives a computation graph and can automatically generate the derivative of the expression it represents. It’s much easier to do these things if your computation is expressed as an explicit graph data structure rather than, say, lines of ASCII characters in a .py file.</p>

  <p class="body"><a id="pgfId-1022407"></a>To explain backpropagation clearly, let’s look at a really basic example of a computation graph (see figure 2.22). We’ll consider a simplified version of figure 2.21, where we only have one linear layer and where all variables are scalar. We’ll take two scalar variables <code class="fm-code-in-text">w</code> and <code class="fm-code-in-text">b</code>, a scalar input <code class="fm-code-in-text">x</code>, and apply some operations to them to combine them into an output <code class="fm-code-in-text">y</code>. Finally, we’ll apply an absolute value error-loss function: <code class="fm-code-in-text">loss_val</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">abs(y_true</code> <code class="fm-code-in-text">-</code> <code class="fm-code-in-text">y)</code>. Since we want to update <code class="fm-code-in-text">w</code> and <code class="fm-code-in-text">b</code> in a way that will minimize <code class="fm-code-in-text">loss_val</code>, we are interested in computing <code class="fm-code-in-text">grad(loss_val,</code> <code class="fm-code-in-text">b)</code> and <code class="fm-code-in-text">grad(loss _val,</code> <code class="fm-code-in-text">w)</code>.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/02-22.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1042735"></a>Figure 2.22 A basic example of a computation graph</p>

  <p class="body"><a id="pgfId-1022502"></a>Let’s set concrete values for the “input nodes” in the graph, that is to say, the input <code class="fm-code-in-text">x</code>, the target <code class="fm-code-in-text">y_true</code>, <code class="fm-code-in-text">w</code>, and <code class="fm-code-in-text">b</code>. We’ll propagate these values to all nodes in the graph, from top to bottom, until we reach <code class="fm-code-in-text">loss_val</code>. This is <a id="marker-1042777"></a>the <i class="fm-italics">forward pass</i> (see figure 2.23).</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/02-23.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1042806"></a>Figure 2.23 Running a forward pass</p>

  <p class="body"><a id="pgfId-1022617"></a>Now let’s “reverse” the graph: for each edge in the graph going from <code class="fm-code-in-text">A</code> to <code class="fm-code-in-text">B</code>, we will create an opposite edge from <code class="fm-code-in-text">B</code> to <code class="fm-code-in-text">A</code>, and ask, how much does <code class="fm-code-in-text">B</code> vary when <code class="fm-code-in-text">A</code> varies? That is to say, what is <code class="fm-code-in-text">grad(B,</code> <code class="fm-code-in-text">A)</code>? We’ll annotate each inverted edge with this value. This backward graph represents <a id="marker-1042785"></a>the <i class="fm-italics">backward pass</i> (see figure 2.24).</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/02-24.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1042848"></a>Figure 2.24 Running a backward pass</p>

  <p class="body"><a id="pgfId-1022636"></a>We have the following:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1022688"></a><code class="fm-code-in-text">grad(loss_val,</code> <code class="fm-code-in-text">x2)</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">1</code>, because as <code class="fm-code-in-text">x2</code> varies by an amount epsilon, <code class="fm-code-in-text">loss_val</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">abs(4</code> <code class="fm-code-in-text">-</code> <code class="fm-code-in-text">x2)</code> varies by the same amount.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1022725"></a><code class="fm-code-in-text">grad(x2,</code> <code class="fm-code-in-text">x1)</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">1</code>, because as <code class="fm-code-in-text">x1</code> varies by an amount epsilon, <code class="fm-code-in-text">x2</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">x1</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">b</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">x1</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">1</code> varies by the same amount.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1022762"></a><code class="fm-code-in-text">grad(x2,</code> <code class="fm-code-in-text">b)</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">1</code>, because as <code class="fm-code-in-text">b</code> varies by an amount epsilon, <code class="fm-code-in-text">x2</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">x1</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">b</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">6</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">b</code> varies by the same amount.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1022809"></a><code class="fm-code-in-text">grad(x1,</code> <code class="fm-code-in-text">w)</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">2</code>, because as <code class="fm-code-in-text">w</code> varies by an amount epsilon, <code class="fm-code-in-text">x1</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">x</code> <code class="fm-code-in-text">*</code> <code class="fm-code-in-text">w</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">2</code> <code class="fm-code-in-text">*</code> <code class="fm-code-in-text">w</code> varies by <code class="fm-code-in-text">2</code> <code class="fm-code-in-text">*</code> <code class="fm-code-in-text">epsilon</code>.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1022834"></a>What the chain rule says about this backward graph is that you can obtain the derivative of a node with respect to another node by <i class="fm-italics">multiplying the derivatives for each edge along the path linking the two nodes</i>. For instance, <code class="fm-code-in-text">grad(loss_val,</code> <code class="fm-code-in-text">w)</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">grad(loss_val,</code> <code class="fm-code-in-text">x2)</code> <code class="fm-code-in-text">*</code> <code class="fm-code-in-text">grad(x2,</code> <code class="fm-code-in-text">x1)</code> <code class="fm-code-in-text">*</code> <code class="fm-code-in-text">grad(x1,</code> <code class="fm-code-in-text">w)</code> (see figure 2.25).</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/02-25.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1042890"></a>Figure 2.25 Path from <code class="fm-code-in-text">loss_val</code> to <code class="fm-code-in-text">w</code> in the backward graph</p>

  <p class="body"><a id="pgfId-1022872"></a>By applying the chain rule to our graph, we obtain what we were looking for:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1022892"></a><code class="fm-code-in-text">grad(loss_val,</code> <code class="fm-code-in-text">w)</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">1</code> <code class="fm-code-in-text">*</code> <code class="fm-code-in-text">1</code> <code class="fm-code-in-text">*</code> <code class="fm-code-in-text">2</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">2</code></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1022910"></a><code class="fm-code-in-text">grad(loss_val,</code> <code class="fm-code-in-text">b)</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">1</code> <code class="fm-code-in-text">*</code> <code class="fm-code-in-text">1</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">1</code></p>
    </li>
  </ul>

  <p class="fm-callout"><a id="pgfId-1022924"></a><span class="fm-callout-head">Note</span> If there are multiple paths linking the two nodes of interest, <code class="fm-code-in-text1">a</code> and <code class="fm-code-in-text1">b</code>, in the backward graph, we would obtain <code class="fm-code-in-text1">grad(b,</code> <code class="fm-code-in-text1">a)</code> by summing the contributions of all the paths.</p>

  <p class="body"><a id="pgfId-1022969"></a>And with that, you just saw backpropagation in action! Backpropagation is simply the application of the chain rule to a computation graph. There’s nothing more to it. Backpropagation starts with the final loss value and works backward from the top layers to the bottom layers, computing the contribution that each parameter had in the loss value. That’s where the name “backpropagation” comes from: we “back propagate” the loss contributions of different nodes in a computation graph.</p>

  <p class="body"><a id="pgfId-1022988"></a>Nowadays people implement neural networks in modern frameworks that are capable <a id="marker-1022977"></a>of <i class="fm-italics">automatic differentiation</i>, such as TensorFlow. Automatic differentiation is implemented with the kind of computation graph you’ve just seen. Automatic differentiation makes it possible to retrieve the gradients of arbitrary compositions of differentiable tensor operations without doing any extra work besides writing down the forward pass. When I wrote my first neural networks in C in the 2000s, I had to write my gradients by hand. Now, thanks to modern automatic differentiation tools, you’ll never have to implement backpropagation yourself. Consider yourself lucky!<a id="marker-1022993"></a><a id="marker-1022996"></a><a id="marker-1022998"></a></p>

  <p class="fm-head2"><a id="pgfId-1023004"></a>The gradient tape in TensorFlow</p>

  <p class="body"><a id="pgfId-1023067"></a><a id="marker-1034646"></a><a id="marker-1034647"></a><a id="marker-1034648"></a><a id="marker-1034649"></a>The API through which you can leverage TensorFlow’s powerful automatic differentiation capabilities is the <code class="fm-code-in-text">GradientTape</code>. It’s a Python scope that will “record” the tensor operations that run inside it, in the form of a computation graph (sometimes called a “tape”). This graph can then be used to retrieve the gradient of any output with respect to any variable or set of variables (instances of the <code class="fm-code-in-text">tf.Variable</code> class). A <code class="fm-code-in-text">tf.Variable</code> is a <a id="marker-1034651"></a>specific kind of tensor meant to hold mutable state—for instance, the weights of a neural network are always <code class="fm-code-in-text">tf.Variable</code> instances.</p>
  <pre class="programlisting"><a id="pgfId-1034674"></a><b class="fm-codebrown">import</b> tensorflow <b class="fm-codebrown">as</b> tf
<a id="pgfId-1023090"></a>x = tf.Variable(<span class="fm-codeblue">0.</span>)                      <span class="fm-combinumeral">❶</span>
<a id="pgfId-1023102"></a><b class="fm-codebrown">with</b> tf.GradientTape() <b class="fm-codebrown">as</b> tape:          <span class="fm-combinumeral">❷</span>
<a id="pgfId-1023114"></a>    y = <span class="fm-codeblue">2</span> * x + <span class="fm-codeblue">3</span>                        <span class="fm-combinumeral">❸</span>
<a id="pgfId-1023126"></a>grad_of_y_wrt_x = tape.gradient(y, x)    <span class="fm-combinumeral">❹</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1038115"></a><span class="fm-combinumeral">❶</span> Instantiate a scalar Variable with an initial value of 0.</p>

  <p class="fm-code-annotation"><a id="pgfId-1038132"></a><span class="fm-combinumeral">❷</span> Open a GradientTape scope.</p>

  <p class="fm-code-annotation"><a id="pgfId-1038149"></a><span class="fm-combinumeral">❸</span> Inside the scope, apply some tensor operations to our variable.</p>

  <p class="fm-code-annotation"><a id="pgfId-1038166"></a><span class="fm-combinumeral">❹</span> Use the tape to retrieve the gradient of the output y with respect to our variable x.</p>

  <p class="body"><a id="pgfId-1023206"></a>The <code class="fm-code-in-text">GradientTape</code> works with tensor operations:</p>
  <pre class="programlisting"><a id="pgfId-1023221"></a>x = tf.Variable(tf.random.uniform((<span class="fm-codeblue">2</span>, <span class="fm-codeblue">2</span>)))     <span class="fm-combinumeral">❶</span>
<a id="pgfId-1034744"></a><b class="fm-codebrown">with</b> tf.GradientTape() <b class="fm-codebrown">as</b> tape:
<a id="pgfId-1034745"></a>    y = <span class="fm-codeblue">2</span> * x + <span class="fm-codeblue">3</span> 
<a id="pgfId-1023253"></a>grad_of_y_wrt_x = tape.gradient(y, x)          <span class="fm-combinumeral">❷</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1038011"></a><span class="fm-combinumeral">❶</span> Instantiate a Variable with shape (2, 2) and an initial value of all zeros.</p>

  <p class="fm-code-annotation"><a id="pgfId-1038028"></a><span class="fm-combinumeral">❷</span> grad_of_y_wrt_x is a tensor of shape (2, 2) (like x) describing the curvature of y = 2 * a + 3 around x = [[0, 0], [0, 0]].</p>

  <p class="body"><a id="pgfId-1023301"></a>It also works with lists of variables:</p>
  <pre class="programlisting"><a id="pgfId-1034760"></a>W = tf.Variable(tf.random.uniform((<span class="fm-codeblue">2</span>, <span class="fm-codeblue">2</span>)))
<a id="pgfId-1034761"></a>b = tf.Variable(tf.zeros((<span class="fm-codeblue">2</span>,)))
<a id="pgfId-1034762"></a>x = tf.random.uniform((<span class="fm-codeblue">2</span>, <span class="fm-codeblue">2</span>)) 
<a id="pgfId-1034763"></a><b class="fm-codebrown">with</b> tf.GradientTape() <b class="fm-codebrown">as</b> tape:
<a id="pgfId-1023339"></a>    y = tf.matmul(x, W) + b                         <span class="fm-combinumeral">❶</span>
<a id="pgfId-1023351"></a>grad_of_y_wrt_W_and_b = tape.gradient(y, [W, b])    <span class="fm-combinumeral">❷</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1037898"></a><span class="fm-combinumeral">❶</span> matmul is how you say “dot product” in TensorFlow.</p>

  <p class="fm-code-annotation"><a id="pgfId-1037919"></a><span class="fm-combinumeral">❷</span> grad_of_y_wrt_W_and_b is a list of two tensors with the same shapes as W and b, respectively.</p>

  <p class="body"><a id="pgfId-1023399"></a>You will learn about the gradient tape in the next chapter. <a id="marker-1023401"></a><a id="marker-1023404"></a><a id="marker-1023406"></a><a id="marker-1023408"></a><a id="marker-1023410"></a><a id="marker-1023412"></a><a id="marker-1023414"></a><a id="marker-1023416"></a><a id="marker-1023418"></a></p>

  <h2 class="fm-head" id="heading_id_29"><a id="pgfId-1023424"></a>2.5 Looking back at our first example</h2>

  <p class="body"><a id="pgfId-1023434"></a><a id="marker-1023435"></a>You’re nearing the end of this chapter, and you should now have a general understanding of what’s going on behind the scenes in a neural network. What was a magical black box at the start of the chapter has turned into a clearer picture, as illustrated in figure 2.26: the model, composed of layers that are chained together, maps the input data to predictions. The loss function then compares these predictions to the targets, producing a loss value: a measure of how well the model’s predictions match what was expected. The optimizer uses this loss value to update the model’s weights.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/02-26.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1042935"></a>Figure 2.26 Relationship between the network, layers, loss function, and optimizer</p>

  <p class="body"><a id="pgfId-1023453"></a>Let’s go back to the first example in this chapter and review each piece of it in the light of what you’ve learned since.</p>

  <p class="body"><a id="pgfId-1023473"></a>This was the input data:</p>
  <pre class="programlisting"><a id="pgfId-1034794"></a>(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
<a id="pgfId-1034795"></a>train_images = train_images.reshape((<span class="fm-codeblue">60000</span>, <span class="fm-codeblue">28</span> * <span class="fm-codeblue">28</span>))
<a id="pgfId-1034796"></a>train_images = train_images.astype(<span class="fm-codegreen">"float32"</span>) / <span class="fm-codeblue">255</span> 
<a id="pgfId-1034797"></a>test_images = test_images.reshape((<span class="fm-codeblue">10000</span>, <span class="fm-codeblue">28</span> * <span class="fm-codeblue">28</span>))
<a id="pgfId-1023511"></a>test_images = test_images.astype(<span class="fm-codegreen">"float32"</span>) / <span class="fm-codeblue">255</span></pre>

  <p class="body"><a id="pgfId-1023543"></a>Now you understand that the input images are stored in NumPy tensors, which are here formatted as <code class="fm-code-in-text">float32</code> tensors of shape <code class="fm-code-in-text">(60000,</code> <code class="fm-code-in-text">784)</code> (training data) and <code class="fm-code-in-text">(10000,</code> <code class="fm-code-in-text">784)</code> (test data) respectively.</p>

  <p class="body"><a id="pgfId-1023552"></a>This was our model:</p>
  <pre class="programlisting"><a id="pgfId-1034837"></a>model = keras.Sequential([
<a id="pgfId-1034838"></a>    layers.Dense(<span class="fm-codeblue">512</span>, activation=<span class="fm-codegreen">"relu"</span>),
<a id="pgfId-1034839"></a>    layers.Dense(<span class="fm-codeblue">10</span>, activation=<span class="fm-codegreen">"softmax"</span>)
<a id="pgfId-1023584"></a>])</pre>

  <p class="body"><a id="pgfId-1023606"></a>Now you understand that this model consists of a chain of two <code class="fm-code-in-text">Dense</code> layers, that each layer applies a few simple tensor operations to the input data, and that these operations involve weight tensors. Weight tensors, which are attributes of the layers, are where the <i class="fm-italics">knowledge</i> of the model persists.</p>

  <p class="body"><a id="pgfId-1023615"></a>This was the model-compilation step:</p>
  <pre class="programlisting"><a id="pgfId-1034854"></a>model.compile(optimizer=<span class="fm-codegreen">"rmsprop"</span>,
<a id="pgfId-1034855"></a>              loss=<span class="fm-codegreen">"sparse_categorical_crossentropy"</span>,
<a id="pgfId-1023641"></a>              metrics=[<span class="fm-codegreen">"accuracy"</span>])</pre>

  <p class="body"><a id="pgfId-1023663"></a>Now you understand that <code class="fm-code-in-text">sparse_categorical_crossentropy</code> is the loss function that’s used as a feedback signal for learning the weight tensors, and which the training phase will attempt to minimize. You also know that this reduction of the loss happens via mini-batch stochastic gradient descent. The exact rules governing a specific use of gradient descent are defined by the <code class="fm-code-in-text">rmsprop</code> optimizer passed <a id="marker-1034831"></a>as the first argument.</p>

  <p class="body"><a id="pgfId-1023678"></a>Finally, this was the training loop:</p>
  <pre class="programlisting"><a id="pgfId-1023684"></a>model.fit(train_images, train_labels, epochs=<span class="fm-codeblue">5</span>, batch_size=<span class="fm-codeblue">128</span>)</pre>

  <p class="body"><a id="pgfId-1023720"></a>Now you understand what happens when you call <code class="fm-code-in-text">fit</code>: the model <a id="marker-1023709"></a>will start to iterate on the training data in mini-batches of 128 samples, 5 times over (each iteration over all the training data is called an <i class="fm-italics">epoch</i>). For each batch, the model will compute the gradient of the loss with regard to the weights (using the Backpropagation algorithm, which derives from the chain rule in calculus) and move the weights in the direction that will reduce the value of the loss for this batch.</p>

  <p class="body"><a id="pgfId-1023729"></a>After these 5 epochs, the model will have performed 2,345 gradient updates (469 per epoch), and the loss of the model will be sufficiently low that the model will be capable of classifying handwritten digits with high accuracy.</p>

  <p class="body"><a id="pgfId-1023735"></a>At this point, you already know most of what there is to know about neural networks. Let’s prove it by reimplementing a simplified version of that first example “from scratch” in TensorFlow, step by step.</p>

  <h3 class="fm-head1" id="heading_id_30"><a id="pgfId-1023741"></a>2.5.1 Reimplementing our first example from scratch in TensorFlow</h3>

  <p class="body"><a id="pgfId-1023751"></a><a id="marker-1023752"></a>What better demonstrates full, unambiguous understanding than implementing everything from scratch? Of course, what “from scratch” means here is relative: we won’t reimplement basic tensor operations, and we won’t implement backpropagation. But we’ll go to such a low level that we will barely use any Keras functionality at all.</p>

  <p class="body"><a id="pgfId-1023760"></a>Don’t worry if you don’t understand every little detail in this example just yet. The next chapter will dive in more detail into the TensorFlow API. For now, just try to follow the gist of what’s going on—the intent of this example is to help crystalize your understanding of the mathematics of deep learning using a concrete implementation. Let’s go!</p>

  <p class="fm-head2"><a id="pgfId-1023766"></a>A simple Dense class</p>

  <p class="body"><a id="pgfId-1023842"></a><a id="marker-1023777"></a>You’ve learned earlier that the <code class="fm-code-in-text">Dense</code> layer implements the following input transformation, where <code class="fm-code-in-text">W</code> and <code class="fm-code-in-text">b</code> are model parameters, and <code class="fm-code-in-text">activation</code> is an <a id="marker-1023821"></a>element-wise function (usually <code class="fm-code-in-text">relu</code>, but it would be <code class="fm-code-in-text">softmax</code> for the last layer):</p>
  <pre class="programlisting"><a id="pgfId-1023851"></a>output = activation(dot(W, input) + b)</pre>

  <p class="body"><a id="pgfId-1023907"></a>Let’s implement a simple Python class, <code class="fm-code-in-text">NaiveDense</code>, that creates <a id="marker-1023876"></a>two TensorFlow variables, <code class="fm-code-in-text">W</code> and <code class="fm-code-in-text">b</code>, and exposes a <code class="fm-code-in-text">__call__()</code> method that <a id="marker-1023912"></a>applies the preceding transformation.</p>
  <pre class="programlisting"><a id="pgfId-1034914"></a><b class="fm-codebrown">import</b> tensorflow <b class="fm-codebrown">as</b> tf
<a id="pgfId-1034915"></a>  
<a id="pgfId-1034916"></a><b class="fm-codebrown">class</b> NaiveDense:
<a id="pgfId-1034917"></a>    <b class="fm-codebrown">def</b> __init__(self, input_size, output_size, activation):
<a id="pgfId-1034918"></a>        self.activation = activation
<a id="pgfId-1034919"></a> 
<a id="pgfId-1023959"></a>        w_shape = (input_size, output_size)                                <span class="fm-combinumeral">❶</span>
<a id="pgfId-1034934"></a>        w_initial_value = tf.random.uniform(w_shape, minval=<span class="fm-codeblue">0</span>, maxval=<span class="fm-codeblue">1e-1</span>)
<a id="pgfId-1034935"></a>        self.W = tf.Variable(w_initial_value)
<a id="pgfId-1034936"></a>  
<a id="pgfId-1023988"></a>        b_shape = (output_size,                                            <span class="fm-combinumeral">❷</span>
<a id="pgfId-1034953"></a>        b_initial_value = tf.zeros(b_shape)
<a id="pgfId-1034954"></a>        self.b = tf.Variable(b_initial_value)
<a id="pgfId-1034955"></a>  
<a id="pgfId-1024017"></a>    <b class="fm-codebrown">def</b> __call__(self, inputs)::                                           <span class="fm-combinumeral">❸</span>
<a id="pgfId-1035043"></a>        <b class="fm-codebrown">return</b> self.activation(tf.matmul(inputs, self.W) + self.b)
<a id="pgfId-1035044"></a>  
<a id="pgfId-1035045"></a>    <code class="fm-codeblue">@property</code>
<a id="pgfId-1024051"></a>    <b class="fm-codebrown">def</b> weights(self):                                                     <span class="fm-combinumeral">❹</span>
<a id="pgfId-1024063"></a>        <b class="fm-codebrown">return</b> [self.W, self.b]</pre>

  <p class="fm-code-annotation"><a id="pgfId-1037585"></a><span class="fm-combinumeral">❶</span> Create a matrix, W, of shape (input_size, output_size), initialized with random values.</p>

  <p class="fm-code-annotation"><a id="pgfId-1037606"></a><span class="fm-combinumeral">❷</span> Create a vector, b, of shape (output_size,), initialized with zeros.</p>

  <p class="fm-code-annotation"><a id="pgfId-1037623"></a><span class="fm-combinumeral">❸</span> Apply the forward pass.</p>

  <p class="fm-code-annotation"><a id="pgfId-1037640"></a><span class="fm-combinumeral">❹</span> Convenience method for retrieving the layer’s weights <a id="marker-1037645"></a></p>

  <p class="fm-head2"><a id="pgfId-1024140"></a>A simple Sequential class</p>

  <p class="body"><a id="pgfId-1024186"></a><a id="marker-1024151"></a>Now, let’s create a <code class="fm-code-in-text">NaiveSequential</code> class to <a id="marker-1024165"></a>chain these layers. It wraps a list of layers and exposes a <code class="fm-code-in-text">__call__()</code> method that simply calls the underlying layers on the inputs, in order. It also features a <code class="fm-code-in-text">weights</code> property to easily keep track of the layers’ parameters.</p>
  <pre class="programlisting"><a id="pgfId-1035072"></a><b class="fm-codebrown">class</b> NaiveSequential:
<a id="pgfId-1035073"></a>    <b class="fm-codebrown">def</b> __init__(self, layers):
<a id="pgfId-1035074"></a>        self.layers = layers
<a id="pgfId-1035075"></a>  
<a id="pgfId-1035076"></a>    <b class="fm-codebrown">def</b> __call__(self, inputs):
<a id="pgfId-1035077"></a>        x = inputs
<a id="pgfId-1035078"></a>        <b class="fm-codebrown">for</b> layer <b class="fm-codebrown">in</b> self.layers:
<a id="pgfId-1035079"></a>           x = layer(x)
<a id="pgfId-1035080"></a>        <b class="fm-codebrown">return</b> x
<a id="pgfId-1035081"></a>  
<a id="pgfId-1035082"></a>    <code class="fm-codeblue">@property</code> 
<a id="pgfId-1035083"></a>    <b class="fm-codebrown">def</b> weights(self):
<a id="pgfId-1035084"></a>       weights = []
<a id="pgfId-1035085"></a>       <b class="fm-codebrown">for</b> layer <b class="fm-codebrown">in</b> self.layers:
<a id="pgfId-1035086"></a>           weights += layer.weights
<a id="pgfId-1024291"></a>       <b class="fm-codebrown">return</b> weights</pre>

  <p class="body"><a id="pgfId-1024313"></a>Using this <code class="fm-code-in-text">NaiveDense</code> class and this <code class="fm-code-in-text">NaiveSequential</code> class, we can create a mock Keras model:<a id="marker-1024318"></a></p>
  <pre class="programlisting"><a id="pgfId-1035108"></a>model = NaiveSequential([
<a id="pgfId-1035109"></a>    NaiveDense(input_size=<span class="fm-codeblue">28</span> * <span class="fm-codeblue">28</span>, output_size=<span class="fm-codeblue">512</span>, activation=tf.nn.relu),
<a id="pgfId-1035110"></a>    NaiveDense(input_size=<span class="fm-codeblue">512</span>, output_size=<span class="fm-codeblue">10</span>, activation=tf.nn.softmax)
<a id="pgfId-1035111"></a>]) 
<a id="pgfId-1024357"></a><b class="fm-codebrown">assert</b> len(model.weights) == <span class="fm-codeblue">4</span> </pre>

  <p class="fm-head2"><a id="pgfId-1024363"></a>A batch generator</p>

  <p class="body"><a id="pgfId-1024373"></a>Next, we need a way to iterate over the MNIST data in mini-batches. This is easy:<a id="marker-1024381"></a></p>
  <pre class="programlisting"><a id="pgfId-1035128"></a><b class="fm-codebrown">import</b> math
<a id="pgfId-1035129"></a>  
<a id="pgfId-1035130"></a><b class="fm-codebrown">class</b> BatchGenerator:
<a id="pgfId-1035131"></a>    <b class="fm-codebrown">def</b> __init__(self, images, labels, batch_size=<span class="fm-codeblue">128</span>):
<a id="pgfId-1035132"></a>        <b class="fm-codebrown">assert</b> len(images) == len(labels)
<a id="pgfId-1035133"></a>        self.index = <span class="fm-codeblue">0</span>
<a id="pgfId-1035324"></a>        self.images = images
<a id="pgfId-1035134"></a>        self.labels = labels
<a id="pgfId-1035135"></a>        self.batch_size = batch_size
<a id="pgfId-1035136"></a>        self.num_batches = math.ceil(len(images) / batch_size)
<a id="pgfId-1035137"></a> 
<a id="pgfId-1035138"></a>    <b class="fm-codebrown">def</b> next(self):
<a id="pgfId-1035139"></a>        images = self.images[self.index : self.index + self.batch_size]
<a id="pgfId-1035140"></a>        labels = self.labels[self.index : self.index + self.batch_size]
<a id="pgfId-1035141"></a>        self.index += self.batch_size
<a id="pgfId-1024483"></a>        <b class="fm-codebrown">return</b> images, labels</pre>

  <h3 class="fm-head1" id="heading_id_31"><a id="pgfId-1024489"></a>2.5.2 Running one training step</h3>

  <p class="body"><a id="pgfId-1024499"></a><a id="marker-1024500"></a>The most difficult part of the process is the “training step”: updating the weights of the model after running it on one batch of data. We need to</p>

  <ol class="calibre14">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1024508"></a>Compute the predictions of the model for the images in the batch.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1024522"></a>Compute the loss value for these predictions, given the actual labels.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1024532"></a>Compute the gradient of the loss with regard to the model’s weights.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1024542"></a>Move the weights by a small amount in the direction opposite to the gradient.</p>
    </li>
  </ol>

  <p class="body"><a id="pgfId-1024552"></a>To compute the gradient, we will use the TensorFlow <code class="fm-code-in-text">GradientTape</code> object we <a id="marker-1024563"></a>introduced in section 2.4.4:</p>
  <pre class="programlisting"><a id="pgfId-1035161"></a><b class="fm-codebrown">def</b> one_training_step(model, images_batch, labels_batch):
<a id="pgfId-1024587"></a>    <b class="fm-codebrown">with</b> tf.GradientTape() <b class="fm-codebrown">as</b> tape:                                         <span class="fm-combinumeral">❶</span>
<a id="pgfId-1024599"></a>        predictions = model(images_batch)                                   <span class="fm-combinumeral">❶</span>
<a id="pgfId-1024611"></a>        per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy(<span class="fm-combinumeral">❶</span>
<a id="pgfId-1024623"></a>            labels_batch, predictions)                                      <span class="fm-combinumeral">❶</span>
<a id="pgfId-1024635"></a>        average_loss = tf.reduce_mean(per_sample_losses)                    <span class="fm-combinumeral">❶</span>
<a id="pgfId-1024647"></a>    gradients = tape.gradient(average_loss, model.weights)                  <span class="fm-combinumeral">❷</span>
<a id="pgfId-1024659"></a>    update_weights(gradients, model.weights)                                <span class="fm-combinumeral">❸</span>
<a id="pgfId-1024671"></a>    <b class="fm-codebrown">return</b> average_loss</pre>

  <p class="fm-code-annotation"><a id="pgfId-1037258"></a><span class="fm-combinumeral">❶</span> Run the “forward pass” (compute the model’s predictions under a GradientTape scope).</p>

  <p class="fm-code-annotation"><a id="pgfId-1037275"></a><span class="fm-combinumeral">❷</span> Compute the gradient of the loss with regard to the weights. The output gradients is a list where each entry corresponds to a weight from the model.weights list.</p>

  <p class="fm-code-annotation"><a id="pgfId-1037292"></a><span class="fm-combinumeral">❸</span> Update the weights using the gradients (we will define this function shortly).</p>

  <p class="body"><a id="pgfId-1024761"></a>As you already know, the purpose of the “weight update” step (represented by the preceding <code class="fm-code-in-text">update_weights</code> function) is to move the weights by “a bit” in a direction that will reduce the loss on this batch. The magnitude of the move is determined by the “learning rate,” typically a small quantity. The simplest way to implement this <code class="fm-code-in-text">update_weights</code> function is to subtract <code class="fm-code-in-text">gradient</code> <code class="fm-code-in-text">*</code> <code class="fm-code-in-text">learning_rate</code> from each weight:</p>
  <pre class="programlisting"><a id="pgfId-1035198"></a>learning_rate = <span class="fm-codeblue">1e-3</span> 
<a id="pgfId-1035199"></a>  
<a id="pgfId-1035200"></a><b class="fm-codebrown">def</b> update_weights(gradients, weights):
<a id="pgfId-1035201"></a>    <b class="fm-codebrown">for</b> g, w <b class="fm-codebrown">in</b> zip(gradients, weights):
<a id="pgfId-1024801"></a>        w.assign_sub(g * learning_rate)      <span class="fm-combinumeral">❶</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1037210"></a><span class="fm-combinumeral">❶</span> assign_sub is the equivalent of -= for TensorFlow variables.</p>

  <p class="body"><a id="pgfId-1024833"></a>In practice, you would almost never implement a weight update step like this by hand. Instead, you would use an <code class="fm-code-in-text">Optimizer</code> instance from Keras, like this:</p>
  <pre class="programlisting"><a id="pgfId-1035220"></a><b class="fm-codebrown">from</b> tensorflow.keras <b class="fm-codebrown">import</b> optimizers
<a id="pgfId-1035221"></a>  
<a id="pgfId-1035222"></a>optimizer = optimizers.SGD(learning_rate=<span class="fm-codeblue">1e-3</span>)
<a id="pgfId-1035223"></a>  
<a id="pgfId-1035224"></a><b class="fm-codebrown">def</b> update_weights(gradients, weights):
<a id="pgfId-1024884"></a>    optimizer.apply_gradients(zip(gradients, weights))</pre>

  <p class="body"><a id="pgfId-1024890"></a>Now that our per-batch training step is ready, we can move on to implementing an entire epoch of training. <a id="marker-1024892"></a></p>

  <h3 class="fm-head1" id="heading_id_32"><a id="pgfId-1024899"></a>2.5.3 The full training loop</h3>

  <p class="body"><a id="pgfId-1024909"></a><a id="marker-1024910"></a>An epoch of training simply consists of repeating the training step for each batch in the training data, and the full training loop is simply the repetition of one epoch:</p>
  <pre class="programlisting"><a id="pgfId-1035243"></a><b class="fm-codebrown">def</b> fit(model, images, labels, epochs, batch_size=<span class="fm-codeblue">128</span>):
<a id="pgfId-1035244"></a>    <b class="fm-codebrown">for</b> epoch_counter <b class="fm-codebrown">in</b> range(epochs):
<a id="pgfId-1035245"></a>        <b class="fm-codebrown">print</b>(f<span class="fm-codegreen">"Epoch {epoch_counter}"</span>)
<a id="pgfId-1035246"></a>        batch_generator = BatchGenerator(images, labels)
<a id="pgfId-1035247"></a>        <b class="fm-codebrown">for</b> batch_counter <b class="fm-codebrown">in</b> range(batch_generator.num_batches):
<a id="pgfId-1035248"></a>            images_batch, labels_batch = batch_generator.next()
<a id="pgfId-1035249"></a>            loss = one_training_step(model, images_batch, labels_batch)
<a id="pgfId-1035250"></a>            <b class="fm-codebrown">if</b> batch_counter % <span class="fm-codeblue">100</span> == <span class="fm-codeblue">0</span>:
<a id="pgfId-1024974"></a>                <b class="fm-codebrown">print</b>(f<span class="fm-codegreen">"loss at batch {batch_counter}: {loss:.2f}"</span>)</pre>

  <p class="body"><a id="pgfId-1024980"></a>Let’s test drive it:<a id="marker-1024982"></a></p>
  <pre class="programlisting"><a id="pgfId-1035267"></a><b class="fm-codebrown">from</b> tensorflow.keras.datasets <b class="fm-codebrown">import</b> mnist
<a id="pgfId-1035268"></a>(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
<a id="pgfId-1035269"></a>  
<a id="pgfId-1035270"></a>train_images = train_images.reshape((<span class="fm-codeblue">60000</span>, <span class="fm-codeblue">28</span> * <span class="fm-codeblue">28</span>))
<a id="pgfId-1035271"></a>train_images = train_images.astype(<span class="fm-codegreen">"float32"</span>) / <span class="fm-codeblue">255</span>  
<a id="pgfId-1035272"></a>test_images = test_images.reshape((<span class="fm-codeblue">10000</span>, <span class="fm-codeblue">28</span> * <span class="fm-codeblue">28</span>))
<a id="pgfId-1035273"></a>test_images = test_images.astype(<span class="fm-codegreen">"float32"</span>) / <span class="fm-codeblue">255</span> 
<a id="pgfId-1035274"></a>  
<a id="pgfId-1025038"></a>fit(model, train_images, train_labels, epochs=<span class="fm-codeblue">10</span>, batch_size=<span class="fm-codeblue">128</span>)</pre>

  <h3 class="fm-head1" id="heading_id_33"><a id="pgfId-1025049"></a>2.5.4 Evaluating the model</h3>

  <p class="body"><a id="pgfId-1025069"></a><a id="marker-1025060"></a>We can evaluate the model by taking the <code class="fm-code-in-text">argmax</code> of its predictions over the test images, and comparing it to the expected labels:</p>
  <pre class="programlisting"><a id="pgfId-1025078"></a>predictions = model(test_images)
<a id="pgfId-1025092"></a>predictions = predictions.numpy()                  <span class="fm-combinumeral">❶</span>
<a id="pgfId-1035289"></a>predicted_labels = np.argmax(predictions, axis=<span class="fm-codeblue">1</span>)
<a id="pgfId-1035290"></a>matches = predicted_labels == test_labels
<a id="pgfId-1025116"></a><b class="fm-codebrown">print</b>(f<span class="fm-codegreen">"accuracy: {matches.mean():.2f}"</span>)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1037144"></a><span class="fm-combinumeral">❶</span> Calling .numpy() on a TensorFlow tensor converts it to a NumPy tensor.</p>

  <p class="body"><a id="pgfId-1025142"></a>All done! As you can see, it’s quite a bit of work to do “by hand” what you can do in a few lines of Keras code. But because you’ve gone through these steps, you should now have a crystal clear understanding of what goes on inside a neural network when you call <code class="fm-code-in-text">fit()</code>. Having this low-level mental model of what your code is doing behind the scenes will make you better able to leverage the high-level features of the Keras API. <a id="marker-1025153"></a><a id="marker-1025156"></a></p>

  <h2 class="fm-head" id="heading_id_34"><a id="pgfId-1025162"></a>Summary</h2>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1025214"></a><i class="fm-italics1">Tensors</i> form the foundation of modern machine learning systems. They come in various flavors of <code class="fm-code-in-text">dtype</code>, <code class="fm-code-in-text">rank</code>, and <code class="fm-code-in-text">shape</code>.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1025223"></a>You can manipulate numerical tensors via <i class="fm-italics1">tensor operations</i> (such as addition, tensor product, or element-wise multiplication), which can be interpreted as encoding geometric transformations. In general, everything in deep learning is amenable to a geometric interpretation.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1025242"></a>Deep learning models consist of chains of simple tensor operations, parameterized by <i class="fm-italics1">weights</i>, which are themselves tensors. The weights of a model are where its “knowledge” is stored.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1025279"></a><i class="fm-italics1">Learning</i> means finding a set of values for the model’s weights that minimizes a <i class="fm-italics1">loss function</i> for a given set of training data samples and their corresponding targets.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1025288"></a>Learning happens by drawing random batches of data samples and their targets, and computing the gradient of the model parameters with respect to the loss on the batch. The model parameters are then moved a bit (the magnitude of the move is defined by the learning rate) in the opposite direction from the gradient. This is called <i class="fm-italics1">mini-batch stochastic gradient descent</i>.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1025307"></a>The entire learning process is made possible by the fact that all tensor operations in neural networks are differentiable, and thus it’s possible to apply the chain rule of derivation to find the gradient function mapping the current parameters and current batch of data to a gradient value. This is called <i class="fm-italics1">backpropagation</i>.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1025346"></a>Two key concepts you’ll see frequently in future chapters are <i class="fm-italics1">loss</i> and <i class="fm-italics1">optimizers</i>. These are the two things you need to define before you begin feeding data into a model.</p>

      <ul class="calibre16">
        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1025355"></a>The <i class="fm-italics1">loss</i> is the quantity you’ll attempt to minimize during training, so it should represent a measure of success for the task you’re trying to solve.</p>
        </li>

        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1025382"></a>The <i class="fm-italics1">optimizer</i> specifies the exact way in which the gradient of the loss will be used to update parameters: for instance, it could be the RMSProp optimizer, SGD with momentum, and so on.</p>
        </li>
      </ul>
    </li>
  </ul>
</body>
</html>
