<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>3</title>
    
  <link href="../../stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="../../page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
  <h1 class="tochead" id="heading_id_2"><a id="pgfId-998407"></a><a id="pgfId-1021155"></a>3 Introduction to Keras and TensorFlow</h1>

  <p class="co-summary-head"><a id="pgfId-1011754"></a>This chapter covers</p>

  <ul class="calibre10">
    <li class="co-summary-bullet"><a class="calibre11" id="pgfId-1011760"></a>A closer look at TensorFlow, Keras, and their relationship</li>

    <li class="co-summary-bullet"><a class="calibre11" id="pgfId-1011774"></a>Setting up a deep learning workspace</li>

    <li class="co-summary-bullet"><a class="calibre11" id="pgfId-1011784"></a>An overview of how core deep learning concepts translate to Keras and TensorFlow</li>
  </ul>

  <p class="body"><a id="pgfId-1011794"></a>This chapter is meant to give you everything you need to start doing deep learning in practice. I’ll give you a quick presentation of Keras (<span class="fm-hyperlink"><a class="url" href="https://keras.io">https://keras.io</a></span>) and TensorFlow (<span class="fm-hyperlink"><a class="url" href="https://www.tensorflow.org/">https://tensorflow.org</a></span>), the Python-based deep learning tools that we’ll use throughout the book. You’ll find out how to set up a deep learning workspace, with TensorFlow, Keras, and GPU support. Finally, building on top of the first contact you had with Keras and TensorFlow in chapter 2, we’ll review the core components of neural networks and how they translate to the Keras and TensorFlow APIs.</p>

  <p class="body"><a id="pgfId-1011802"></a>By the end of this chapter, you’ll be ready to move on to practical, real-world applications, which will start with chapter 4.</p>

  <h2 class="fm-head" id="heading_id_3"><a id="pgfId-1011808"></a>3.1 What’s TensorFlow?</h2>

  <p class="body"><a id="pgfId-1011818"></a><a id="marker-1011819"></a>TensorFlow is a Python-based, free, open source machine learning platform, developed primarily by Google. Much like NumPy, the primary purpose of TensorFlow is to enable engineers and researchers to manipulate mathematical expressions over numerical tensors. But TensorFlow goes far beyond the scope of NumPy in the following ways:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1011827"></a>It can automatically compute the gradient of any differentiable expression (as you saw in chapter 2), making it highly suitable for machine learning.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1011841"></a>It can run not only on CPUs, but also on GPUs and TPUs, highly parallel hardware accelerators.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1011851"></a>Computation defined in TensorFlow can be easily distributed across many machines.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1011861"></a>TensorFlow programs can be exported to other runtimes, such as C++, JavaScript (for browser-based applications), or TensorFlow Lite (for applications running on mobile devices or embedded devices), etc. This makes TensorFlow applications easy to deploy in practical settings.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1011871"></a>It’s important to keep in mind that TensorFlow is much more than a single library. It’s really a platform, home to a vast ecosystem of components, some developed by Google and some developed by third parties. For instance, there’s TF-Agents for reinforcement-learning research, TFX for industry-strength machine learning workflow management, TensorFlow Serving for production deployment, and there’s the TensorFlow Hub repository of pretrained models. Together, these components cover a very wide range of use cases, from cutting-edge research to large-scale production applications.</p>

  <p class="body"><a id="pgfId-1011877"></a>TensorFlow scales fairly well: for instance, scientists from Oak Ridge National Lab have used it to train a 1.1 exaFLOPS extreme weather forecasting model on the 27,000 GPUs of the IBM Summit supercomputer. Likewise, Google has used TensorFlow to develop very compute-intensive deep learning applications, such as the chess-playing and Go-playing agent AlphaZero. For your own models, if you have the budget, you can realistically hope to scale to around 10 petaFLOPS on a small TPU pod or a large cluster of GPUs rented on Google Cloud or AWS. That would still be around 1% of the peak compute power of the top supercomputer in 2019!<a id="marker-1011879"></a></p>

  <h2 class="fm-head" id="heading_id_4"><a id="pgfId-1011886"></a>3.2 What’s Keras?</h2>

  <p class="body"><a id="pgfId-1011896"></a><a id="marker-1011897"></a>Keras is a deep learning API for Python, built on top of TensorFlow, that provides a convenient way to define and train any kind of deep learning model. Keras was initially developed for research, with the aim of enabling fast deep learning experimentation.</p>

  <p class="body"><a id="pgfId-1011905"></a>Through TensorFlow, Keras can run on top of different types of hardware (see figure 3.1)—GPU, TPU, or plain CPU—and can be seamlessly scaled to thousands of machines.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/03-01.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1033737"></a>Figure 3.1 Keras and TensorFlow: TensorFlow is a low-level tensor computing platform, and Keras is a high-level deep learning API</p>

  <p class="body"><a id="pgfId-1011921"></a>Keras is known for prioritizing the developer experience. It’s an API for human beings, not machines. It follows best practices for reducing cognitive load: it offers consistent and simple workflows, it minimizes the number of actions required for common use cases, and it provides clear and actionable feedback upon user error. This makes Keras easy to learn for a beginner, and highly productive to use for an expert.</p>

  <p class="body"><a id="pgfId-1011941"></a>Keras has well over a million users as of late 2021, ranging from academic researchers, engineers, and data scientists at both startups and large companies to graduate students and hobbyists. Keras is used at Google, Netflix, Uber, CERN, NASA, Yelp, Instacart, Square, and hundreds of startups working on a wide range of problems across every industry. Your YouTube recommendations originate from Keras models. The Waymo self-driving cars are developed with Keras models. Keras is also a popular framework on Kaggle, the machine learning competition website, where most deep learning competitions have been won using Keras.</p>

  <p class="body"><a id="pgfId-1011947"></a>Because Keras has a large and diverse user base, it doesn’t force you to follow a single “true” way of building and training models. Rather, it enables a wide range of different workflows, from the very high level to the very low level, corresponding to different user profiles. For instance, you have an array of ways to build models and an array of ways to train them, each representing a certain trade-off between usability and flexibility. In chapter 5, we’ll review in detail a good fraction of this spectrum of workflows. You could be using Keras like you would use Scikit-learn—just calling <code class="fm-code-in-text">fit()</code> and letting the framework do its thing—or you could be using it like NumPy—taking full control of every little detail.</p>

  <p class="body"><a id="pgfId-1011962"></a>This means that everything you’re learning now as you’re getting started will still be relevant once you’ve become an expert. You can get started easily and then gradually dive into workflows where you’re writing more and more logic from scratch. You won’t have to switch to an entirely different framework as you go from student to researcher, or from data scientist to deep learning engineer.</p>

  <p class="body"><a id="pgfId-1011968"></a>This philosophy is not unlike that of Python itself! Some languages only offer one way to write programs—for instance, object-oriented programming or functional programming. Meanwhile, Python is a multiparadigm language: it offers an array of possible usage patterns that all work nicely together. This makes Python suitable to a wide range of very different use cases: system administration, data science, machine learning engineering, web development . . . or just learning how to program. Likewise, you can think of Keras as the Python of deep learning: a user-friendly deep learning language that offers a variety of workflows to different user profiles. <a id="marker-1011970"></a></p>

  <h2 class="fm-head" id="heading_id_5"><a id="pgfId-1011977"></a>3.3 Keras and TensorFlow: A brief history</h2>

  <p class="body"><a id="pgfId-1011994"></a><a id="marker-1011988"></a><a id="marker-1011990"></a>Keras predates TensorFlow by eight months. It was released in March 2015, and TensorFlow was released in November 2015. You may ask, if Keras is built on top of TensorFlow, how it could exist before TensorFlow was released? Keras was originally built on top of Theano, another tensor-manipulation library that provided automatic differentiation and GPU support—the earliest of its kind. Theano, developed at the Montréal Institute for Learning Algorithms (MILA) at the Université de Montréal, was in many ways a precursor of TensorFlow. It pioneered the idea of using static computation graphs for automatic differentiation and for compiling code to both CPU and GPU.</p>

  <p class="body"><a id="pgfId-1011999"></a>In late 2015, after the release of TensorFlow, Keras was refactored to a multibackend architecture: it became possible to use Keras with either Theano or TensorFlow, and switching between the two was as easy as changing an environment variable. By September 2016, TensorFlow had reached a level of technical maturity where it became possible to make it the default backend option for Keras. In 2017, two new additional backend options were added to Keras: CNTK (developed by Microsoft) and MXNet (developed by Amazon). Nowadays, both Theano and CNTK are out of development, and MXNet is not widely used outside of Amazon. Keras is back to being a single-backend API—on top of TensorFlow.</p>

  <p class="body"><a id="pgfId-1012005"></a>Keras and TensorFlow have had a symbiotic relationship for many years. Throughout 2016 and 2017, Keras became well known as the user-friendly way to develop TensorFlow applications, funneling new users into the TensorFlow ecosystem. By late 2017, a majority of TensorFlow users were using it through Keras or in combination with Keras. In 2018, the TensorFlow leadership picked Keras as TensorFlow’s official high-level API. As a result, the Keras API is front and center in TensorFlow 2.0, released in September 2019—an extensive redesign of TensorFlow and Keras that takes into account over four years of user feedback and technical progress.</p>

  <p class="body"><a id="pgfId-1012011"></a>By this point, you must be eager to start running Keras and TensorFlow code in practice. Let’s get you started. <a id="marker-1028294"></a><a id="marker-1028295"></a></p>

  <h2 class="fm-head" id="heading_id_6"><a id="pgfId-1012022"></a>3.4 Setting up a deep learning workspace</h2>

  <p class="body"><a id="pgfId-1012041"></a><a id="marker-1028298"></a><a id="marker-1028299"></a>Before you can get started developing deep learning applications, you need to set up your development environment. It’s highly recommended, although not strictly necessary, that you run deep learning code on a modern NVIDIA GPU rather than your computer’s CPU. Some applications—in particular, image processing with convolutional networks—will be excruciatingly slow on CPU, even a fast multicore CPU. And even for applications that can realistically be run on CPU, you’ll generally see the speed increase by a factor of 5 or 10 by using a recent GPU.</p>

  <p class="body"><a id="pgfId-1012046"></a>To do deep learning on a GPU, you have three options:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012052"></a>Buy and install a physical NVIDIA GPU on your workstation.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012066"></a>Use GPU instances on Google Cloud or AWS EC2.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012076"></a>Use the free GPU runtime from Colaboratory, a hosted notebook service offered by Google (for details about what a “notebook” is, see the next section).</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1012086"></a>Colaboratory is the easiest way to get started, as it requires no hardware purchase and no software installation—just open a tab in your browser and start coding. It’s the option we recommend for running the code examples in this book. However, the free version of Colaboratory is only suitable for small workloads. If you want to scale up, you’ll have to use the first or second option.</p>

  <p class="body"><a id="pgfId-1012092"></a>If you don’t already have a GPU that you can use for deep learning (a recent, high-end NVIDIA GPU), then running deep learning experiments in the cloud is a simple, low-cost way for you to move to larger workloads without having to buy any additional hardware. If you’re developing using Jupyter notebooks, the experience of running in the cloud is no different from running locally.</p>

  <p class="body"><a id="pgfId-1012098"></a>But if you’re a heavy user of deep learning, this setup isn’t sustainable in the long term—or even for more than a few months. Cloud instances aren’t cheap: you’d pay $2.48 per hour for a V100 GPU on Google Cloud in mid-2021. Meanwhile, a solid consumer-class GPU will cost you somewhere between $1,500 and $2,500—a price that has been fairly stable over time, even as the specs of these GPUs keep improving. If you’re a heavy user of deep learning, consider setting up a local workstation with one or more GPUs.</p>

  <p class="body"><a id="pgfId-1012104"></a>Additionally, whether you’re running locally or in the cloud, it’s better to be using a Unix workstation. Although it’s technically possible to run Keras on Windows directly, we don’t recommend it. If you’re a Windows user and you want to do deep learning on your own workstation, the simplest solution to get everything running is to set up an Ubuntu dual boot on your machine, or to leverage Windows Subsystem for Linux (WSL), a compatibility layer that enables you to run Linux applications from Windows. It may seem like a hassle, but it will save you a lot of time and trouble in the long run.</p>

  <h3 class="fm-head1" id="heading_id_7"><a id="pgfId-1012110"></a>3.4.1 Jupyter notebooks: The preferred way to run deep learning experiments</h3>

  <p class="body"><a id="pgfId-1012137"></a><a id="marker-1012121"></a><a id="marker-1012125"></a><a id="marker-1012127"></a>Jupyter notebooks are a great way to run deep learning experiments—in particular, the many code examples in this book. They’re widely used in the data science and machine learning communities. A <i class="fm-italics">notebook</i> is a <a id="marker-1012142"></a>file generated by the Jupyter Notebook app (<span class="fm-hyperlink"><a class="url" href="https://jupyter.org">https://jupyter.org</a></span>) that you can edit in your browser. It mixes the ability to execute Python code with rich text-editing capabilities for annotating what you’re doing. A notebook also allows you to break up long experiments into smaller pieces that can be executed independently, which makes development interactive and means you don’t have to rerun all of your previous code if something goes wrong late in an experiment.</p>

  <p class="body"><a id="pgfId-1012153"></a>I recommend using Jupyter notebooks to get started with Keras, although that isn’t a requirement: you can also run standalone Python scripts or run code from within an IDE such as PyCharm. All the code examples in this book are available as open source notebooks; you can download them from GitHub at <span class="fm-hyperlink"><a class="url" href="https://github.com/fchollet/deep-learning-with-python-notebooks">github.com/fchollet/deep-learning-with-python-notebooks</a></span>. <a id="marker-1028377"></a><a id="marker-1028379"></a><a id="marker-1028380"></a></p>

  <h3 class="fm-head1" id="heading_id_8"><a id="pgfId-1012168"></a>3.4.2 Using Colaboratory</h3>

  <p class="body"><a id="pgfId-1012193"></a><a id="marker-1029279"></a><a id="marker-1029280"></a><a id="marker-1029281"></a><a id="marker-1029282"></a><a id="marker-1029283"></a>Colaboratory (or Colab for short) is a free Jupyter notebook service that requires no installation and runs entirely in the cloud. Effectively, it’s a web page that lets you write and execute Keras scripts right away. It gives you access to a free (but limited) GPU runtime and even a TPU runtime, so you don’t have to buy your own GPU. Colaboratory is what we recommend for running the code examples in this book.</p>

  <p class="fm-head2"><a id="pgfId-1012198"></a>First steps with Colaboratory</p>

  <p class="body"><a id="pgfId-1012208"></a><a id="marker-1012209"></a>To get started with Colab, go to <span class="fm-hyperlink"><a class="url" href="https://colab.research.google.com">https://colab.research.google.com</a></span> and click the New Notebook button. You’ll see the standard Notebook interface shown in figure 3.2.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/03-02.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1033782"></a>Figure 3.2 A Colab notebook</p>

  <p class="body"><a id="pgfId-1012228"></a>You’ll notice two buttons in the toolbar: + Code and + Text. They’re for creating executable Python code cells and annotation text cells, respectively. After entering code in a code cell, Pressing Shift-Enter will execute it (see figure 3.3).</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/03-03.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1033831"></a>Figure 3.3 Creating a code cell</p>

  <p class="body"><a id="pgfId-1012258"></a>In a text cell, you can use Markdown syntax (see figure 3.4). Pressing Shift-Enter on a text cell will render it.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/03-04.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1033876"></a>Figure 3.4 Creating a text cell</p>

  <p class="body"><a id="pgfId-1012288"></a>Text cells are useful for giving a readable structure to your notebooks: use them to annotate your code with section titles and long explanation paragraphs or to embed figures. Notebooks are meant to be a multimedia experience!<a id="marker-1012304"></a></p>

  <p class="fm-head2"><a id="pgfId-1012311"></a>Installing packages with pip</p>

  <p class="body"><a id="pgfId-1012344"></a><a id="marker-1012322"></a><a id="marker-1012324"></a>The default Colab environment already comes with TensorFlow and Keras installed, so you can start using it right away without any installation steps required. But if you ever need to install something with <code class="fm-code-in-text">pip</code>, you can do so by using the following syntax in a code cell (note that the line starts with <code class="fm-code-in-text">!</code> to indicate that it is a shell command rather than Python code):<a id="marker-1012349"></a><a id="marker-1012352"></a></p>
  <pre class="programlisting"><a id="pgfId-1012358"></a>!pip install package_name</pre>

  <p class="fm-head2"><a id="pgfId-1012372"></a>Using the GPU runtime</p>

  <p class="body"><a id="pgfId-1012389"></a><a id="marker-1012383"></a><a id="marker-1012385"></a>To use the GPU runtime with Colab, select Runtime &gt; Change Runtime Type in the menu and select GPU for the Hardware Accelerator (see figure 3.5).</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/03-05.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1033921"></a>Figure 3.5 Using the GPU runtime with Colab</p>

  <p class="body"><a id="pgfId-1012404"></a>TensorFlow and Keras will automatically execute on GPU if a GPU is available, so there’s nothing more you need to do after you’ve selected the GPU runtime.</p>

  <p class="body"><a id="pgfId-1012424"></a>You’ll notice that there’s also a TPU runtime option in that Hardware Accelerator dropdown menu. Unlike the GPU runtime, using the TPU runtime with TensorFlow and Keras does require a bit of manual setup in your code. We’ll cover this in chapter 13. For the time being, we recommend that you stick to the GPU runtime to follow along with the code examples in the book.</p>

  <p class="body"><a id="pgfId-1012430"></a>You now have a way to start running Keras code in practice. Next, let’s see how the key ideas you learned about in chapter 2 translate to Keras and TensorFlow code. <a id="marker-1012432"></a><a id="marker-1012435"></a><a id="marker-1012437"></a><a id="marker-1012441"></a><a id="marker-1012443"></a><a id="marker-1012447"></a><a id="marker-1012449"></a></p>

  <h2 class="fm-head" id="heading_id_9"><a id="pgfId-1012455"></a>3.5 First steps with TensorFlow</h2>

  <p class="body"><a id="pgfId-1012465"></a><a id="marker-1012466"></a>As you saw in the previous chapters, training a neural network revolves around the following concepts:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012474"></a>First, low-level tensor manipulation—the infrastructure that underlies all modern machine learning. This translates to TensorFlow APIs:</p>

      <ul class="calibre16">
        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1012514"></a><i class="fm-italics1">Tensors</i>, including special tensors that store the network’s state (<i class="fm-italics1">variables</i>)</p>
        </li>

        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1012523"></a><i class="fm-italics1">Tensor operations</i> such as addition, <code class="fm-code-in-text">relu</code>, <code class="fm-code-in-text">matmul</code></p>
        </li>

        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1012558"></a><i class="fm-italics1">Backpropagation</i>, a way to compute the gradient of mathematical expressions (handled in TensorFlow via the <code class="fm-code-in-text">GradientTape</code> object)</p>
        </li>
      </ul>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012567"></a>Second, high-level deep learning concepts. This translates to Keras APIs:</p>

      <ul class="calibre16">
        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1012603"></a><i class="fm-italics1">Layers</i>, which are combined into a <i class="fm-italics1">model</i></p>
        </li>

        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1012609"></a>A <i class="fm-italics1">loss function</i>, which defines the feedback signal used for learning</p>
        </li>

        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1012628"></a>An <i class="fm-italics1">optimizer</i>, which determines how learning proceeds</p>
        </li>

        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1012647"></a><i class="fm-italics1">Metrics</i> to evaluate model performance, such as accuracy</p>
        </li>

        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1012664"></a>A <i class="fm-italics1">training loop</i> that performs mini-batch stochastic gradient descent</p>
        </li>
      </ul>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1012776"></a>In the previous chapter, you already had a first light contact with some of the corresponding TensorFlow and Keras APIs: you’ve briefly <a id="marker-1012685"></a>used <a id="marker-1012691"></a>TensorFlow’s <code class="fm-code-in-text">Variable</code> class, the <code class="fm-code-in-text">matmul</code> operation, and the <code class="fm-code-in-text">GradientTape</code>. You’ve instantiated <a id="marker-1012727"></a>Keras <code class="fm-code-in-text">Dense</code> layers, packed them <a id="marker-1012743"></a>into a <code class="fm-code-in-text">Sequential</code> model, and trained <a id="marker-1012759"></a>that model with <a id="marker-1012765"></a>the <code class="fm-code-in-text">fit()</code> method.</p>

  <p class="body"><a id="pgfId-1012785"></a>Now let’s take a deeper dive into how all of these different concepts can be approached in practice using TensorFlow and Keras.</p>

  <h3 class="fm-head1" id="heading_id_10"><a id="pgfId-1012791"></a>3.5.1 Constant tensors and variables</h3>

  <p class="body"><a id="pgfId-1012808"></a><a id="marker-1012802"></a><a id="marker-1012804"></a>To do anything in TensorFlow, we’re going to need some tensors. Tensors need to be created with some initial value. For instance, you could create all-ones or all-zeros tensors (see listing 3.1), or tensors of values drawn from a random distribution (see listing 3.2).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1012864"></a>Listing 3.1 All-ones or all-zeros tensors</p>
  <pre class="programlisting"><a id="pgfId-1028773"></a>&gt;&gt;&gt; <b class="fm-codebrown">import</b> tensorflow <b class="fm-codebrown">as</b> tf
<a id="pgfId-1012903"></a>&gt;&gt;&gt; x = tf.ones(shape=(<span class="fm-codeblue">2</span>, <span class="fm-codeblue">1</span>))        <span class="fm-combinumeral">❶</span>
<a id="pgfId-1026607"></a>&gt;&gt;&gt; <b class="fm-codebrown">print</b>(x)
<a id="pgfId-1026608"></a>tf.Tensor(
<a id="pgfId-1026609"></a>[[1.]
<a id="pgfId-1026610"></a> [1.]], shape=(2, 1), dtype=float32)
<a id="pgfId-1012939"></a>&gt;&gt;&gt; x = tf.zeros(shape=(<span class="fm-codeblue">2</span>, <span class="fm-codeblue">1</span>))       <span class="fm-combinumeral">❷</span>
<a id="pgfId-1026629"></a>&gt;&gt;&gt; <b class="fm-codebrown">print</b>(x)
<a id="pgfId-1026630"></a>tf.Tensor(
<a id="pgfId-1026631"></a>[[0.]
<a id="pgfId-1012969"></a> [0.]], shape=(2, 1), dtype=float32)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1032833"></a><span class="fm-combinumeral">❶</span> Equivalent to np.ones(shape=(2, 1))</p>

  <p class="fm-code-annotation"><a id="pgfId-1032850"></a><span class="fm-combinumeral">❷</span> Equivalent to np.zeros(shape=(2, 1))</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1013062"></a>Listing 3.2 Random tensors</p>
  <pre class="programlisting"><a id="pgfId-1013011"></a>&gt;&gt;&gt; x = tf.random.normal(shape=(<span class="fm-codeblue">3</span>, <span class="fm-codeblue">1</span>), mean=<span class="fm-codeblue">0.</span>, stddev=<span class="fm-codeblue">1.</span>)      <span class="fm-combinumeral">❶</span>
<a id="pgfId-1026666"></a>&gt;&gt;&gt; <b class="fm-codebrown">print</b>(x)
<a id="pgfId-1026667"></a>tf.Tensor(
<a id="pgfId-1026668"></a>[[-0.14208166]
<a id="pgfId-1026669"></a> [-0.95319825]
<a id="pgfId-1026670"></a> [ 1.1096532 ]], shape=(3, 1), dtype=float32)
<a id="pgfId-1013137"></a>&gt;&gt;&gt; x = tf.random.uniform(shape=(<span class="fm-codeblue">3</span>, <span class="fm-codeblue">1</span>), minval=<span class="fm-codeblue">0.</span>, maxval=<span class="fm-codeblue">1.</span>)   <span class="fm-combinumeral">❷</span>
<a id="pgfId-1026683"></a>&gt;&gt;&gt; <b class="fm-codebrown">print</b>(x)
<a id="pgfId-1026684"></a>tf.Tensor(
<a id="pgfId-1026685"></a>[[0.33779848]
<a id="pgfId-1026686"></a> [0.06692922]
<a id="pgfId-1013173"></a> [0.7749394 ]], shape=(3, 1), dtype=float32)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1032727"></a><span class="fm-combinumeral">❶</span> Tensor of random values drawn from a normal distribution with mean 0 and standard deviation 1. Equivalent to np.random.normal(size=(3, 1), loc=0., scale=1.).</p>

  <p class="fm-code-annotation"><a id="pgfId-1032748"></a><span class="fm-combinumeral">❷</span> Tensor of random values drawn from a uniform distribution between 0 and 1. Equivalent to np.random.uniform(size=(3, 1), low=0., high=1.).</p>

  <p class="body"><a id="pgfId-1013215"></a>A significant difference between NumPy arrays and TensorFlow tensors is that TensorFlow tensors aren’t assignable: they’re constant. For instance, in NumPy, you can do the following.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1013272"></a>Listing 3.3 NumPy arrays are assignable</p>
  <pre class="programlisting"><a id="pgfId-1026701"></a><b class="fm-codebrown">import</b> numpy <b class="fm-codebrown">as</b> np
<a id="pgfId-1026702"></a>x = np.ones(shape=(<span class="fm-codeblue">2</span>, <span class="fm-codeblue">2</span>))
<a id="pgfId-1013317"></a>x[<span class="fm-codeblue">0</span>, <span class="fm-codeblue">0</span>] = <span class="fm-codeblue">0</span>.</pre>

  <p class="body"><a id="pgfId-1013323"></a>Try to do the same thing in TensorFlow, and you will get an error: “EagerTensor object does not support item assignment.”</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1013380"></a>Listing 3.4 TensorFlow tensors are not assignable</p>
  <pre class="programlisting"><a id="pgfId-1026719"></a>x = tf.ones(shape=(<span class="fm-codeblue">2</span>, <span class="fm-codeblue">2</span>))
<a id="pgfId-1013419"></a>x[<span class="fm-codeblue">0</span>, <span class="fm-codeblue">0</span>] = <span class="fm-codeblue">0.</span>             <span class="fm-combinumeral">❶</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1032661"></a><span class="fm-combinumeral">❶</span> This will fail, as a tensor isn’t assignable.</p>

  <p class="body"><a id="pgfId-1013451"></a>To train a model, we’ll need to update its state, which is a set of tensors. If tensors aren’t assignable, how do we do it? That’s where <i class="fm-italics">variables</i> come in. <code class="fm-code-in-text">tf.Variable</code> is the class <a id="marker-1013462"></a>meant to manage modifiable state in TensorFlow. You’ve already briefly seen it in action in the training loop implementation at the end of chapter 2.</p>

  <p class="body"><a id="pgfId-1013472"></a>To create a variable, you need to provide some initial value, such as a random tensor.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1013529"></a>Listing 3.5 Creating a TensorFlow variable</p>
  <pre class="programlisting"><a id="pgfId-1028930"></a>&gt;&gt;&gt; v = tf.Variable(initial_value=tf.random.normal(shape=(<span class="fm-codeblue">3</span>, <span class="fm-codeblue">1</span>)))
<a id="pgfId-1026735"></a>&gt;&gt;&gt; <b class="fm-codebrown">print</b>(v)
<a id="pgfId-1026736"></a>array([[-0.75133973],
<a id="pgfId-1026737"></a>       [-0.4872893 ],
<a id="pgfId-1013586"></a>       [ 1.6626885 ]], dtype=float32)&gt;</pre>

  <p class="body"><a id="pgfId-1013605"></a>The state of a variable can be modified via <a id="marker-1013594"></a>its <code class="fm-code-in-text">assign</code> method, as follows.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1013665"></a>Listing 3.6 Assigning a value to a TensorFlow variable</p>
  <pre class="programlisting"><a id="pgfId-1026754"></a>&gt;&gt;&gt; v.assign(tf.ones((<span class="fm-codeblue">3</span>, <span class="fm-codeblue">1</span>)))
<a id="pgfId-1026755"></a>array([[1.],
<a id="pgfId-1026756"></a>       [1.],
<a id="pgfId-1013716"></a>       [1.]], dtype=float32)&gt;</pre>

  <p class="body"><a id="pgfId-1013722"></a>It also works for a subset of the coefficients.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1013779"></a>Listing 3.7 Assigning a value to a subset of a TensorFlow variable</p>
  <pre class="programlisting"><a id="pgfId-1026769"></a>&gt;&gt;&gt; v[<span class="fm-codeblue">0</span>, <span class="fm-codeblue">0</span>].assign(<span class="fm-codeblue">3.</span>)
<a id="pgfId-1026770"></a>array([[3.],
<a id="pgfId-1026771"></a>       [1.],
<a id="pgfId-1013830"></a>       [1.]], dtype=float32)&gt;</pre>

  <p class="body"><a id="pgfId-1013872"></a>Similarly, <code class="fm-code-in-text">assign_add()</code> and <code class="fm-code-in-text">assign_sub()</code> are efficient equivalents of <code class="fm-code-in-text">+=</code> and <code class="fm-code-in-text">-=</code>, as shown next. <a id="marker-1028987"></a><a id="marker-1028988"></a></p>

  <p class="fm-code-listing-caption"><a id="pgfId-1013937"></a>Listing 3.8 Using <code class="fm-code-in-text">assign_add()</code></p>
  <pre class="programlisting"><a id="pgfId-1026786"></a>&gt;&gt;&gt; v.assign_add(tf.ones((<span class="fm-codeblue">3</span>, <span class="fm-codeblue">1</span>)))
<a id="pgfId-1026787"></a>array([[2.],
<a id="pgfId-1026788"></a>       [2.],
<a id="pgfId-1013996"></a>       [2.]], dtype=float32)&gt;</pre>

  <h3 class="fm-head1" id="heading_id_11"><a id="pgfId-1014002"></a>3.5.2 Tensor operations: Doing math in TensorFlow</h3>

  <p class="body"><a id="pgfId-1014019"></a><a id="marker-1014013"></a><a id="marker-1014015"></a>Just like NumPy, TensorFlow offers a large collection of tensor operations to express mathematical formulas. Here are a few examples.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1014075"></a>Listing 3.9 A few basic math operations</p>
  <pre class="programlisting"><a id="pgfId-1026805"></a>a = tf.ones((<span class="fm-codeblue">2</span>, <span class="fm-codeblue">2</span>))
<a id="pgfId-1014114"></a>b = tf.square(a)       <span class="fm-combinumeral">❶</span>
<a id="pgfId-1014126"></a>c = tf.sqrt(a)         <span class="fm-combinumeral">❷</span>
<a id="pgfId-1014138"></a>d = b + c              <span class="fm-combinumeral">❸</span>
<a id="pgfId-1014150"></a>e = tf.matmul(a, b)    <span class="fm-combinumeral">❹</span>
<a id="pgfId-1014162"></a>e *= d                 <span class="fm-combinumeral">❺</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1032359"></a><span class="fm-combinumeral">❶</span> Take the square.</p>

  <p class="fm-code-annotation"><a id="pgfId-1032376"></a><span class="fm-combinumeral">❷</span> Take the square root.</p>

  <p class="fm-code-annotation"><a id="pgfId-1032400"></a><span class="fm-combinumeral">❸</span> Add two tensors (element-wise).</p>

  <p class="fm-code-annotation"><a id="pgfId-1032420"></a><span class="fm-combinumeral">❹</span> Take the product of two tensors (as discussed in chapter 2).</p>

  <p class="fm-code-annotation"><a id="pgfId-1032437"></a><span class="fm-combinumeral">❺</span> Multiply two tensors (element-wise).</p>

  <p class="body"><a id="pgfId-1014271"></a>Importantly, each of the preceding operations gets executed on the fly: at any point, you can print what the current result is, just like in NumPy. We call <a id="marker-1014260"></a>this <i class="fm-italics">eager execution</i>. <a id="marker-1014276"></a><a id="marker-1014279"></a></p>

  <h3 class="fm-head1" id="heading_id_12"><a id="pgfId-1014285"></a>3.5.3 A second look at the GradientTape API</h3>

  <p class="body"><a id="pgfId-1014308"></a><a id="marker-1014296"></a><a id="marker-1014298"></a>So far, TensorFlow seems to look a lot like NumPy. But here’s something NumPy can’t do: retrieve the gradient of any differentiable expression with respect to any of its inputs. Just open a <code class="fm-code-in-text">GradientTape</code> scope, apply some computation to one or several input tensors, and retrieve the gradient of the result with respect to the inputs.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1014368"></a>Listing 3.10 Using the <code class="fm-code-in-text">GradientTape</code></p>
  <pre class="programlisting"><a id="pgfId-1026874"></a>input_var = tf.Variable(initial_value=<span class="fm-codeblue">3.</span>) 
<a id="pgfId-1026875"></a><b class="fm-codebrown">with</b> tf.GradientTape() <b class="fm-codebrown">as</b> tape:
<a id="pgfId-1026876"></a>   result = tf.square(input_var)
<a id="pgfId-1014427"></a>gradient = tape.gradient(result, input_var)</pre>

  <p class="body"><a id="pgfId-1014433"></a>This is most commonly used to retrieve the gradients of the loss of a model with respect to its weights: <code class="fm-code-in-text">gradients</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">tape.gradient(loss,</code><a id="marker-1029465"></a> <code class="fm-code-in-text">weights)</code>. You saw this in action in chapter 2.</p>

  <p class="body"><a id="pgfId-1014493"></a>So far, you’ve only seen the case where the input tensors <a id="marker-1014450"></a>in <code class="fm-code-in-text">tape.gradient()</code> were TensorFlow variables. It’s actually possible for these inputs to be any arbitrary tensor. However, only <i class="fm-italics">trainable variables</i> are tracked <a id="marker-1014476"></a>by default. With a constant tensor, you’d have to manually mark it as being tracked by <a id="marker-1014482"></a>calling <code class="fm-code-in-text">tape.watch()</code> on it<a id="marker-1029476"></a>.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1014553"></a>Listing 3.11 Using <code class="fm-code-in-text">GradientTape</code> with constant tensor inputs</p>
  <pre class="programlisting"><a id="pgfId-1026895"></a>input_const = tf.constant(<span class="fm-codeblue">3.</span>) 
<a id="pgfId-1026896"></a><b class="fm-codebrown">with</b> tf.GradientTape() <b class="fm-codebrown">as</b> tape:
<a id="pgfId-1026897"></a>   tape.watch(input_const)
<a id="pgfId-1026898"></a>   result = tf.square(input_const)
<a id="pgfId-1014623"></a>gradient = tape.gradient(result, input_const)</pre>

  <p class="body"><a id="pgfId-1014629"></a>Why is this necessary? Because it would be too expensive to preemptively store the information required to compute the gradient of anything with respect to anything. To avoid wasting resources, the tape needs to know what to watch. Trainable variables are watched by default because computing the gradient of a loss with regard to a list of trainable variables is the most common use of the gradient tape.</p>

  <p class="body"><a id="pgfId-1014635"></a>The gradient tape is a powerful utility, even capable of computing <i class="fm-italics">second-order gradients</i>, that is <a id="marker-1014646"></a>to say, the gradient of a gradient. For instance, the gradient of the position of an object with regard to time is the speed of that object, and the second-order gradient is its acceleration.</p>

  <p class="body"><a id="pgfId-1014656"></a>If you measure the position of a falling apple along a vertical axis over time and find that it verifies <code class="fm-code-in-text">position(time)</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">4.9</code> <code class="fm-code-in-text">*</code> <code class="fm-code-in-text">time</code> <code class="fm-code-in-text">**</code> <code class="fm-code-in-text">2</code>, what is its acceleration? Let’s use two nested gradient tapes to find out.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1014722"></a>Listing 3.12 Using nested gradient tapes to compute second-order gradients</p>
  <pre class="programlisting"><a id="pgfId-1026915"></a>time = tf.Variable(<span class="fm-codeblue">0.</span>) 
<a id="pgfId-1026916"></a><b class="fm-codebrown">with</b> tf.GradientTape() <b class="fm-codebrown">as</b> outer_tape:
<a id="pgfId-1026917"></a>    <b class="fm-codebrown">with</b> tf.GradientTape() <b class="fm-codebrown">as</b> inner_tape:
<a id="pgfId-1026918"></a>        position =  <span class="fm-codeblue">4.9</span> * time ** <span class="fm-codeblue">2</span> 
<a id="pgfId-1026919"></a>    speed = inner_tape.gradient(position, time)
<a id="pgfId-1014785"></a>acceleration = outer_tape.gradient(speed, time)    <span class="fm-combinumeral">❶</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1032306"></a><span class="fm-combinumeral">❶</span> We use the outer tape to compute the gradient of the gradient from the inner tape. Naturally, the answer is 4.9 * 2 = 9.8. <a id="marker-1032311"></a><a id="marker-1032312"></a></p>

  <h3 class="fm-head1" id="heading_id_13"><a id="pgfId-1014822"></a>3.5.4 An end-to-end example: A linear classifier in pure TensorFlow</h3>

  <p class="body"><a id="pgfId-1014839"></a><a id="marker-1014833"></a><a id="marker-1014835"></a>You know about tensors, variables, and tensor operations, and you know how to compute gradients. That’s enough to build any machine learning model based on gradient descent. And you’re only at chapter 3!</p>

  <p class="body"><a id="pgfId-1014844"></a>In a machine learning job interview, you may be asked to implement a linear classifier from scratch in TensorFlow: a very simple task that serves as a filter between candidates who have some minimal machine learning background and those who don’t. Let’s get you past that filter and use your newfound knowledge of TensorFlow to implement such a linear classifier.</p>

  <p class="body"><a id="pgfId-1014850"></a>First, let’s come up with some nicely linearly separable synthetic data to work with: two classes of points in a 2D plane. We’ll generate each class of points by drawing their coordinates from a random distribution with a specific covariance matrix and a specific mean. Intuitively, the covariance matrix describes the shape of the point cloud, and the mean describes its position in the plane (see figure 3.6). We’ll reuse the same covariance matrix for both point clouds, but we’ll use two different mean values—the point clouds will have the same shape, but different positions.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1014907"></a>Listing 3.13 Generating two classes of random points in a 2D plane</p>
  <pre class="programlisting"><a id="pgfId-1026934"></a>num_samples_per_class = <span class="fm-codeblue">1000</span> 
<a id="pgfId-1014946"></a>negative_samples = np.random.multivariate_normal(   <span class="fm-combinumeral">❶</span>
<a id="pgfId-1014958"></a>    mean=[<span class="fm-codeblue">0</span>, <span class="fm-codeblue">3</span>],                                    <span class="fm-combinumeral">❶</span>
<a id="pgfId-1014970"></a>    cov=[[<span class="fm-codeblue">1</span>, <span class="fm-codeblue">0.5</span>],[<span class="fm-codeblue">0.5</span>, <span class="fm-codeblue">1</span>]],                        <span class="fm-combinumeral">❶</span>
<a id="pgfId-1014982"></a>    size=num_samples_per_class)                     <span class="fm-combinumeral">❶</span>
<a id="pgfId-1014994"></a>positive_samples = np.random.multivariate_normal(   <span class="fm-combinumeral">❷</span>
<a id="pgfId-1015006"></a>    mean=[<span class="fm-codeblue">3</span>, <span class="fm-codeblue">0</span>],                                    <span class="fm-combinumeral">❷</span>
<a id="pgfId-1015018"></a>    cov=[[<span class="fm-codeblue">1</span>, <span class="fm-codeblue">0.5</span>],[<span class="fm-codeblue">0.5</span>, <span class="fm-codeblue">1</span>]],                        <span class="fm-combinumeral">❷</span>
<a id="pgfId-1015030"></a>    size=num_samples_per_class)                     <span class="fm-combinumeral">❷</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1032020"></a><span class="fm-combinumeral">❶</span> Generate the first class of points: 1000 random 2D points. cov=[[1, 0.5],[0.5, 1]] corresponds to an oval-like point cloud oriented from bottom left to top right.</p>

  <p class="fm-code-annotation"><a id="pgfId-1032041"></a><span class="fm-combinumeral">❷</span> Generate the other class of points with a different mean and the same covariance matrix.</p>

  <p class="body"><a id="pgfId-1015138"></a>In the preceding code, <code class="fm-code-in-text">negative_samples</code> and <code class="fm-code-in-text">positive_samples</code> are both arrays with shape <code class="fm-code-in-text">(1000,</code> <code class="fm-code-in-text">2)</code>. Let’s stack them into a single array with shape <code class="fm-code-in-text">(2000,</code> <code class="fm-code-in-text">2)</code>.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1015198"></a>Listing 3.14 Stacking the two classes into an array with shape (2000, 2)</p>
  <pre class="programlisting"><a id="pgfId-1015147"></a>inputs = np.vstack((negative_samples, positive_samples)).astype(np.float32)</pre>

  <p class="body"><a id="pgfId-1015263"></a>Let’s generate the corresponding target labels, an array of zeros and ones of shape <code class="fm-code-in-text">(2000,</code> <code class="fm-code-in-text">1)</code>, where <code class="fm-code-in-text">targets[i,</code> <code class="fm-code-in-text">0]</code> is 0 if <code class="fm-code-in-text">inputs[i]</code> belongs to class 0 (and inversely).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1015323"></a>Listing 3.15 Generating the corresponding targets (0 and 1)</p>
  <pre class="programlisting"><a id="pgfId-1027059"></a>targets = np.vstack((np.zeros((num_samples_per_class, <span class="fm-codeblue">1</span>), dtype=<span class="fm-codegreen">"float32"</span>),
<a id="pgfId-1015362"></a>                     np.ones((num_samples_per_class, <span class="fm-codeblue">1</span>), dtype=<span class="fm-codegreen">"float32"</span>)))</pre>

  <p class="body"><a id="pgfId-1015368"></a>Next, let’s plot our data with Matplotlib.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1015425"></a>Listing 3.16 Plotting the two point classes (see figure 3.6)</p>
  <pre class="programlisting"><a id="pgfId-1027072"></a><b class="fm-codebrown">import</b> matplotlib.pyplot <b class="fm-codebrown">as</b> plt
<a id="pgfId-1027073"></a>plt.scatter(inputs[:, <span class="fm-codeblue">0</span>], inputs[:, <span class="fm-codeblue">1</span>], c=targets[:, <span class="fm-codeblue">0</span>])
<a id="pgfId-1015470"></a>plt.show()</pre>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/03-06.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1033971"></a>Figure 3.6 Our synthetic data: two classes of random points in the 2D plane</p>

  <p class="body"><a id="pgfId-1015476"></a>Now let’s create a linear classifier that can learn to separate these two blobs. A linear classifier is an affine transformation (<code class="fm-code-in-text">prediction</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">W</code> <code class="fm-code-in-text">•</code> <code class="fm-code-in-text">input</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">b</code>) trained to minimize the square of the difference between predictions and the targets.</p>

  <p class="body"><a id="pgfId-1015491"></a>As you’ll see, it’s actually a much simpler example than the end-to-end example of a toy two-layer neural network you saw at the end of chapter 2. However, this time you should be able to understand everything about the code, line by line.</p>

  <p class="body"><a id="pgfId-1015513"></a>Let’s create our variables, <code class="fm-code-in-text">W</code> and <code class="fm-code-in-text">b</code>, initialized with random values and with zeros, respectively.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1015573"></a>Listing 3.17 Creating the linear classifier variables</p>
  <pre class="programlisting"><a id="pgfId-1015522"></a>input_dim = <span class="fm-codeblue">2</span>      <span class="fm-combinumeral">❶</span>
<a id="pgfId-1015618"></a>output_dim = <span class="fm-codeblue">1</span>     <span class="fm-combinumeral">❷</span>
<a id="pgfId-1015630"></a>W = tf.Variable(initial_value=tf.random.uniform(shape=(input_dim, output_dim)))
<a id="pgfId-1015636"></a>b = tf.Variable(initial_value=tf.zeros(shape=(output_dim,)))</pre>

  <p class="fm-code-annotation"><a id="pgfId-1031914"></a><span class="fm-combinumeral">❶</span> The inputs will be 2D points.</p>

  <p class="fm-code-annotation"><a id="pgfId-1031935"></a><span class="fm-combinumeral">❷</span> The output predictions will be a single score per sample (close to 0 if the sample is predicted to be in class 0, and close to 1 if the sample is predicted to be in class 1).</p>

  <p class="body"><a id="pgfId-1015678"></a>Here’s our forward pass function.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1015735"></a>Listing 3.18 The forward pass function</p>
  <pre class="programlisting"><a id="pgfId-1027127"></a><b class="fm-codebrown">def</b> model(inputs):
<a id="pgfId-1015774"></a>    <b class="fm-codebrown">return</b> tf.matmul(inputs, W) + b</pre>

  <p class="body"><a id="pgfId-1015846"></a>Because our linear classifier operates on 2D inputs, <code class="fm-code-in-text">W</code> is really just two scalar coefficients, <code class="fm-code-in-text">w1</code> and <code class="fm-code-in-text">w2</code>: <code class="fm-code-in-text">W</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">[[w1],</code> <code class="fm-code-in-text">[w2]]</code>. Meanwhile, <code class="fm-code-in-text">b</code> is a single scalar coefficient. As such, for a given input point <code class="fm-code-in-text">[x,</code> <code class="fm-code-in-text">y]</code>, its prediction value is <code class="fm-code-in-text">prediction</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">[[w1],</code> <code class="fm-code-in-text">[w2]]</code> <code class="fm-code-in-text">•</code> <code class="fm-code-in-text">[x,</code> <code class="fm-code-in-text">y]</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">b</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">w1</code> <code class="fm-code-in-text">*</code> <code class="fm-code-in-text">x</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">w2</code> <code class="fm-code-in-text">*</code> <code class="fm-code-in-text">y</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">b</code>.</p>

  <p class="body"><a id="pgfId-1015855"></a>The following listing shows our loss function.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1015912"></a>Listing 3.19 The mean squared error loss function</p>
  <pre class="programlisting"><a id="pgfId-1027144"></a><b class="fm-codebrown">def</b> square_loss(targets, predictions):
<a id="pgfId-1015951"></a>    per_sample_losses = tf.square(targets - predictions)   <span class="fm-combinumeral">❶</span>
<a id="pgfId-1015963"></a>    <b class="fm-codebrown">return</b> tf.reduce_mean(per_sample_losses)               <span class="fm-combinumeral">❷</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1031775"></a><span class="fm-combinumeral">❶</span> per_sample_losses will be a tensor with the same shape as targets and predictions, containing per-sample loss scores.</p>

  <p class="fm-code-annotation"><a id="pgfId-1031796"></a><span class="fm-combinumeral">❷</span> We need to average these per-sample loss scores into a single scalar loss value: this is what reduce_mean does.</p>

  <p class="body"><a id="pgfId-1016027"></a>Next is the training step, which receives some training data and updates the weights <code class="fm-code-in-text">W</code> and <code class="fm-code-in-text">b</code> so as to minimize the loss on the data.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1016087"></a>Listing 3.20 The training step function</p>
  <pre class="programlisting"><a id="pgfId-1027171"></a>learning_rate = <span class="fm-codeblue">0.1</span> 
<a id="pgfId-1027172"></a>  
<a id="pgfId-1027173"></a><b class="fm-codebrown">def</b> training_step(inputs, targets):
<a id="pgfId-1016137"></a>    <b class="fm-codebrown">with</b> tf.GradientTape() <b class="fm-codebrown">as</b> tape:                                  <span class="fm-combinumeral">❶</span>
<a id="pgfId-1016149"></a>        predictions = model(inputs)                                  <span class="fm-combinumeral">❶</span>
<a id="pgfId-1016161"></a>        loss = square_loss(predictions, targets)                     <span class="fm-combinumeral">❶</span>
<a id="pgfId-1016173"></a>    grad_loss_wrt_W, grad_loss_wrt_b = tape.gradient(loss, [W, b])   <span class="fm-combinumeral">❷</span>
<a id="pgfId-1016185"></a>    W.assign_sub(grad_loss_wrt_W * learning_rate)                    <span class="fm-combinumeral">❸</span>
<a id="pgfId-1016197"></a>    b.assign_sub(grad_loss_wrt_b * learning_rate)                    <span class="fm-combinumeral">❸</span>
<a id="pgfId-1016209"></a>    <b class="fm-codebrown">return</b> loss</pre>

  <p class="fm-code-annotation"><a id="pgfId-1031497"></a><span class="fm-combinumeral">❶</span> Forward pass, inside a gradient tape scope</p>

  <p class="fm-code-annotation"><a id="pgfId-1031518"></a><span class="fm-combinumeral">❷</span> Retrieve the gradient of the loss with regard to weights.</p>

  <p class="fm-code-annotation"><a id="pgfId-1031535"></a><span class="fm-combinumeral">❸</span> Update the weights.</p>

  <p class="body"><a id="pgfId-1016299"></a>For simplicity, we’ll do <i class="fm-italics">batch training</i> instead of <i class="fm-italics">mini-batch training</i>: we’ll run <a id="marker-1016288"></a>each training step (gradient computation and weight update) for all the data, rather than iterate over the data in small batches. On one hand, this means that each training step will take much longer to run, since we’ll compute the forward pass and the gradients for 2,000 samples at once. On the other hand, each gradient update will be much more effective at reducing the loss on the training data, since it will encompass information from all training samples instead of, say, only 128 random samples. As a result, we will need many fewer steps of training, and we should use a larger learning rate than we would typically use for mini-batch training (we’ll use <code class="fm-code-in-text">learning_rate</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">0.1</code>, defined in listing 3.20).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1016359"></a>Listing 3.21 The batch training loop</p>
  <pre class="programlisting"><a id="pgfId-1027276"></a><b class="fm-codebrown">for</b> step <b class="fm-codebrown">in</b> range(<span class="fm-codeblue">40</span>):
<a id="pgfId-1027277"></a>    loss = training_step(inputs, targets)
<a id="pgfId-1016404"></a>    <b class="fm-codebrown">print</b>(f<span class="fm-codegreen">"Loss at step {step}: {loss:.4f}"</span>)</pre>

  <p class="body"><a id="pgfId-1016410"></a>After 40 steps, the training loss seems to have stabilized around 0.025. Let’s plot how our linear model classifies the training data points. Because our targets are zeros and ones, a given input point will be classified as “0” if its prediction value is below 0.5, and as “1” if it is above 0.5 (see figure 3.7):</p>
  <pre class="programlisting"><a id="pgfId-1027294"></a>predictions = model(inputs)
<a id="pgfId-1027295"></a>plt.scatter(inputs[:, <span class="fm-codeblue">0</span>], inputs[:, <span class="fm-codeblue">1</span>], c=predictions[:, <span class="fm-codeblue">0</span>] &gt; <span class="fm-codeblue">0.5</span>)
<a id="pgfId-1016436"></a>plt.show()</pre>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/03-07.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1034017"></a>Figure 3.7 Our model’s predictions on the training inputs: pretty similar to the training targets</p>

  <p class="body"><a id="pgfId-1016532"></a>Recall that the prediction value for a given point <code class="fm-code-in-text">[x,</code> <code class="fm-code-in-text">y]</code> is simply <code class="fm-code-in-text">prediction</code> <code class="fm-code-in-text">==</code> <code class="fm-code-in-text">[[w1],</code> <code class="fm-code-in-text">[w2]]</code> <code class="fm-code-in-text">•</code> <code class="fm-code-in-text">[x,</code> <code class="fm-code-in-text">y]</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">b</code> <code class="fm-code-in-text">==</code> <code class="fm-code-in-text">w1</code> <code class="fm-code-in-text">*</code> <code class="fm-code-in-text">x</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">w2</code> <code class="fm-code-in-text">*</code> <code class="fm-code-in-text">y</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">b</code>. Thus, class 0 is defined as <code class="fm-code-in-text">w1</code> <code class="fm-code-in-text">*</code> <code class="fm-code-in-text">x</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">w2</code> <code class="fm-code-in-text">*</code> <code class="fm-code-in-text">y</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">b</code> <code class="fm-code-in-text">&lt;</code> <code class="fm-code-in-text">0.5</code>, and class 1 is defined as <code class="fm-code-in-text">w1</code> <code class="fm-code-in-text">*</code> <code class="fm-code-in-text">x</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">w2</code> <code class="fm-code-in-text">*</code> <code class="fm-code-in-text">y</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">b</code> <code class="fm-code-in-text">&gt;</code> <code class="fm-code-in-text">0.5</code>. You’ll notice that what you’re looking at is really the equation of a line in the 2D plane: <code class="fm-code-in-text">w1</code> <code class="fm-code-in-text">*</code> <code class="fm-code-in-text">x</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">w2</code> <code class="fm-code-in-text">*</code> <code class="fm-code-in-text">y</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">b</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">0.5</code>. Above the line is class 1, and below the line is class 0. You may be used to seeing line equations in the format <code class="fm-code-in-text">y</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">a</code> <code class="fm-code-in-text">*</code> <code class="fm-code-in-text">x</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">b</code>; in the same format, our line becomes <code class="fm-code-in-text">y</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">-</code> <code class="fm-code-in-text">w1</code> <code class="fm-code-in-text">/</code> <code class="fm-code-in-text">w2</code> <code class="fm-code-in-text">*</code> <code class="fm-code-in-text">x</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">(0.5</code> <code class="fm-code-in-text">-</code> <code class="fm-code-in-text">b)</code> <code class="fm-code-in-text">/</code> <code class="fm-code-in-text">w2</code>.</p>

  <p class="body"><a id="pgfId-1016541"></a>Let’s plot this line (shown in figure 3.8):</p>
  <pre class="programlisting"><a id="pgfId-1016547"></a>x = np.linspace(-<span class="fm-codeblue">1</span>, <span class="fm-codeblue">4</span>, <span class="fm-codeblue">100</span>)                                          <span class="fm-combinumeral">❶</span>
<a id="pgfId-1016567"></a>y = - W[<span class="fm-codeblue">0</span>] /  W[<span class="fm-codeblue">1</span>] * x + (<span class="fm-codeblue">0.5</span> - b) / W[<span class="fm-codeblue">1</span>]                            <span class="fm-combinumeral">❷</span>
<a id="pgfId-1016579"></a>plt.plot(x, y, <span class="fm-codegreen">"-r"</span>)                                                 <span class="fm-combinumeral">❸</span>
<a id="pgfId-1016591"></a>plt.scatter(inputs[:, <span class="fm-codeblue">0</span>], inputs[:, <span class="fm-codeblue">1</span>], c=predictions[:, <span class="fm-codeblue">0</span>] &gt; <span class="fm-codeblue">0.5</span>)   <span class="fm-combinumeral">❹</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1031193"></a><span class="fm-combinumeral">❶</span> Generate 100 regularly spaced numbers between –1 and 4, which we will use to plot our line.</p>

  <p class="fm-code-annotation"><a id="pgfId-1031214"></a><span class="fm-combinumeral">❷</span> This is our line’s equation.</p>

  <p class="fm-code-annotation"><a id="pgfId-1031231"></a><span class="fm-combinumeral">❸</span> Plot our line ("-r" means “plot it as a red line”).</p>

  <p class="fm-code-annotation"><a id="pgfId-1031248"></a><span class="fm-combinumeral">❹</span> Plot our model’s predictions on the same plot.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/03-08.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1034063"></a>Figure 3.8 Our model, visualized as a line</p>

  <p class="body"><a id="pgfId-1016681"></a>This is really what a linear classifier is all about: finding the parameters of a line (or, in higher-dimensional spaces, a hyperplane) neatly separating two classes of data. <a id="marker-1016697"></a><a id="marker-1016700"></a><a id="marker-1016702"></a></p>

  <h2 class="fm-head" id="heading_id_14"><a id="pgfId-1016708"></a>3.6 Anatomy of a neural network: Understanding core Keras APIs</h2>

  <p class="body"><a id="pgfId-1016718"></a><a id="marker-1016719"></a>At this point, you know the basics of TensorFlow, and you can use it to implement a toy model from scratch, such as the batch linear classifier in the previous section, or the toy neural network at the end of chapter 2. That’s a solid foundation to build upon. It’s now time to move on to a more productive, more robust path to deep learning: the Keras API.</p>

  <h3 class="fm-head1" id="heading_id_15"><a id="pgfId-1016727"></a>3.6.1 Layers: The building blocks of deep learning</h3>

  <p class="body"><a id="pgfId-1016776"></a><a id="marker-1016738"></a><a id="marker-1016740"></a>The fundamental data structure in neural networks is the <i class="fm-italics">layer</i>, to which you were introduced in chapter 2. A layer is a data processing module that takes as input one or more tensors and that outputs one or more tensors. Some layers are stateless, but more frequently layers have a state: the layer’s <i class="fm-italics">weights</i>, one or <a id="marker-1016765"></a>several tensors learned with stochastic gradient descent, which together contain the network’s <i class="fm-italics">knowledge</i>.</p>

  <p class="body"><a id="pgfId-1016917"></a>Different types of layers are appropriate for different tensor formats and different types of data processing. For instance, simple vector data, stored in rank-2 tensors of shape <code class="fm-code-in-text">(samples,</code> <code class="fm-code-in-text">features)</code>, is often processed <a id="marker-1016796"></a>by <i class="fm-italics">densely connected</i> layers, also called <i class="fm-italics">fully connected</i> or <i class="fm-italics">dense</i> layers (the <code class="fm-code-in-text">Dense</code> class in <a id="marker-1016842"></a>Keras). Sequence data, stored in rank-3 tensors of shape <code class="fm-code-in-text">(samples,</code> <code class="fm-code-in-text">timesteps,</code> <code class="fm-code-in-text">features)</code>, is typically processed <a id="marker-1016858"></a>by <i class="fm-italics">recurrent</i> layers, such as <a id="marker-1016874"></a>an <code class="fm-code-in-text">LSTM</code> layer, or 1D convolution layers (<code class="fm-code-in-text">Conv1D</code>). Image data, stored in <a id="marker-1016900"></a>rank-4 tensors, is usually processed <a id="marker-1016906"></a>by 2D convolution layers (<code class="fm-code-in-text">Conv2D</code>).</p>

  <p class="body"><a id="pgfId-1016926"></a>You can think of layers as the LEGO bricks of deep learning, a metaphor that is made explicit by Keras. Building deep learning models in Keras is done by clipping together compatible layers to form useful data-transformation pipelines.</p>

  <p class="fm-head2"><a id="pgfId-1016932"></a>The base Layer class in Keras</p>

  <p class="body"><a id="pgfId-1016975"></a><a id="marker-1016943"></a><a id="marker-1016945"></a>A simple API should have a single abstraction around which everything is centered. In Keras, that’s the <code class="fm-code-in-text">Layer</code> class. Everything in Keras is either a <code class="fm-code-in-text">Layer</code> or something that closely interacts with a <code class="fm-code-in-text">Layer</code>.</p>

  <p class="body"><a id="pgfId-1017032"></a>A <code class="fm-code-in-text">Layer</code> is an object that encapsulates some state (weights) and some computation (a forward pass). The weights are typically defined <a id="marker-1016995"></a>in a <code class="fm-code-in-text">build()</code> (although they could also be created in the constructor, <code class="fm-code-in-text">__init__()</code>), and the computation is defined in <a id="marker-1017021"></a>the <code class="fm-code-in-text">call()</code> method.</p>

  <p class="body"><a id="pgfId-1017083"></a>In the previous chapter, we implemented a <code class="fm-code-in-text">NaiveDense</code> class that <a id="marker-1027379"></a>contained two weights <code class="fm-code-in-text">W</code> and <code class="fm-code-in-text">b</code> and applied the computation <code class="fm-code-in-text">output</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">activation(dot(input,</code> <code class="fm-code-in-text">W)</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">b)</code>. This is what the same layer would look like in Keras.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1017182"></a>Listing 3.22 A <code class="fm-code-in-text">Dense</code> layer implemented as a <code class="fm-code-in-text">Layer</code> subclass</p>
  <pre class="programlisting"><a id="pgfId-1027400"></a><b class="fm-codebrown">from</b> tensorflow <b class="fm-codebrown">import</b> keras
<a id="pgfId-1027401"></a>  
<a id="pgfId-1017207"></a><b class="fm-codebrown">class</b> SimpleDense(keras.layers.Layer):                               <span class="fm-combinumeral">❶</span>
<a id="pgfId-1027419"></a>    <b class="fm-codebrown">def</b> __init__(self, units, activation=<code class="fm-codegreen">None</code>):
<a id="pgfId-1027420"></a>        super().__init__()
<a id="pgfId-1027421"></a>        self.units = units
<a id="pgfId-1027422"></a>        self.activation = activation
<a id="pgfId-1027423"></a>  
<a id="pgfId-1017253"></a>    <b class="fm-codebrown">def</b> build(self, input_shape):                                    <span class="fm-combinumeral">❷</span>
<a id="pgfId-1027445"></a>        input_dim = input_shape[-<span class="fm-codeblue">1</span>]
<a id="pgfId-1017276"></a>        self.W = self.add_weight(shape=(input_dim, self.units),      <span class="fm-combinumeral">❸</span>
<a id="pgfId-1027462"></a>                                 initializer=<span class="fm-codegreen">"random_normal"</span>)
<a id="pgfId-1027463"></a>        self.b = self.add_weight(shape=(self.units,),
<a id="pgfId-1027464"></a>                                 initializer=<span class="fm-codegreen">"zeros"</span>)
<a id="pgfId-1027465"></a>  
<a id="pgfId-1017306"></a>    <b class="fm-codebrown">def</b> call(self, inputs):                                          <span class="fm-combinumeral">❹</span>
<a id="pgfId-1027480"></a>        y = tf.matmul(inputs, self.W) + self.b
<a id="pgfId-1027481"></a>        <b class="fm-codebrown">if</b> self.activation <b class="fm-codebrown">is</b> <b class="fm-codebrown">not</b> <code class="fm-codegreen">None</code>:
<a id="pgfId-1027482"></a>            y = self.activation(y)
<a id="pgfId-1017341"></a>        <b class="fm-codebrown">return</b> y</pre>

  <p class="fm-code-annotation"><a id="pgfId-1030900"></a><span class="fm-combinumeral">❶</span> All Keras layers inherit from the base Layer class.</p>

  <p class="fm-code-annotation"><a id="pgfId-1030921"></a><span class="fm-combinumeral">❷</span> Weight creation takes place in the build() method.</p>

  <p class="fm-code-annotation"><a id="pgfId-1030938"></a><span class="fm-combinumeral">❸</span> add_weight() is a shortcut method for creating weights. It is also possible to create standalone variables and assign them as layer attributes, like self.W = tf.Variable(tf.random.uniform(w_shape)).</p>

  <p class="fm-code-annotation"><a id="pgfId-1030955"></a><span class="fm-combinumeral">❹</span> We define the forward pass computation in the call() method.</p>

  <p class="body"><a id="pgfId-1017431"></a>In the next section, we’ll cover in detail the purpose of these <code class="fm-code-in-text">build()</code> and <code class="fm-code-in-text">call()</code> methods. Don’t worry if you don’t understand everything just yet!</p>

  <p class="body"><a id="pgfId-1017440"></a>Once instantiated, a layer like this can be used just like a function, taking as input a TensorFlow tensor:</p>
  <pre class="programlisting"><a id="pgfId-1017446"></a>&gt;&gt;&gt; my_dense = SimpleDense(units=<span class="fm-codeblue">32</span>, activation=tf.nn.relu)   <span class="fm-combinumeral">❶</span>
<a id="pgfId-1017466"></a>&gt;&gt;&gt; input_tensor = tf.ones(shape=(<span class="fm-codeblue">2</span>, <span class="fm-codeblue">784</span>))                    <span class="fm-combinumeral">❷</span>
<a id="pgfId-1017478"></a>&gt;&gt;&gt; output_tensor = my_dense(input_tensor)                    <span class="fm-combinumeral">❸</span>
<a id="pgfId-1027572"></a>&gt;&gt;&gt; <b class="fm-codebrown">print</b>(output_tensor.shape)
<a id="pgfId-1017496"></a>(2, 32))</pre>

  <p class="fm-code-annotation"><a id="pgfId-1030688"></a><span class="fm-combinumeral">❶</span> Instantiate our layer, defined previously.</p>

  <p class="fm-code-annotation"><a id="pgfId-1030705"></a><span class="fm-combinumeral">❷</span> Create some test inputs.</p>

  <p class="fm-code-annotation"><a id="pgfId-1030722"></a><span class="fm-combinumeral">❸</span> Call the layer on the inputs, just like a function.</p>

  <p class="body"><a id="pgfId-1017580"></a>You’re probably wondering, why did we have to implement <code class="fm-code-in-text">call()</code> and <code class="fm-code-in-text">build()</code>, since we ended up using our layer by plainly calling it, that is to say, by using its <code class="fm-code-in-text">__call__()</code> method? It’s because we want to be able to create the state just in time. Let’s see how that works. <a id="marker-1017585"></a><a id="marker-1017588"></a></p>

  <p class="fm-head2"><a id="pgfId-1017594"></a>Automatic shape inference: Building layers on the fly</p>

  <p class="body"><a id="pgfId-1017617"></a><a id="marker-1017605"></a><a id="marker-1017607"></a>Just like with LEGO bricks, you can only “clip” together layers that are compatible. The notion of <i class="fm-italics">layer compatibility</i> here refers <a id="marker-1017622"></a>specifically to the fact that every layer will only accept input tensors of a certain shape and will return output tensors of a certain shape. Consider the following example:</p>
  <pre class="programlisting"><a id="pgfId-1027587"></a><b class="fm-codebrown">from</b> tensorflow.keras <b class="fm-codebrown">import</b> layers
<a id="pgfId-1017646"></a>layer = layers.Dense(<span class="fm-codeblue">32</span>, activation=<span class="fm-codegreen">"relu"</span>)     <span class="fm-combinumeral">❶</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1030664"></a><span class="fm-combinumeral">❶</span> A dense layer with 32 output units</p>

  <p class="body"><a id="pgfId-1017678"></a>This layer will return a tensor where the first dimension has been transformed to be 32. It can only be connected to a downstream layer that expects 32-dimensional vectors as its input.</p>

  <p class="body"><a id="pgfId-1017684"></a>When using Keras, you don’t have to worry about size compatibility most of the time, because the layers you add to your models are dynamically built to match the shape of the incoming layer. For instance, suppose you write the following:</p>
  <pre class="programlisting"><a id="pgfId-1027606"></a><b class="fm-codebrown">from</b> tensorflow.keras <b class="fm-codebrown">import</b> models 
<a id="pgfId-1027607"></a><b class="fm-codebrown">from</b> tensorflow.keras <b class="fm-codebrown">import</b> layers
<a id="pgfId-1027608"></a>model = models.Sequential([
<a id="pgfId-1027609"></a>    layers.Dense(<span class="fm-codeblue">32</span>, activation=<span class="fm-codegreen">"relu"</span>),
<a id="pgfId-1027610"></a>    layers.Dense(<span class="fm-codeblue">32</span>)
<a id="pgfId-1017728"></a>])</pre>

  <p class="body"><a id="pgfId-1017734"></a>The layers didn’t receive any information about the shape of their inputs—instead, they automatically inferred their input shape as being the shape of the first inputs they see.</p>

  <p class="body"><a id="pgfId-1017756"></a>In the toy version of the <code class="fm-code-in-text">Dense</code> layer we implemented in chapter 2 (which we named <code class="fm-code-in-text">NaiveDense</code>), we had to pass the layer’s input size explicitly to the constructor in order to be able to create its weights. That’s not ideal, because it would lead to models that look like this, where each new layer needs to be made aware of the shape of the layer before it:</p>
  <pre class="programlisting"><a id="pgfId-1027629"></a>model = NaiveSequential([
<a id="pgfId-1027630"></a>    NaiveDense(input_size=<span class="fm-codeblue">784</span>, output_size=<span class="fm-codeblue">32</span>, activation=<span class="fm-codegreen">"relu"</span>),
<a id="pgfId-1027631"></a>    NaiveDense(input_size=<span class="fm-codeblue">32</span>, output_size=<span class="fm-codeblue">64</span>, activation=<span class="fm-codegreen">"relu"</span>),
<a id="pgfId-1027632"></a>    NaiveDense(input_size=<span class="fm-codeblue">64</span>, output_size=<span class="fm-codeblue">32</span>, activation=<span class="fm-codegreen">"relu"</span>),
<a id="pgfId-1027633"></a>    NaiveDense(input_size=<span class="fm-codeblue">32</span>, output_size=<span class="fm-codeblue">10</span>, activation=<span class="fm-codegreen">"softmax"</span>)
<a id="pgfId-1017803"></a>])</pre>

  <p class="body"><a id="pgfId-1017809"></a>It would be even worse if the rules used by a layer to produce its output shape are complex. For instance, what if our layer returned outputs of shape <code class="fm-code-in-text">(batch,</code> <code class="fm-code-in-text">input_ size</code> <code class="fm-code-in-text">*</code> <code class="fm-code-in-text">2</code> <code class="fm-code-in-text">if</code> <code class="fm-code-in-text">input_size</code> <code class="fm-code-in-text">%</code> <code class="fm-code-in-text">2</code> <code class="fm-code-in-text">==</code> <code class="fm-code-in-text">0</code> <code class="fm-code-in-text">else</code> <code class="fm-code-in-text">input_size</code> <code class="fm-code-in-text">*</code> <code class="fm-code-in-text">3)</code>?</p>

  <p class="body"><a id="pgfId-1017866"></a>If we were to reimplement our <code class="fm-code-in-text">NaiveDense</code> layer as <a id="marker-1017835"></a>a Keras layer capable of automatic shape inference, it would look like the previous <code class="fm-code-in-text">SimpleDense</code> layer (see listing 3.22), with its <code class="fm-code-in-text">build()</code> and <code class="fm-code-in-text">call()</code> methods.</p>

  <p class="body"><a id="pgfId-1017957"></a>In <code class="fm-code-in-text">SimpleDense</code>, we no longer create weights in the constructor like in the <code class="fm-code-in-text">NaiveDense</code> example; instead, we create them in a dedicated state-creation method, <code class="fm-code-in-text">build()</code>, which receives as an argument the first input shape seen by the layer. The <code class="fm-code-in-text">build()</code> method is called automatically the first time the layer is called (via its <code class="fm-code-in-text">__call__()</code> method). In fact, that’s why we defined the computation in a separate <code class="fm-code-in-text">call()</code> method rather than in <a id="marker-1017936"></a>the <code class="fm-code-in-text">__call__()</code> method directly. The <code class="fm-code-in-text">__call__()</code> method of the base layer schematically looks like this:</p>
  <pre class="programlisting"><a id="pgfId-1027652"></a><b class="fm-codebrown">def</b> __call__(self, inputs):
<a id="pgfId-1027653"></a>    <b class="fm-codebrown">if</b> <b class="fm-codebrown">not</b> self.built:
<a id="pgfId-1027654"></a>         self.build(inputs.shape)
<a id="pgfId-1027655"></a>         self.built = <code class="fm-codegreen">True</code>
<a id="pgfId-1017998"></a>    <b class="fm-codebrown">return</b> self.call(inputs)</pre>

  <p class="body"><a id="pgfId-1018004"></a>With automatic shape inference, our previous example becomes simple and neat:</p>
  <pre class="programlisting"><a id="pgfId-1027672"></a>model = keras.Sequential([
<a id="pgfId-1027673"></a>    SimpleDense(<span class="fm-codeblue">32</span>, activation=<span class="fm-codegreen">"relu"</span>),
<a id="pgfId-1027674"></a>    SimpleDense(<span class="fm-codeblue">64</span>, activation=<span class="fm-codegreen">"relu"</span>),
<a id="pgfId-1027675"></a>    SimpleDense(<span class="fm-codeblue">32</span>, activation=<span class="fm-codegreen">"relu"</span>),
<a id="pgfId-1027676"></a>    SimpleDense(<span class="fm-codeblue">10</span>, activation=<span class="fm-codegreen">"softmax"</span>)
<a id="pgfId-1018048"></a>])</pre>

  <p class="body"><a id="pgfId-1018107"></a>Note that automatic shape inference is not the only thing that <a id="marker-1018056"></a>the <code class="fm-code-in-text">Layer</code> class’s <code class="fm-code-in-text">__call__()</code> method handles. It takes care of many more things, in particular routing between <i class="fm-italics">eager</i> and <i class="fm-italics">graph</i> execution (a concept you’ll learn about in chapter 7), and input masking (which we’ll cover in chapter 11). For now, just remember: when implementing your own layers, put the forward pass in the <code class="fm-code-in-text">call()</code> method. <a id="marker-1018112"></a><a id="marker-1018115"></a><a id="marker-1018117"></a><a id="marker-1018119"></a></p>

  <h3 class="fm-head1" id="heading_id_16"><a id="pgfId-1018125"></a>3.6.2 From layers to models</h3>

  <p class="body"><a id="pgfId-1018171"></a><a id="marker-1018136"></a>A deep learning model is a graph of layers. In Keras, that’s the <code class="fm-code-in-text">Model</code> class. Until now, you’ve only <a id="marker-1018150"></a>seen <code class="fm-code-in-text">Sequential</code> models (a subclass of <code class="fm-code-in-text">Model</code>), which are simple stacks of layers, mapping a single input to a single output. But as you move forward, you’ll be exposed to a much broader variety of network topologies. These are some common ones:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018180"></a>Two-branch networks</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018194"></a>Multihead networks</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018204"></a>Residual connections</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1018214"></a>Network topology can get quite involved. For instance, figure 3.9 shows the topology of the graph of layers of a Transformer, a common architecture designed to process text data.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/03-09.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1034105"></a>Figure 3.9 The Transformer architecture (covered in chapter 11). There’s a lot going on here. Throughout the next few chapters, you’ll climb your way up to understanding it.</p>

  <p class="body"><a id="pgfId-1018230"></a>There are generally two ways of building such models in Keras: you could directly subclass the <code class="fm-code-in-text">Model</code> class, or you could use the Functional API, which lets you do more with less code. We’ll cover both approaches in chapter 7.</p>

  <p class="body"><a id="pgfId-1018281"></a>The topology of a model defines a <i class="fm-italics">hypothesis space</i>. You may <a id="marker-1018270"></a>remember that in chapter 1 we described machine learning as searching for useful representations of some input data, within a predefined <i class="fm-italics">space of possibilities</i>, using guidance from a feedback signal. By choosing a network topology, you constrain your space of possibilities (hypothesis space) to a specific series of tensor operations, mapping input data to output data. What you’ll then be searching for is a good set of values for the weight tensors involved in these tensor operations.</p>

  <p class="body"><a id="pgfId-1018290"></a>To learn from data, you have to make assumptions about it. These assumptions define what can be learned. As such, the structure of your hypothesis space—the architecture of your model—is extremely important. It encodes the assumptions you make about your problem, the prior knowledge that the model starts with. For instance, if you’re working on a two-class classification problem with a model made of a single <code class="fm-code-in-text">Dense</code> layer with no activation (a pure affine transformation), you are assuming that your two classes are linearly separable.</p>

  <p class="body"><a id="pgfId-1018305"></a>Picking the right network architecture is more an art than a science, and although there are some best practices and principles you can rely on, only practice can help you become a proper neural-network architect. The next few chapters will both teach you explicit principles for building neural networks and help you develop intuition as to what works or doesn’t work for specific problems. You’ll build a solid intuition about what type of model architectures work for different kinds of problems, how to build these networks in practice, how to pick the right learning configuration, and how to tweak a model until it yields the results you want to see. <a id="marker-1018307"></a></p>

  <h3 class="fm-head1" id="heading_id_17"><a id="pgfId-1018314"></a>3.6.3 The “compile” step: Configuring the learning process</h3>

  <p class="body"><a id="pgfId-1018324"></a><a id="marker-1018325"></a>Once the model architecture is defined, you still have to choose three more things:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018333"></a><i class="fm-italics1">Loss function (objective function)</i>—The quantity <a class="calibre11" id="marker-1018350"></a>that will be minimized during training. It represents a measure of success for the task at hand.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018360"></a><i class="fm-italics1">Optimizer</i>—Determines how <a class="calibre11" id="marker-1018373"></a>the network will be updated based on the loss function. It implements a specific variant of stochastic gradient descent (SGD).</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018383"></a><i class="fm-italics1">Metrics</i>—The measures <a class="calibre11" id="marker-1018396"></a>of success you want to monitor during training and validation, such as classification accuracy. Unlike the loss, training will not optimize directly for these metrics. As such, metrics don’t need to be differentiable.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1018449"></a>Once you’ve picked your loss, optimizer, and metrics, you can use the <a id="marker-1018408"></a>built-in <code class="fm-code-in-text">compile()</code> and <code class="fm-code-in-text">fit()</code> methods to start training your model. Alternatively, you could also write your own custom training loops—we’ll cover how to do this in chapter 7. It’s a lot more work! For now, let’s take a look at <code class="fm-code-in-text">compile()</code> and <code class="fm-code-in-text">fit()</code>.</p>

  <p class="body"><a id="pgfId-1018494"></a>The <code class="fm-code-in-text">compile()</code> method configures the training process—you’ve already been introduced to it in your very first neural network example in chapter 2. It takes the arguments <code class="fm-code-in-text">optimizer</code>, <code class="fm-code-in-text">loss</code>, and <code class="fm-code-in-text">metrics</code> (a list):</p>
  <pre class="programlisting"><a id="pgfId-1018503"></a>model = keras.Sequential([keras.layers.Dense(<span class="fm-codeblue">1</span>)])   <span class="fm-combinumeral">❶</span>
<a id="pgfId-1018523"></a>model.compile(optimizer=<span class="fm-codegreen">"rmsprop"</span>,                  <span class="fm-combinumeral">❷</span>
<a id="pgfId-1018535"></a>              loss=<span class="fm-codegreen">"mean_squared_error"</span>,            <span class="fm-combinumeral">❸</span>
<a id="pgfId-1018547"></a>              metrics=[<span class="fm-codegreen">"accuracy"</span>])                 <span class="fm-combinumeral">❹</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1030426"></a><span class="fm-combinumeral">❶</span> Define a linear classifier.</p>

  <p class="fm-code-annotation"><a id="pgfId-1030443"></a><span class="fm-combinumeral">❷</span> Specify the optimizer by name: RMSprop (it’s case-insensitive).</p>

  <p class="fm-code-annotation"><a id="pgfId-1030460"></a><span class="fm-combinumeral">❸</span> Specify the loss by name: mean squared error.</p>

  <p class="fm-code-annotation"><a id="pgfId-1030477"></a><span class="fm-combinumeral">❹</span> Specify a list of metrics: in this case, only accuracy.</p>

  <p class="body"><a id="pgfId-1018663"></a>In the preceding call to <code class="fm-code-in-text">compile()</code>, we passed the optimizer, loss, and metrics as strings (such as <code class="fm-code-in-text">"rmsprop"</code>). These strings are actually shortcuts that get converted to Python objects. For instance, <code class="fm-code-in-text">"rmsprop"</code> becomes <code class="fm-code-in-text">keras.optimizers.RMSprop()</code>. Importantly, it’s also possible to specify these arguments as object instances, like this:</p>
  <pre class="programlisting"><a id="pgfId-1027745"></a>model.compile(optimizer=keras.optimizers.RMSprop(),
<a id="pgfId-1027746"></a>              loss=keras.losses.MeanSquaredError(),
<a id="pgfId-1018692"></a>              metrics=[keras.metrics.BinaryAccuracy()])</pre>

  <p class="body"><a id="pgfId-1018698"></a>This is useful if you want to pass your own custom losses or metrics, or if you want to further configure the objects you’re using—for instance, by passing a <code class="fm-code-in-text">learning_rate</code> argument to the optimizer:</p>
  <pre class="programlisting"><a id="pgfId-1027765"></a>model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=<span class="fm-codeblue">1e-4</span>),
<a id="pgfId-1027766"></a>              loss=my_custom_loss,
<a id="pgfId-1018733"></a>              metrics=[my_custom_metric_1, my_custom_metric_2])</pre>

  <p class="body"><a id="pgfId-1018739"></a>In chapter 7, we’ll cover how to create custom losses and metrics. In general, you won’t have to create your own losses, metrics, or optimizers from scratch, because Keras offers a wide range of built-in options that is likely to include what you need:</p>

  <p class="body"><a id="pgfId-1018745"></a>Optimizers:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018751"></a><code class="fm-code-in-text">SGD</code> (with or without momentum)</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018772"></a><code class="fm-code-in-text">RMSprop</code></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018786"></a><code class="fm-code-in-text">Adam</code></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018800"></a><code class="fm-code-in-text">Adagrad</code></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018814"></a>Etc.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1018824"></a>Losses:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018830"></a><code class="fm-code-in-text">CategoricalCrossentropy</code></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018848"></a><code class="fm-code-in-text">SparseCategoricalCrossentropy</code></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018862"></a><code class="fm-code-in-text">BinaryCrossentropy</code></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018876"></a><code class="fm-code-in-text">MeanSquaredError</code></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018890"></a><code class="fm-code-in-text">KLDivergence</code></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018904"></a><code class="fm-code-in-text">CosineSimilarity</code></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018918"></a>Etc.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1018928"></a>Metrics:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018934"></a><code class="fm-code-in-text">CategoricalAccuracy</code></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018952"></a><code class="fm-code-in-text">SparseCategoricalAccuracy</code></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018966"></a><code class="fm-code-in-text">BinaryAccuracy</code></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018980"></a><code class="fm-code-in-text">AUC</code></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018994"></a><code class="fm-code-in-text">Precision</code></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1019008"></a><code class="fm-code-in-text">Recall</code></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1019022"></a>Etc.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1019032"></a>Throughout this book, you’ll see concrete applications of many of these options. <a id="marker-1019034"></a></p>

  <h3 class="fm-head1" id="heading_id_18"><a id="pgfId-1019041"></a>3.6.4 Picking a loss function</h3>

  <p class="body"><a id="pgfId-1019058"></a><a id="marker-1019052"></a><a id="marker-1019054"></a>Choosing the right loss function for the right problem is extremely important: your network will take any shortcut it can to minimize the loss, so if the objective doesn’t fully correlate with success for the task at hand, your network will end up doing things you may not have wanted. Imagine a stupid, omnipotent AI trained via SGD with this poorly chosen objective function: “maximizing the average well-being of all humans alive.” To make its job easier, this AI might choose to kill all humans except a few and focus on the well-being of the remaining ones—because average well-being isn’t affected by how many humans are left. That might not be what you intended! Just remember that all neural networks you build will be just as ruthless in lowering their loss function—so choose the objective wisely, or you’ll have to face unintended side effects.</p>

  <p class="body"><a id="pgfId-1019063"></a>Fortunately, when it comes to common problems such as classification, regression, and sequence prediction, there are simple guidelines you can follow to choose the correct loss. For instance, you’ll use binary crossentropy for a two-class classification problem, categorical crossentropy for a many-class classification problem, and so on. Only when you’re working on truly new research problems will you have to develop your own loss functions. In the next few chapters, we’ll detail explicitly which loss functions to choose for a wide range of common tasks. <a id="marker-1019065"></a><a id="marker-1019068"></a></p>

  <h3 class="fm-head1" id="heading_id_19"><a id="pgfId-1019074"></a>3.6.5 Understanding the fit() method</h3>

  <p class="body"><a id="pgfId-1019117"></a><a id="marker-1019085"></a>After <code class="fm-code-in-text">compile()</code> comes <code class="fm-code-in-text">fit()</code>. The <code class="fm-code-in-text">fit()</code> method implements the training loop itself. These are its key arguments:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1019160"></a>The <i class="fm-italics1">data</i> (inputs and targets) to train on. It will typically be passed either in the form of NumPy arrays or a TensorFlow <code class="fm-code-in-text">Dataset</code> object. You’ll learn more about the <code class="fm-code-in-text">Dataset</code> API in the next chapters.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1019169"></a>The number of <i class="fm-italics1">epochs</i> to train for: how many times the training loop should iterate over the data passed.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1019188"></a>The batch size to use within each epoch of mini-batch gradient descent: the number of training examples considered to compute the gradients for one weight update step.</p>
    </li>
  </ul>

  <p class="fm-code-listing-caption"><a id="pgfId-1019249"></a>Listing 3.23 Calling <code class="fm-code-in-text">fit()</code> with NumPy data</p>
  <pre class="programlisting"><a id="pgfId-1027785"></a>history = model.fit(
<a id="pgfId-1019301"></a>    inputs,          <span class="fm-combinumeral">❶</span>
<a id="pgfId-1019313"></a>    targets,         <span class="fm-combinumeral">❷</span>
<a id="pgfId-1019325"></a>    epochs=<span class="fm-codeblue">5</span>,        <span class="fm-combinumeral">❸</span>
<a id="pgfId-1019337"></a>    batch_size=<span class="fm-codeblue">128</span>   <span class="fm-combinumeral">❹</span>
<a id="pgfId-1019349"></a>)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1030223"></a><span class="fm-combinumeral">❶</span> The input examples, as a NumPy array</p>

  <p class="fm-code-annotation"><a id="pgfId-1030244"></a><span class="fm-combinumeral">❷</span> The corresponding training targets, as a NumPy array</p>

  <p class="fm-code-annotation"><a id="pgfId-1030261"></a><span class="fm-combinumeral">❸</span> The training loop will iterate over the data 5 times.</p>

  <p class="fm-code-annotation"><a id="pgfId-1030278"></a><span class="fm-combinumeral">❹</span> The training loop will iterate over the data in batches of 128 examples.</p>

  <p class="body"><a id="pgfId-1019471"></a>The call to <code class="fm-code-in-text">fit()</code> returns a <code class="fm-code-in-text">History</code> object. This object <a id="marker-1019444"></a>contains a <code class="fm-code-in-text">history</code> field, which is a dict mapping keys such as <code class="fm-code-in-text">"loss"</code> or specific metric names to the list of their per-epoch values. <a id="marker-1019476"></a></p>
  <pre class="programlisting"><a id="pgfId-1027840"></a>&gt;&gt;&gt; history.history
<a id="pgfId-1027841"></a>{"binary_accuracy": [0.855, 0.9565, 0.9555, 0.95, 0.951],
<a id="pgfId-1027842"></a> "loss": [0.6573270302042366,
<a id="pgfId-1027843"></a>          0.07434618508815766,
<a id="pgfId-1027844"></a>          0.07687718723714351,
<a id="pgfId-1027845"></a>          0.07412414988875389,
<a id="pgfId-1019529"></a>          0.07617757616937161]}</pre>

  <h3 class="fm-head1" id="heading_id_20"><a id="pgfId-1019535"></a>3.6.6 Monitoring loss and metrics on validation data</h3>

  <p class="body"><a id="pgfId-1019562"></a><a id="marker-1019546"></a><a id="marker-1019548"></a><a id="marker-1019550"></a><a id="marker-1019552"></a>The goal of machine learning is not to obtain models that perform well on the training data, which is easy—all you have to do is follow the gradient. The goal is to obtain models that perform well in general, and particularly on data points that the model has never encountered before. Just because a model performs well on its training data doesn’t mean it will perform well on data it has never seen! For instance, it’s possible that your model could end up merely <i class="fm-italics">memorizing</i> a mapping between your training samples and their targets, which would be useless for the task of predicting targets for data the model has never seen before. We’ll go over this point in much more detail in chapter 5.</p>

  <p class="body"><a id="pgfId-1019620"></a>To keep an eye on how the model does on new data, it’s standard practice to reserve a subset of the training data <a id="marker-1019573"></a>as <i class="fm-italics">validation data</i>: you won’t be training the model on this data, but you will use it to compute a loss value and metrics value. You do this by using the <code class="fm-code-in-text">validation_data</code> argument in <code class="fm-code-in-text">fit()</code>. Like the training data, the validation data could be passed as NumPy arrays or as a <a id="marker-1019609"></a>TensorFlow <code class="fm-code-in-text">Dataset</code> object.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1019680"></a>Listing 3.24 Using the <code class="fm-code-in-text">validation_data</code> argument</p>
  <pre class="programlisting"><a id="pgfId-1027864"></a>model = keras.Sequential([keras.layers.Dense(<span class="fm-codeblue">1</span>)])
<a id="pgfId-1027865"></a>model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=<span class="fm-codeblue">0.1</span>),
<a id="pgfId-1027866"></a>              loss=keras.losses.MeanSquaredError(),
<a id="pgfId-1027867"></a>              metrics=[keras.metrics.BinaryAccuracy()])
<a id="pgfId-1027868"></a>  
<a id="pgfId-1019750"></a>indices_permutation = np.random.permutation(len(inputs))        <span class="fm-combinumeral">❶</span>
<a id="pgfId-1019767"></a>shuffled_inputs = inputs[indices_permutation]                   <span class="fm-combinumeral">❶</span>
<a id="pgfId-1019779"></a>shuffled_targets = targets[indices_permutation]                 <span class="fm-combinumeral">❶</span>
<a id="pgfId-1019796"></a> 
<a id="pgfId-1019791"></a>num_validation_samples = int(<span class="fm-codeblue">0.3</span> * len(inputs))                 <span class="fm-combinumeral">❷</span>
<a id="pgfId-1019808"></a>val_inputs = shuffled_inputs[:num_validation_samples]           <span class="fm-combinumeral">❷</span>
<a id="pgfId-1019820"></a>val_targets = shuffled_targets[:num_validation_samples]         <span class="fm-combinumeral">❷</span>
<a id="pgfId-1019832"></a>training_inputs = shuffled_inputs[num_validation_samples:]      <span class="fm-combinumeral">❷</span>
<a id="pgfId-1019844"></a>training_targets = shuffled_targets[num_validation_samples:]    <span class="fm-combinumeral">❷</span>
<a id="pgfId-1027970"></a>model.fit(
<a id="pgfId-1019862"></a>    training_inputs,                                            <span class="fm-combinumeral">❸</span>
<a id="pgfId-1019874"></a>    training_targets,                                           <span class="fm-combinumeral">❸</span>
<a id="pgfId-1027995"></a>    epochs=<span class="fm-codeblue">5</span>,
<a id="pgfId-1027996"></a>    batch_size=<span class="fm-codeblue">16</span>,
<a id="pgfId-1019898"></a>    validation_data=(val_inputs, val_targets)                   <span class="fm-combinumeral">❹</span>
<a id="pgfId-1019910"></a>)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1029811"></a><span class="fm-combinumeral">❶</span> To avoid having samples from only one class in the validation data, shuffle the inputs and targets using a random indices permutation.</p>

  <p class="fm-code-annotation"><a id="pgfId-1029837"></a><span class="fm-combinumeral">❷</span> Reserve 30% of the training inputs and targets for validation (we’ll exclude these samples from training and reserve them to compute the validation loss and metrics).</p>

  <p class="fm-code-annotation"><a id="pgfId-1029854"></a><span class="fm-combinumeral">❸</span> Training data, used to update the weights of the model</p>

  <p class="fm-code-annotation"><a id="pgfId-1029871"></a><span class="fm-combinumeral">❹</span> Validation data, used only to monitor the validation loss and metrics</p>

  <p class="body"><a id="pgfId-1019984"></a>The value of the loss on the validation data is called the “validation loss,” to distinguish it from the “training loss.” Note that it’s essential to keep the training data and validation data strictly separate: the purpose of validation is to monitor whether what the model is learning is actually useful on new data. If any of the validation data has been seen by the model during training, your validation loss and metrics will be flawed.</p>

  <p class="body"><a id="pgfId-1020003"></a>Note that if you want to compute the validation loss and metrics after the training is complete, you can call <a id="marker-1019992"></a>the <code class="fm-code-in-text">evaluate()</code> method:</p>
  <pre class="programlisting"><a id="pgfId-1020012"></a>loss_and_metrics = model.evaluate(val_inputs, val_targets, batch_size=<span class="fm-codeblue">128</span>)</pre>

  <p class="body"><a id="pgfId-1020036"></a><code class="fm-code-in-text">evaluate()</code> will iterate in batches (of size <code class="fm-code-in-text">batch_size</code>) over the data passed and return a list of scalars, where the first entry is the validation loss and the following entries are the validation metrics. If the model has no metrics, only the validation loss is returned (rather than a list). <a id="marker-1020041"></a><a id="marker-1020044"></a><a id="marker-1020046"></a><a id="marker-1020048"></a></p>

  <h3 class="fm-head1" id="heading_id_21"><a id="pgfId-1020054"></a>3.6.7 Inference: Using a model after training</h3>

  <p class="body"><a id="pgfId-1020087"></a><a id="marker-1020065"></a><a id="marker-1020067"></a>Once you’ve trained your model, you’re going to want to use it to make predictions on new data. This is called <i class="fm-italics">inference</i>. To do this, a naive approach would simply be to <code class="fm-code-in-text">__call__()</code> the model:</p>
  <pre class="programlisting"><a id="pgfId-1020096"></a>predictions = model(new_inputs)    <span class="fm-combinumeral">❶</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1029759"></a><span class="fm-combinumeral">❶</span> Takes a NumPy array or TensorFlow tensor and returns a TensorFlow tensor</p>

  <p class="body"><a id="pgfId-1020136"></a>However, this will process all inputs in <code class="fm-code-in-text">new_inputs</code> at once, which may not be feasible if you’re looking at a lot of data (in particular, it may require more memory than your GPU has).</p>

  <p class="body"><a id="pgfId-1020183"></a>A better way to do inference is to use the <code class="fm-code-in-text">predict()</code> method. It will <a id="marker-1020162"></a>iterate over the data in small batches and return a NumPy array of predictions. And unlike <code class="fm-code-in-text">__call__()</code>, it can also process TensorFlow <code class="fm-code-in-text">Dataset</code> objects.</p>
  <pre class="programlisting"><a id="pgfId-1020192"></a>predictions = model.predict(new_inputs, batch_size=<span class="fm-codeblue">128</span>)     <span class="fm-combinumeral">❶</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1029690"></a><span class="fm-combinumeral">❶</span> Takes a NumPy array or a Dataset and returns a NumPy array</p>

  <p class="body"><a id="pgfId-1020232"></a>For instance, if we use <code class="fm-code-in-text">predict()</code> on some of our validation data with the linear model we trained earlier, we get scalar scores that correspond to the model’s prediction for each input sample:</p>
  <pre class="programlisting"><a id="pgfId-1029090"></a>&gt;&gt;&gt; predictions = model.predict(val_inputs, batch_size=<span class="fm-codeblue">128</span>)
<a id="pgfId-1028042"></a>&gt;&gt;&gt; <b class="fm-codebrown">print</b>(predictions[:<span class="fm-codeblue">10</span>])
<a id="pgfId-1028043"></a>[[0.3590725 ]
<a id="pgfId-1028044"></a> [0.82706255]
<a id="pgfId-1028045"></a> [0.74428225]
<a id="pgfId-1028046"></a> [0.682058  ]
<a id="pgfId-1028047"></a> [0.7312616 ]
<a id="pgfId-1028048"></a> [0.6059811 ]
<a id="pgfId-1028049"></a> [0.78046083]
<a id="pgfId-1028050"></a> [0.025846  ]
<a id="pgfId-1028051"></a> [0.16594526]
<a id="pgfId-1020321"></a> [0.72068727]]</pre>

  <p class="body"><a id="pgfId-1020327"></a>For now, this is all you need to know about Keras models. You are ready to move on to solving real-world machine learning problems with Keras in the next chapter. <a id="marker-1020329"></a><a id="marker-1020332"></a><a id="marker-1020334"></a></p>

  <h2 class="fm-head" id="heading_id_22"><a id="pgfId-1020340"></a>Summary</h2>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1020350"></a>TensorFlow is an industry-strength numerical computing framework that can run on CPU, GPU, or TPU. It can automatically compute the gradient of any differentiable expression, it can be distributed to many devices, and it can export programs to various external runtimes—even JavaScript.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1020364"></a>Keras is the standard API for doing deep learning with TensorFlow. It’s what we’ll use throughout this book.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1020374"></a>Key TensorFlow objects include tensors, variables, tensor operations, and the gradient tape.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1020414"></a>The central class of Keras is the <code class="fm-code-in-text">Layer</code>. A <i class="fm-italics1">layer</i> encapsulates some weights and some computation. Layers are assembled into <i class="fm-italics1">models</i>.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1020463"></a>Before you start training a model, you need to pick an <i class="fm-italics1">optimizer</i>, a <i class="fm-italics1">loss</i>, and some <i class="fm-italics1">metrics</i>, which you specify via the <code class="fm-code-in-text">model.compile()</code> method.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1020492"></a>To train a model, you can use the <code class="fm-code-in-text">fit()</code> method, which runs mini-batch gradient descent for you. You can also use it to monitor your loss and metrics on <i class="fm-italics1">validation data</i>, a set of inputs that the model doesn’t see during training.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1020501"></a>Once your model is trained, you use the <code class="fm-code-in-text">model.predict()</code> method to generate predictions on new inputs.</p>
    </li>
  </ul>
</body>
</html>
