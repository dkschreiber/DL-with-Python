<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>11</title>
    
  <link href="../../stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="../../page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
  <h1 class="tochead" id="heading_id_2"><a id="pgfId-998407"></a><a id="pgfId-1029276"></a>11 Deep learning for text</h1>

  <p class="co-summary-head"><a id="pgfId-1011754"></a>This chapter covers</p>

  <ul class="calibre10">
    <li class="co-summary-bullet"><a class="calibre11" id="pgfId-1011760"></a>Preprocessing text data for machine learning applications</li>

    <li class="co-summary-bullet"><a class="calibre11" id="pgfId-1011774"></a>Bag-of-words approaches and sequence-modeling approaches for text processing</li>

    <li class="co-summary-bullet"><a class="calibre11" id="pgfId-1011784"></a>The Transformer architecture</li>

    <li class="co-summary-bullet"><a class="calibre11" id="pgfId-1011794"></a>Sequence-to-sequence learning</li>
  </ul>

  <h2 class="fm-head" id="heading_id_3"><a id="pgfId-1011804"></a>11.1 Natural language processing: The bird’s eye view</h2>

  <p class="body"><a id="pgfId-1011827"></a><a id="marker-1011817"></a>In computer science, we refer to human languages, like English or Mandarin, as “natural” languages, to distinguish them from languages that were designed for machines, like Assembly, LISP, or XML. Every machine language was <i class="fm-italics">designed</i>: its starting point was a human engineer writing down a set of formal rules to describe what statements you could make in that language and what they meant. Rules came first, and people only started using the language once the rule set was complete. With human language, it’s the reverse: usage comes first, rules arise later. Natural language was shaped by an evolution process, much like biological organisms—that’s what makes it “natural.” Its “rules,” like the grammar of English, were formalized after the fact and are often ignored or broken by its users. As a result, while machine-readable language is highly structured and rigorous, using precise syntactic rules to weave together exactly defined concepts from a fixed vocabulary, natural language is messy—ambiguous, chaotic, sprawling, and constantly in flux.</p>

  <p class="body"><a id="pgfId-1011836"></a>Creating algorithms that can make sense of natural language is a big deal: language, and in particular text, underpins most of our communications and our cultural production. The internet is mostly text. Language is how we store almost all of our knowledge. Our very thoughts are largely built upon language. However, the ability to understand natural language has long eluded machines. Some people once naively thought that you could simply write down the “rule set of English,” much like one can write down the rule set of LISP. Early attempts to build natural language processing (NLP) systems were <a id="marker-1047452"></a>thus made through the lens of “applied linguistics.” Engineers and linguists would handcraft complex sets of rules to perform basic machine translation or create simple chatbots—like the famous ELIZA program from the 1960s, which used pattern matching to sustain very basic conversation. But language is a rebellious thing: it’s not easily pliable to formalization. After several decades of effort, the capabilities of these systems remained disappointing.</p>

  <p class="body"><a id="pgfId-1011848"></a>Handcrafted rules held out as the dominant approach well into the 1990s. But starting in the late 1980s, faster computers and greater data availability started making a better alternative viable. When you find yourself building systems that are big piles of ad hoc rules, as a clever engineer, you’re likely to start asking: “Could I use a corpus of data to automate the process of finding these rules? Could I search for the rules within some kind of rule space, instead of having to come up with them myself?” And just like that, you’ve graduated to doing machine learning. And so, in the late 1980s, we started seeing machine learning approaches to natural language processing. The earliest ones were based on decision trees—the intent was literally to automate the development of the kind of if/then/else rules of previous systems. Then statistical approaches started gaining speed, starting with logistic regression. Over time, learned parametric models fully took over, and linguistics came to be seen as more of a hindrance than a useful tool. Frederick Jelinek, an early speech recognition researcher, joked in the 1990s: “Every time I fire a linguist, the performance of the speech recognizer goes up.”</p>

  <p class="body"><a id="pgfId-1011854"></a>That’s what modern NLP is about: using machine learning and large datasets to give computers the ability not to <i class="fm-italics">understand</i> language, which is a more lofty goal, but to ingest a piece of language as input and return something useful, like predicting the following:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1011869"></a>“What’s the topic of this text?” (text classification)</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1011883"></a>“Does this text contain abuse?” (content filtering)</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1011893"></a>“Does this text sound positive or negative?” (sentiment analysis)</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1011903"></a>“What should be the next word in this incomplete sentence?” (language modeling)</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1011913"></a>“How would you say this in German?” (translation)</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1011923"></a>“How would you summarize this article in one paragraph?” (summarization)</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1011933"></a>etc.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1011943"></a>Of course, keep in mind throughout this chapter that the text-processing models you will train won’t possess a human-like understanding of language; rather, they simply look for statistical regularities in their input data, which turns out to be sufficient to perform well on many simple tasks. In much the same way that computer vision is pattern recognition applied to pixels, NLP is pattern recognition applied to words, sentences, and paragraphs.</p>

  <p class="body"><a id="pgfId-1011949"></a>The toolset of NLP—decision trees, logistic regression—only saw slow evolution from the 1990s to the early 2010s. Most of the research focus was on feature engineering. When I won my first NLP competition on Kaggle in 2013, my model was, you guessed it, based on decision trees and logistic regression. However, around 2014–2015, things started changing at last. Multiple researchers began to investigate the language-understanding capabilities of recurrent neural networks, in particular LSTM—a sequence-processing algorithm from the late 1990s that had stayed under the radar until then.</p>

  <p class="body"><a id="pgfId-1011955"></a>In early 2015, Keras made available the first open source, easy-to-use implementation of LSTM, just at the start of a massive wave of renewed interest in recurrent neural networks—until then, there had only been “research code” that couldn’t be readily reused. Then from 2015 to 2017, recurrent neural networks dominated the booming NLP scene. Bidirectional LSTM models, in particular, set the state of the art on many important tasks, from summarization to question-answering to machine translation.</p>

  <p class="body"><a id="pgfId-1011961"></a>Finally, around 2017–2018, a new architecture rose to replace RNNs: the Transformer, which you will learn about in the second half of this chapter. Transformers unlocked considerable progress across the field in a short period of time, and today most NLP systems are based on them.</p>

  <p class="body"><a id="pgfId-1011967"></a>Let’s dive into the details. This chapter will take you from the very basics to doing machine translation with a Transformer. <a id="marker-1011972"></a></p>

  <h2 class="fm-head" id="heading_id_4"><a id="pgfId-1011978"></a>11.2 Preparing text data</h2>

  <p class="body"><a id="pgfId-1012003"></a><a id="marker-1011991"></a><a id="marker-1011993"></a>Deep learning models, being differentiable functions, can only process numeric tensors: they can’t take raw text as input. <i class="fm-italics">Vectorizing</i> text is <a id="marker-1012008"></a>the process of transforming text into numeric tensors. Text vectorization processes come in many shapes and forms, but they all follow the same template (see figure 11.1):</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012018"></a>First, you <i class="fm-italics1">standardize</i> the text to make it easier to process, such as by converting it to lowercase or removing punctuation.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012058"></a>You split the text into units (called <i class="fm-italics1">tokens</i>), such as characters, words, or groups of words. This is <a class="calibre11" id="marker-1012047"></a>called <i class="fm-italics1">tokenization</i>.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012067"></a>You convert each such token into a numerical vector. This will usually involve first <i class="fm-italics1">indexing</i> all tokens present in the data.</p>
    </li>
  </ul>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/11-01.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1059433"></a>Figure 11.1 From raw text to vectors</p>

  <p class="body"><a id="pgfId-1012096"></a>Let’s review each of these steps.</p>

  <h3 class="fm-head1" id="heading_id_5"><a id="pgfId-1012116"></a>11.2.1 Text standardization</h3>

  <p class="body"><a id="pgfId-1012135"></a><a id="marker-1012127"></a><a id="marker-1012129"></a><a id="marker-1012131"></a>Consider these two sentences:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012140"></a>“sunset came. i was staring at the Mexico sky. Isnt nature splendid??”</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012154"></a>“Sunset came; I stared at the México sky. Isn’t nature splendid?”</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1012164"></a>They’re very similar—in fact, they’re almost identical. Yet, if you were to convert them to byte strings, they would end up with very different representations, because “i” and “I” are two different characters, “Mexico” and “México” are two different words, “isnt” isn’t “isn’t,” and so on. A machine learning model doesn’t know a priori that “i” and “I” are the same letter, that “é” is an “e” with an accent, or that “staring” and “stared” are two forms of the same verb.</p>

  <p class="body"><a id="pgfId-1012170"></a>Text standardization is a basic form of feature engineering that aims to erase encoding differences that you don’t want your model to have to deal with. It’s not exclusive to machine learning, either—you’d have to do the same thing if you were building a search engine.</p>

  <p class="body"><a id="pgfId-1012176"></a>One of the simplest and most widespread standardization schemes is “convert to lowercase and remove punctuation characters.” Our two sentences would become</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012182"></a>“sunset came i was staring at the mexico sky isnt nature splendid”</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012196"></a>“sunset came i stared at the méxico sky isnt nature splendid”</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1012206"></a>Much closer already. Another common transformation is to convert special characters to a standard form, such as replacing “é” with “e,” “æ” with “ae,” and so on. Our token “méxico” would then become “mexico”.</p>

  <p class="body"><a id="pgfId-1012212"></a>Lastly, a much more advanced standardization pattern that is more rarely used in a machine learning context is <i class="fm-italics">stemming</i>: converting variations of a term (such as different conjugated forms of a verb) into a single shared representation, like turning “caught” and “been catching” into “[catch]” or “cats” into “[cat]”. With stemming, “was staring” and “stared” would become something like “[stare]”, and our two similar sentences would finally end up with an identical encoding:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012227"></a>“sunset came i [stare] at the mexico sky isnt nature splendid”</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1012241"></a>With these standardization techniques, your model will require less training data and will generalize better—it won’t need abundant examples of both “Sunset” and “sunset” to learn that they mean the same thing, and it will be able to make sense of “México” even if it has only seen “mexico” in its training set. Of course, standardization may also erase some amount of information, so always keep the context in mind: for instance, if you’re writing a model that extracts questions from interview articles, it should definitely treat “?” as a separate token instead of dropping it, because it’s a useful signal for this specific task. <a id="marker-1012243"></a><a id="marker-1012246"></a><a id="marker-1012248"></a></p>

  <h3 class="fm-head1" id="heading_id_6"><a id="pgfId-1012254"></a>11.2.2 Text splitting (tokenization)</h3>

  <p class="body"><a id="pgfId-1012273"></a><a id="marker-1012265"></a><a id="marker-1012267"></a><a id="marker-1012269"></a>Once your text is standardized, you need to break it up into units to be vectorized (tokens), a step called <i class="fm-italics">tokenization</i>. You could do this in three different ways:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012278"></a><i class="fm-italics1">Word-level tokenization</i>—Where tokens <a class="calibre11" id="marker-1012295"></a>are space-separated (or punctuation-separated) substrings. A variant of this is to further split words into subwords when applicable—for instance, treating “staring” as “star+ing” or “called” as “call+ed.”</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012305"></a><i class="fm-italics1">N-gram tokenization</i>—Where tokens <a class="calibre11" id="marker-1012318"></a>are groups of <i class="fm-italics1">N</i> consecutive words. For instance, “the cat” or “he was” would be 2-gram tokens (also called bigrams).</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1012328"></a><i class="fm-italics1">Character-level tokenization</i>—Where each <a class="calibre11" id="marker-1012341"></a>character is its own token. In practice, this scheme is rarely used, and you only really see it in specialized contexts, like text generation or speech recognition.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1012364"></a>In general, you’ll always use either word-level or N-gram tokenization. There are two kinds of text-processing models: those that care about word order, called <i class="fm-italics">sequence models</i>, and those that treat input words as a set, discarding their original <a id="marker-1034803"></a>order, called <i class="fm-italics">bag-of-words models</i>. If you’re building a sequence model, you’ll use word-level tokenization, and if you’re building a bag-of-words model, you’ll use N-gram tokenization. N-grams are a way to artificially inject a small amount of local word order information into the model. Throughout this chapter, you’ll learn more about each type of model and when to use them.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre11" id="pgfId-1012373"></a>Understanding N-grams and bag-of-words</p>

    <p class="fm-sidebar-text"><a id="pgfId-1012383"></a>Word N-grams are groups of <i class="fm-italics">N</i> (or fewer) consecutive words that you can extract from a sentence. The same concept may also be applied to characters instead of words.</p>

    <p class="fm-sidebar-text"><a id="pgfId-1012389"></a>Here’s a simple example. Consider the sentence “the cat sat on the mat.” It may be decomposed into the following set of 2-grams:</p>
    <pre class="programlisting"><a id="pgfId-1043416"></a>{"the", "the cat", "cat", "cat sat", "sat",
<a id="pgfId-1012413"></a> "sat on", "on", "on the", "the mat", "mat"}</pre>

    <p class="fm-sidebar-text"><a id="pgfId-1012419"></a>It may also be decomposed into the following set of 3-grams:</p>
    <pre class="programlisting"><a id="pgfId-1043432"></a>{"the", "the cat", "cat", "cat sat", "the cat sat",
<a id="pgfId-1043433"></a> "sat", "sat on", "on", "cat sat on", "on the",
<a id="pgfId-1012449"></a> "sat on the", "the mat", "mat", "on the mat"}</pre>

    <p class="fm-sidebar-text"><a id="pgfId-1012498"></a>Such a set <a id="marker-1035930"></a>is called a <i class="fm-italics">bag-of-2-grams</i> or <i class="fm-italics">bag-of-3-grams</i>, respectively. The term “bag” here refers to the fact that you’re dealing with a set of tokens rather than a list or sequence: the tokens have no specific order. This family of tokenization methods is called <i class="fm-italics">bag-of-words</i> (or <i class="fm-italics">bag-of-N-grams</i>).</p>

    <p class="fm-sidebar-text"><a id="pgfId-1012507"></a>Because bag-of-words isn’t an order-preserving tokenization method (the tokens generated are understood as a set, not a sequence, and the general structure of the sentences is lost), it tends to be used in shallow language-processing models rather than in deep learning models. Extracting N-grams is a form of feature engineering, and deep learning sequence models do away with this manual approach, replacing it with hierarchical feature learning. One-dimensional convnets, recurrent neural networks, and Transformers are capable of learning representations for groups of words and characters without being explicitly told about the existence of such groups, by looking at continuous word or character sequences. <a id="marker-1047605"></a><a id="marker-1047606"></a><a id="marker-1047607"></a></p>
  </div>

  <h3 class="fm-head1" id="heading_id_7"><a id="pgfId-1012520"></a>11.2.3 Vocabulary indexing</h3>

  <p class="body"><a id="pgfId-1012539"></a><a id="marker-1012531"></a><a id="marker-1012533"></a><a id="marker-1012535"></a>Once your text is split into tokens, you need to encode each token into a numerical representation. You could potentially do this in a stateless way, such as by hashing each token into a fixed binary vector, but in practice, the way you’d go about it is to build an index of all terms found in the training data (the “vocabulary”), and assign a unique integer to each entry in the vocabulary.</p>

  <p class="body"><a id="pgfId-1012544"></a>Something like this:</p>
  <pre class="programlisting"><a id="pgfId-1043451"></a>vocabulary = {} 
<a id="pgfId-1043452"></a><b class="fm-codebrown">for</b> text <b class="fm-codebrown">in</b> dataset:
<a id="pgfId-1043453"></a>    text = standardize(text)
<a id="pgfId-1043454"></a>    tokens = tokenize(text)
<a id="pgfId-1043455"></a>    <b class="fm-codebrown">for</b> token <b class="fm-codebrown">in</b> tokens:
<a id="pgfId-1043456"></a>        <b class="fm-codebrown">if</b> token <b class="fm-codebrown">not</b> <b class="fm-codebrown">in</b> vocabulary:
<a id="pgfId-1012594"></a>            vocabulary[token] = len(vocabulary)</pre>

  <p class="body"><a id="pgfId-1012600"></a>You can then convert that integer into a vector encoding that can be processed by a neural network, like a one-hot vector:</p>
  <pre class="programlisting"><a id="pgfId-1043473"></a><b class="fm-codebrown">def</b> one_hot_encode_token(token):
<a id="pgfId-1043474"></a>    vector = np.zeros((len(vocabulary),))
<a id="pgfId-1043475"></a>    token_index = vocabulary[token]
<a id="pgfId-1043476"></a>    vector[token_index] = <span class="fm-codeblue">1</span> 
<a id="pgfId-1012638"></a>    <b class="fm-codebrown">return</b> vector</pre>

  <p class="body"><a id="pgfId-1012644"></a>Note that at this step it’s common to restrict the vocabulary to only the top 20,000 or 30,000 most common words found in the training data. Any text dataset tends to feature an extremely large number of unique terms, most of which only show up once or twice—indexing those rare terms would result in an excessively large feature space, where most features would have almost no information content.</p>

  <p class="body"><a id="pgfId-1012672"></a>Remember when you were training your first deep learning models on the IMDB dataset in chapters 4 and 5? The data you were using from <code class="fm-code-in-text">keras.datasets.imdb</code> was already <a id="marker-1012661"></a>preprocessed into sequences of integers, where each integer stood for a given word. Back then, we used the setting <code class="fm-code-in-text">num_words=10000</code>, in order to restrict our vocabulary to the top 10,000 most common words found in the training data.</p>

  <p class="body"><a id="pgfId-1012717"></a>Now, there’s an important detail here that we shouldn’t overlook: when we look up a new token in our vocabulary index, it may not necessarily exist. Your training data may not have contained any instance of the word “cherimoya” (or maybe you excluded it from your index because it was too rare), so doing <code class="fm-code-in-text">token_index</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">vocabulary["cherimoya"]</code> may result in a <code class="fm-code-in-text">KeyError</code>. To handle this, you should use an “out of vocabulary” index (abbreviated as <i class="fm-italics">OOV index</i>)—a catch-all for any token that wasn’t in the index. It’s usually index 1: you’re actually doing <code class="fm-code-in-text">token_index</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">vocabulary.get(token,</code> <code class="fm-code-in-text">1)</code>. When decoding a sequence of integers back into words, you’ll replace 1 with something like “[UNK]” (which you’d call an “OOV token”).</p>

  <p class="body"><a id="pgfId-1012758"></a>“Why use 1 and not 0?” you may ask. That’s because 0 is already taken. There are two special tokens that you will commonly use: the OOV token (index 1), and the <i class="fm-italics">mask token</i> (index 0). While the <a id="marker-1043506"></a>OOV token means “here was a word we did not recognize,” the mask token tells us “ignore me, I’m not a word.” You’d use it in particular to pad sequence data: because data batches need to be contiguous, all sequences in a batch of sequence data must have the same length, so shorter sequences should be padded to the length of the longest sequence. If you want to make a batch of data with the sequences <code class="fm-code-in-text">[5,</code> <code class="fm-code-in-text">7,</code> <code class="fm-code-in-text">124,</code> <code class="fm-code-in-text">4,</code> <code class="fm-code-in-text">89]</code> and <code class="fm-code-in-text">[8,</code> <code class="fm-code-in-text">34,</code> <code class="fm-code-in-text">21]</code>, it would have to look like this:</p>
  <pre class="programlisting"><a id="pgfId-1043518"></a>[[5,  7, 124, 4, 89]
<a id="pgfId-1012781"></a> [8, 34,  21, 0,  0]]</pre>

  <p class="body"><a id="pgfId-1012787"></a>The batches of integer sequences for the IMDB dataset that you worked with in chapters 4 and 5 were padded with zeros in this way. <a id="marker-1043510"></a><a id="marker-1043511"></a><a id="marker-1043512"></a></p>

  <h3 class="fm-head1" id="heading_id_8"><a id="pgfId-1012800"></a>11.2.4 Using the TextVectorization layer</h3>

  <p class="body"><a id="pgfId-1012823"></a><a id="marker-1012811"></a><a id="marker-1012813"></a><a id="marker-1012815"></a><a id="marker-1012817"></a><a id="marker-1012819"></a>Every step I’ve introduced so far would be very easy to implement in pure Python. Maybe you could write something like this:</p>
  <pre class="programlisting"><a id="pgfId-1043533"></a><b class="fm-codebrown">import</b> string
<a id="pgfId-1043534"></a>  
<a id="pgfId-1043535"></a><b class="fm-codebrown">class</b> Vectorizer:
<a id="pgfId-1043536"></a>    <b class="fm-codebrown">def</b> standardize(self, text):
<a id="pgfId-1043537"></a>        text = text.lower()
<a id="pgfId-1043538"></a>        <b class="fm-codebrown">return</b> <span class="fm-codegreen">""</span>.join(char <b class="fm-codebrown">for</b> char <b class="fm-codebrown">in</b> text 
<a id="pgfId-1047680"></a><b class="fm-codebrown">                       if</b> char <b class="fm-codebrown">not</b> <b class="fm-codebrown">in</b> string.punctuation)
<a id="pgfId-1043539"></a>  
<a id="pgfId-1043540"></a>    <b class="fm-codebrown">def</b> tokenize(self, text):
<a id="pgfId-1043541"></a>        text = self.standardize(text)
<a id="pgfId-1043542"></a>        <b class="fm-codebrown">return</b> text.split()
<a id="pgfId-1043543"></a>  
<a id="pgfId-1043544"></a>    <b class="fm-codebrown">def</b> make_vocabulary(self, dataset):
<a id="pgfId-1043545"></a>        self.vocabulary = {<span class="fm-codegreen">""</span>: <span class="fm-codeblue">0</span>, <span class="fm-codegreen">"[UNK]"</span>: <span class="fm-codeblue">1</span>}
<a id="pgfId-1043546"></a>        <b class="fm-codebrown">for</b> text <b class="fm-codebrown">in</b> dataset:
<a id="pgfId-1043547"></a>            text = self.standardize(text)
<a id="pgfId-1043548"></a>            tokens = self.tokenize(text)
<a id="pgfId-1043549"></a>            <b class="fm-codebrown">for</b> token <b class="fm-codebrown">in</b> tokens:
<a id="pgfId-1043550"></a>                <b class="fm-codebrown">if</b> token <b class="fm-codebrown">not</b> <b class="fm-codebrown">in</b> self.vocabulary:
<a id="pgfId-1043551"></a>                    self.vocabulary[token] = len(self.vocabulary)
<a id="pgfId-1043552"></a>        self.inverse_vocabulary = dict(
<a id="pgfId-1043553"></a>            (v, k) <b class="fm-codebrown">for</b> k, v <b class="fm-codebrown">in</b> self.vocabulary.items())
<a id="pgfId-1043554"></a>  
<a id="pgfId-1043555"></a>    <b class="fm-codebrown">def</b> encode(self, text):
<a id="pgfId-1043556"></a>        text = self.standardize(text)
<a id="pgfId-1043557"></a>        tokens = self.tokenize(text)
<a id="pgfId-1043558"></a>        <b class="fm-codebrown">return</b> [self.vocabulary.get(token, <span class="fm-codeblue">1</span>) <b class="fm-codebrown">for</b> token <b class="fm-codebrown">in</b> tokens]
<a id="pgfId-1043559"></a>  
<a id="pgfId-1043560"></a>    <b class="fm-codebrown">def</b> decode(self, int_sequence):
<a id="pgfId-1043561"></a>        <b class="fm-codebrown">return</b> <span class="fm-codegreen">" "</span>.join(
<a id="pgfId-1043562"></a>            self.inverse_vocabulary.get(i, <span class="fm-codegreen">"[UNK]"</span>) <b class="fm-codebrown">for</b> i <b class="fm-codebrown">in</b> int_sequence)
<a id="pgfId-1043563"></a>  
<a id="pgfId-1043564"></a>vectorizer = Vectorizer()
<a id="pgfId-1013022"></a>dataset = [           
<a id="pgfId-1043581"></a>    <span class="fm-codegreen">"I write, erase, rewrite"</span>,   <span class="fm-combinumeral">❶</span>
<a id="pgfId-1043582"></a>    <span class="fm-codegreen">"Erase again, and then"</span>,     <span class="fm-combinumeral">❶</span>
<a id="pgfId-1043583"></a>    <span class="fm-codegreen">"A poppy blooms."</span>,           <span class="fm-combinumeral">❶</span>
<a id="pgfId-1043584"></a>]
<a id="pgfId-1013058"></a>vectorizer.make_vocabulary(dataset)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1057752"></a><span class="fm-combinumeral">❶</span> Haiku by poet Hokushi</p>

  <p class="body"><a id="pgfId-1013084"></a>It does the job:</p>
  <pre class="programlisting"><a id="pgfId-1047750"></a>&gt;&gt;&gt; test_sentence = <span class="fm-codegreen">"I write, rewrite, and still rewrite again"</span> 
<a id="pgfId-1047751"></a>&gt;&gt;&gt; encoded_sentence = vectorizer.encode(test_sentence)
<a id="pgfId-1013110"></a>&gt;&gt;&gt; <b class="fm-codebrown">print</b>(encoded_sentence)
<a id="pgfId-1013116"></a>[2, 3, 5, 7, 1, 5, 6]
<a id="pgfId-1047771"></a>&gt;&gt;&gt; decoded_sentence = vectorizer.decode(encoded_sentence)
<a id="pgfId-1013128"></a>&gt;&gt;&gt; <b class="fm-codebrown">print</b>(decoded_sentence)
<a id="pgfId-1013134"></a>"i write rewrite and [UNK] rewrite again" </pre>

  <p class="body"><a id="pgfId-1013163"></a>However, using something like this wouldn’t be very performant. In practice, you’ll work with the <a id="marker-1013142"></a>Keras <code class="fm-code-in-text">TextVectorization</code> layer, which is fast and efficient and can be dropped directly into a <code class="fm-code-in-text">tf.data</code> pipeline or a Keras model.</p>

  <p class="body"><a id="pgfId-1013172"></a>This is what the <code class="fm-code-in-text">TextVectorization</code> layer looks like:</p>
  <pre class="programlisting"><a id="pgfId-1043629"></a><b class="fm-codebrown">from</b> tensorflow.keras.layers <b class="fm-codebrown">import</b> TextVectorization
<a id="pgfId-1043630"></a>text_vectorization = TextVectorization(
<a id="pgfId-1013207"></a>    output_mode=<span class="fm-codegreen">"int"</span>,                   <span class="fm-combinumeral">❶</span>
<a id="pgfId-1013219"></a>)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1057543"></a><span class="fm-combinumeral">❶</span> Configures the layer to return sequences of words encoded as integer indices. There are several other output modes available, which you will see in action in a bit.</p>

  <p class="body"><a id="pgfId-1013267"></a>By default, the <code class="fm-code-in-text">TextVectorization</code> layer will use the setting “convert to lowercase and remove punctuation” for text standardization, and “split on whitespace” for tokenization. But importantly, you can provide custom functions for standardization and tokenization, which means the layer is flexible enough to handle any use case. Note that such custom functions should operate <a id="marker-1013256"></a>on <code class="fm-code-in-text">tf.string</code> tensors, not regular Python strings! For instance, the default layer behavior is equivalent to the following:</p>
  <pre class="programlisting"><a id="pgfId-1043647"></a><b class="fm-codebrown">import</b> re 
<a id="pgfId-1043648"></a><b class="fm-codebrown">import</b> string 
<a id="pgfId-1043649"></a><b class="fm-codebrown">import</b> tensorflow <b class="fm-codebrown">as</b> tf
<a id="pgfId-1043650"></a>  
<a id="pgfId-1043651"></a><b class="fm-codebrown">def</b> custom_standardization_fn(string_tensor):
<a id="pgfId-1013313"></a>    lowercase_string = tf.strings.lower(string_tensor)              <span class="fm-combinumeral">❶</span>
<a id="pgfId-1013325"></a>    <b class="fm-codebrown">return</b> tf.strings.regex_replace(                                <span class="fm-combinumeral">❷</span>
<a id="pgfId-1043676"></a>        lowercase_string, f<span class="fm-codegreen">"[{re.escape(string.punctuation)}]"</span>, <span class="fm-codegreen">""</span>)
<a id="pgfId-1043677"></a>  
<a id="pgfId-1043678"></a><b class="fm-codebrown">def</b> custom_split_fn(string_tensor):
<a id="pgfId-1013354"></a>    <b class="fm-codebrown">return</b> tf.strings.split(string_tensor)                          <span class="fm-combinumeral">❸</span>
<a id="pgfId-1013371"></a> 
<a id="pgfId-1043691"></a>text_vectorization = TextVectorization(
<a id="pgfId-1043692"></a>    output_mode=<span class="fm-codegreen">"int"</span>,
<a id="pgfId-1043693"></a>    standardize=custom_standardization_fn,
<a id="pgfId-1043694"></a>    split=custom_split_fn,
<a id="pgfId-1013395"></a>)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1057478"></a><span class="fm-combinumeral">❶</span> Convert strings to lowercase.</p>

  <p class="fm-code-annotation"><a id="pgfId-1057506"></a><span class="fm-combinumeral">❷</span> Replace punctuation characters with the empty string.</p>

  <p class="fm-code-annotation"><a id="pgfId-1057523"></a><span class="fm-combinumeral">❸</span> Split strings on whitespace.</p>

  <p class="body"><a id="pgfId-1013475"></a>To index the vocabulary of a text corpus, just call the <code class="fm-code-in-text">adapt()</code> method of <a id="marker-1013464"></a>the layer with a <code class="fm-code-in-text">Dataset</code> object that <a id="marker-1013480"></a>yields strings, or just with a list of Python strings:</p>
  <pre class="programlisting"><a id="pgfId-1043709"></a>dataset = [
<a id="pgfId-1043710"></a>    <span class="fm-codegreen">"I write, erase, rewrite"</span>,
<a id="pgfId-1043711"></a>    <span class="fm-codegreen">"Erase again, and then"</span>,
<a id="pgfId-1043712"></a>    <span class="fm-codegreen">"A poppy blooms."</span>,
<a id="pgfId-1043713"></a>]
<a id="pgfId-1013528"></a>text_vectorization.adapt(dataset)</pre>

  <p class="body"><a id="pgfId-1013534"></a>Note that you can retrieve the computed vocabulary via <code class="fm-code-in-text">get_vocabulary()</code>—this can <a id="marker-1013545"></a>be useful if you need to convert text encoded as integer sequences back into words. The first two entries in the vocabulary are the mask token (index 0) and the OOV token (index 1). Entries in the vocabulary list are sorted by frequency, so with a real-world dataset, very common words like “the” or “a” would come first.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1013606"></a>Listing 11.1 Displaying the vocabulary</p>
  <pre class="programlisting"><a id="pgfId-1013555"></a>&gt;&gt;&gt; text_vectorization.get_vocabulary()
<a id="pgfId-1013645"></a>["", "[UNK]", "erase", "write", ...]</pre>

  <p class="body"><a id="pgfId-1013651"></a>For a demonstration, let’s try to encode and then decode an example sentence:</p>
  <pre class="programlisting"><a id="pgfId-1047810"></a>&gt;&gt;&gt; vocabulary = text_vectorization.get_vocabulary()
<a id="pgfId-1047811"></a>&gt;&gt;&gt; test_sentence = <span class="fm-codegreen">"I write, rewrite, and still rewrite again"</span> 
<a id="pgfId-1047812"></a>&gt;&gt;&gt; encoded_sentence = text_vectorization(test_sentence)
<a id="pgfId-1013683"></a>&gt;&gt;&gt; <b class="fm-codebrown">print</b>(encoded_sentence)
<a id="pgfId-1013689"></a>tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)
<a id="pgfId-1047835"></a>&gt;&gt;&gt; inverse_vocab = dict(enumerate(vocabulary))
<a id="pgfId-1047836"></a>&gt;&gt;&gt; decoded_sentence = <span class="fm-codegreen">" "</span>.join(inverse_vocab[int(i)] <b class="fm-codebrown">for</b> i <b class="fm-codebrown">in</b> encoded_sentence)
<a id="pgfId-1013707"></a>&gt;&gt;&gt; <b class="fm-codebrown">print</b>(decoded_sentence)
<a id="pgfId-1013713"></a>"i write rewrite and [UNK] rewrite again" </pre>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre11" id="pgfId-1013739"></a>Using the <code class="fm-code-in-text2">TextVectorization</code> layer in a <code class="fm-code-in-text2">tf.data</code> pipeline or as part of a model</p>

    <p class="fm-sidebar-text"><a id="pgfId-1013764"></a>Importantly, because <code class="fm-code-in-text1">TextVectorization</code> is mostly a dictionary lookup operation, it can’t be executed on a GPU (or TPU)—only on a CPU. So if you’re training your model on a GPU, your <code class="fm-code-in-text1">TextVectorization</code> layer will run on the CPU before sending its output to the GPU. This has important performance implications.</p>

    <p class="fm-sidebar-text"><a id="pgfId-1013789"></a>There are two ways we could use our <code class="fm-code-in-text1">TextVectorization</code> layer. The first option is to put it in the <code class="fm-code-in-text1">tf.data</code> pipeline, like this:</p>
    <pre class="programlisting"><a id="pgfId-1047865"></a>int_sequence_dataset = string_dataset.map(   <span class="fm-combinumeral">❶</span>
<a id="pgfId-1047866"></a>    text_vectorization,
<a id="pgfId-1013798"></a>    num_parallel_calls=<span class="fm-codeblue">4</span>)                    <span class="fm-combinumeral">❷</span></pre>

    <p class="fm-code-annotation"><a id="pgfId-1057204"></a><span class="fm-combinumeral">❶</span> string_dataset would be a dataset that yields string tensors.</p>

    <p class="fm-code-annotation"><a id="pgfId-1057221"></a><span class="fm-combinumeral">❷</span> The num_parallel_calls argument is used to parallelize the map() call across multiple CPU cores.</p>

    <p class="fm-sidebar-text"><a id="pgfId-1013842"></a>The second option is to make it part of the model (after all, it’s a Keras layer), like this:</p>
    <pre class="programlisting"><a id="pgfId-1013848"></a>text_input = keras.Input(shape=(), dtype=<span class="fm-codegreen">"string"</span>)             <span class="fm-combinumeral">❶</span>
<a id="pgfId-1013872"></a>vectorized_text = text_vectorization(text_input)               <span class="fm-combinumeral">❷</span>
<a id="pgfId-1013884"></a>embedded_input = keras.layers.Embedding(...)(vectorized_text)  <span class="fm-combinumeral">❸</span>
<a id="pgfId-1013896"></a>output = ...                                                   <span class="fm-combinumeral">❸</span>
<a id="pgfId-1013908"></a>model = keras.Model(text_input, output)                        <span class="fm-combinumeral">❸</span></pre>

    <p class="fm-code-annotation"><a id="pgfId-1037758"></a><span class="fm-combinumeral">❶</span> Create a symbolic input that expects strings.</p>

    <p class="fm-code-annotation"><a id="pgfId-1057154"></a><span class="fm-combinumeral">❷</span> Apply the text vectorization layer to it.</p>

    <p class="fm-code-annotation"><a id="pgfId-1057178"></a><span class="fm-combinumeral">❸</span> You can keep chaining new layers on top—just your regular Functional API model.</p>

    <p class="fm-sidebar-text"><a id="pgfId-1059476"></a>There’s an important difference between the two: if the vectorization step is part of the model, it will happen synchronously with the rest of the model. This means that at each training step, the rest of the model (placed on the GPU) will have to wait for the output of the <code class="fm-code-in-text1">TextVectorization</code> layer (placed on the CPU) to be ready in order to get to work. Meanwhile, putting the layer in the <code class="fm-code-in-text1">tf.data</code> pipeline enables you to</p>

    <p class="fm-sidebar-text"><a id="pgfId-1059477"></a> do asynchronous preprocessing of your data on CPU: while the GPU runs the model on one batch of vectorized data, the CPU stays busy by vectorizing the next batch of raw strings.</p>

    <p class="fm-sidebar-text"><a id="pgfId-1059478"></a>So if you’re training the model on GPU or TPU, you’ll probably want to go with the first option to get the best performance. This is what we will do in all practical examples throughout this chapter. When training on a CPU, though, synchronous processing is fine: you will get 100% utilization of your cores regardless of which option you go with.</p>

    <p class="fm-sidebar-text"><a id="pgfId-1059479"></a>Now, if you were to export our model to a production environment, you would want to ship a model that accepts raw strings as input, like in the code snippet for the second option above—otherwise you would have to reimplement text standardization and tokenization in your production environment (maybe in JavaScript?), and you would face the risk of introducing small preprocessing discrepancies that would hurt the model’s accuracy. Thankfully, the <code class="fm-code-in-text1">TextVectorization</code> layer enables you to include text preprocessing right into your model, making it easier to deploy—even if you were originally using the layer as part of a <code class="fm-code-in-text1">tf.data</code> pipeline. In the sidebar “Exporting a model that processes raw strings,” you’ll learn how to export an inference-only trained model that incorporates text preprocessing.</p>
  </div>

  <p class="body"><a id="pgfId-1014028"></a>You’ve now learned everything you need to know about text preprocessing—let’s move on to the modeling stage. <a id="marker-1014030"></a><a id="marker-1014033"></a><a id="marker-1014035"></a><a id="marker-1014037"></a><a id="marker-1014039"></a><a id="marker-1014043"></a><a id="marker-1014045"></a></p>

  <h2 class="fm-head" id="heading_id_9"><a id="pgfId-1014051"></a>11.3 Two approaches for representing groups of words: Sets and sequences</h2>

  <p class="body"><a id="pgfId-1014077"></a>How a machine learning model should represent <i class="fm-italics">individual words</i> is a relatively uncontroversial question: they’re categorical features (values from a predefined set), and we know how to handle those. They should be encoded as dimensions in a feature space, or as category vectors (word vectors in this case). A much more problematic question, however, is how to encode <i class="fm-italics">the way words are woven into sentences</i>: word order.</p>

  <p class="body"><a id="pgfId-1014086"></a>The problem of order in natural language is an interesting one: unlike the steps of a timeseries, words in a sentence don’t have a natural, canonical order. Different languages order similar words in very different ways. For instance, the sentence structure of English is quite different from that of Japanese. Even within a given language, you can typically say the same thing in different ways by reshuffling the words a bit. Even further, if you fully randomize the words in a short sentence, you can still largely figure out what it was saying—though in many cases significant ambiguity seems to arise. Order is clearly important, but its relationship to meaning isn’t straightforward.</p>

  <p class="body"><a id="pgfId-1014114"></a>How to represent word order is the pivotal question from which different kinds of NLP architectures spring. The simplest thing you could do is just discard order and treat text as an unordered set of words—this gives you <i class="fm-italics">bag-of-words models</i>. You could also decide that words should be processed strictly in the order in which they appear, one at a time, like steps in a timeseries—you could then leverage the recurrent models from the last chapter. Finally, a hybrid approach is also possible: the Transformer architecture is technically order-agnostic, yet it injects word-position information into the representations it processes, which enables it to simultaneously look at different parts of a sentence (unlike RNNs) while still being order-aware. Because they take into account word order, both RNNs and Transformers are <a id="marker-1014103"></a>called <i class="fm-italics">sequence models</i>.</p>

  <p class="body"><a id="pgfId-1014123"></a>Historically, most early applications of machine learning to NLP just involved bag-of-words models. Interest in sequence models only started rising in 2015, with the rebirth of recurrent neural networks. Today, both approaches remain relevant. Let’s see how they work, and when to leverage which.</p>

  <p class="body"><a id="pgfId-1014129"></a>We’ll demonstrate each approach on a well-known text classification benchmark: the IMDB movie review sentiment-classification dataset. In chapters 4 and 5, you worked with a prevectorized version of the IMDB dataset; now, let’s process the raw IMDB text data, just like you would do when approaching a new text-classification problem in the real world.</p>

  <h3 class="fm-head1" id="heading_id_10"><a id="pgfId-1014135"></a>11.3.1 Preparing the IMDB movie reviews data</h3>

  <p class="body"><a id="pgfId-1014154"></a><a id="marker-1014148"></a><a id="marker-1014150"></a>Let’s start by downloading the dataset from the Stanford page of Andrew Maas and uncompressing it:</p>
  <pre class="programlisting"><a id="pgfId-1014159"></a>!curl -O https:/ /ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz
<a id="pgfId-1014173"></a>!tar -xf aclImdb_v1.tar.gz</pre>

  <p class="body"><a id="pgfId-1014179"></a>You’re left with a directory named aclImdb, with the following structure:</p>
  <pre class="programlisting"><a id="pgfId-1014185"></a>aclImdb/
<a id="pgfId-1014199"></a>...train/
<a id="pgfId-1014205"></a>......pos/
<a id="pgfId-1014211"></a>......neg/
<a id="pgfId-1014217"></a>...test/
<a id="pgfId-1014223"></a>......pos/
<a id="pgfId-1014229"></a>......neg/</pre>

  <p class="body"><a id="pgfId-1014235"></a>For instance, the train/pos/ directory contains a set of 12,500 text files, each of which contains the text body of a positive-sentiment movie review to be used as training data. The negative-sentiment reviews live in the “neg” directories. In total, there are 25,000 text files for training and another 25,000 for testing.</p>

  <p class="body"><a id="pgfId-1014241"></a>There’s also a train/unsup subdirectory in there, which we don’t need. Let’s delete it:</p>
  <pre class="programlisting"><a id="pgfId-1014247"></a>!rm -r aclImdb/train/unsup</pre>

  <p class="body"><a id="pgfId-1014261"></a>Take a look at the content of a few of these text files. Whether you’re working with text data or image data, remember to always inspect what your data looks like before you dive into modeling it. It will ground your intuition about what your model is actually doing:</p>
  <pre class="programlisting"><a id="pgfId-1014267"></a>!cat aclImdb/train/pos/4077_10.txt</pre>

  <p class="body"><a id="pgfId-1014281"></a>Next, let’s prepare a validation set by setting apart 20% of the training text files in a new directory, aclImdb/val:</p>
  <pre class="programlisting"><a id="pgfId-1043849"></a><b class="fm-codebrown">import</b> os, pathlib, shutil, random
<a id="pgfId-1043850"></a>  
<a id="pgfId-1043851"></a>base_dir = pathlib.Path(<span class="fm-codegreen">"aclImdb"</span>)
<a id="pgfId-1043852"></a>val_dir = base_dir / <span class="fm-codegreen">"val"</span> 
<a id="pgfId-1043853"></a>train_dir = base_dir / <span class="fm-codegreen">"train"</span> 
<a id="pgfId-1043854"></a><b class="fm-codebrown">for</b> category <b class="fm-codebrown">in</b> (<span class="fm-codegreen">"neg"</span>, <span class="fm-codegreen">"pos"</span>):
<a id="pgfId-1043855"></a>    os.makedirs(val_dir / category)
<a id="pgfId-1043856"></a>    files = os.listdir(train_dir / category)
<a id="pgfId-1014342"></a>    random.Random(<span class="fm-codeblue">1337</span>).shuffle(files)              <span class="fm-combinumeral">❶</span>
<a id="pgfId-1014354"></a>    num_val_samples = int(<span class="fm-codeblue">0.2</span> * len(files))         <span class="fm-combinumeral">❷</span>
<a id="pgfId-1014366"></a>    val_files = files[-num_val_samples:]            <span class="fm-combinumeral">❷</span>
<a id="pgfId-1014378"></a>    <b class="fm-codebrown">for</b> fname <b class="fm-codebrown">in</b> val_files:                         <span class="fm-combinumeral">❸</span>
<a id="pgfId-1014390"></a>        shutil.move(train_dir / category / fname,   <span class="fm-combinumeral">❸</span>
<a id="pgfId-1014402"></a>                    val_dir / category / fname)     <span class="fm-combinumeral">❸</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1056923"></a><span class="fm-combinumeral">❶</span> Shuffle the list of training files using a seed, to ensure we get the same validation set every time we run the code.</p>

  <p class="fm-code-annotation"><a id="pgfId-1056944"></a><span class="fm-combinumeral">❷</span> Take 20% of the training files to use for validation.</p>

  <p class="fm-code-annotation"><a id="pgfId-1056961"></a><span class="fm-combinumeral">❸</span> Move the files to aclImdb/val/neg and aclImdb/val/pos.</p>

  <p class="body"><a id="pgfId-1014514"></a>Remember how, in chapter 8, we used the <code class="fm-code-in-text">image_dataset_from_directory</code> utility to <a id="marker-1037922"></a>create a batched <code class="fm-code-in-text">Dataset</code> of images and their labels for a directory structure? You can do the exact same thing for text files using <a id="marker-1037923"></a>the <code class="fm-code-in-text">text_dataset_from_directory</code> utility. Let’s create three <code class="fm-code-in-text">Dataset</code> objects for training, validation, and testing:</p>
  <pre class="programlisting"><a id="pgfId-1043933"></a><b class="fm-codebrown">from</b> tensorflow <b class="fm-codebrown">import</b> keras
<a id="pgfId-1043934"></a>batch_size = <span class="fm-codeblue">32</span> 
<a id="pgfId-1043935"></a>  
<a id="pgfId-1014543"></a>train_ds = keras.utils.text_dataset_from_directory(     <span class="fm-combinumeral">❶</span>
<a id="pgfId-1043948"></a>    <span class="fm-codegreen">"aclImdb/train"</span>, batch_size=batch_size
<a id="pgfId-1043949"></a>)
<a id="pgfId-1043950"></a>val_ds = keras.utils.text_dataset_from_directory(
<a id="pgfId-1043951"></a>    <span class="fm-codegreen">"aclImdb/val"</span>, batch_size=batch_size
<a id="pgfId-1043952"></a>)
<a id="pgfId-1043953"></a>test_ds = keras.utils.text_dataset_from_directory(
<a id="pgfId-1043954"></a>    <span class="fm-codegreen">"aclImdb/test"</span>, batch_size=batch_size
<a id="pgfId-1014602"></a>)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1056867"></a><span class="fm-combinumeral">❶</span> Running this line should output “Found 20000 files belonging to 2 classes”; if you see “Found 70000 files belonging to 3 classes,” it means you forgot to delete the aclImdb/train/unsup directory.</p>

  <p class="body"><a id="pgfId-1014644"></a>These datasets yield inputs that are TensorFlow <code class="fm-code-in-text">tf.string</code> tensors and targets that are <code class="fm-code-in-text">int32</code> tensors encoding the value “0” or “1.”</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1014704"></a>Listing 11.2 Displaying the shapes and dtypes of the first batch</p>
  <pre class="programlisting"><a id="pgfId-1048051"></a>&gt;&gt;&gt; <b class="fm-codebrown">for</b> inputs, targets <b class="fm-codebrown">in</b> train_ds:
<a id="pgfId-1048052"></a>&gt;&gt;&gt;     <b class="fm-codebrown">print</b>(<span class="fm-codegreen">"inputs.shape:"</span>, inputs.shape)
<a id="pgfId-1048053"></a>&gt;&gt;&gt;     <b class="fm-codebrown">print</b>(<span class="fm-codegreen">"inputs.dtype:"</span>, inputs.dtype)
<a id="pgfId-1048054"></a>&gt;&gt;&gt;     <b class="fm-codebrown">print</b>(<span class="fm-codegreen">"targets.shape:"</span>, targets.shape)
<a id="pgfId-1048055"></a>&gt;&gt;&gt;     <b class="fm-codebrown">print</b>(<span class="fm-codegreen">"targets.dtype:"</span>, targets.dtype)
<a id="pgfId-1048056"></a>&gt;&gt;&gt;     <b class="fm-codebrown">print</b>(<span class="fm-codegreen">"inputs[0]:"</span>, inputs[<span class="fm-codeblue">0</span>])
<a id="pgfId-1048057"></a>&gt;&gt;&gt;     <b class="fm-codebrown">print</b>(<span class="fm-codegreen">"targets[0]:"</span>, targets[<span class="fm-codeblue">0</span>])
<a id="pgfId-1014779"></a>&gt;&gt;&gt;     <b class="fm-codebrown">break</b>
<a id="pgfId-1047257"></a>inputs.shape: (32,)
<a id="pgfId-1043969"></a>inputs.dtype: &lt;dtype: "string"&gt;
<a id="pgfId-1043970"></a>targets.shape: (32,)
<a id="pgfId-1043971"></a>targets.dtype: &lt;dtype: "int32"&gt;
<a id="pgfId-1043972"></a>inputs[0]: tf.Tensor(b"This string contains the movie review.", shape=(), dtype=string)
<a id="pgfId-1014815"></a>targets[0]: tf.Tensor(1, shape=(), dtype=int32)</pre>

  <p class="body"><a id="pgfId-1014821"></a>All set. Now let’s try learning something from this data. <a id="marker-1014826"></a><a id="marker-1014828"></a></p>

  <h3 class="fm-head1" id="heading_id_11"><a id="pgfId-1014834"></a>11.3.2 Processing words as a set: The bag-of-words approach</h3>

  <p class="body"><a id="pgfId-1014853"></a><a id="marker-1014847"></a><a id="marker-1014849"></a>The simplest way to encode a piece of text for processing by a machine learning model is to discard order and treat it as a set (a “bag”) of tokens. You could either look at individual words (unigrams), or try to recover some local order information by looking at groups of consecutive token (N-grams).</p>

  <p class="fm-head2"><a id="pgfId-1014858"></a>Single words (unigrams) with binary encoding</p>

  <p class="body"><a id="pgfId-1014879"></a><a id="marker-1014869"></a><a id="marker-1014871"></a><a id="marker-1014873"></a><a id="marker-1014875"></a>If you use a bag of single words, the sentence “the cat sat on the mat” becomes</p>
  <pre class="programlisting"><a id="pgfId-1014884"></a>{"cat", "mat", "on", "sat", "the"}</pre>

  <p class="body"><a id="pgfId-1014898"></a>The main advantage of this encoding is that you can represent an entire text as a single vector, where each entry is a presence indicator for a given word. For instance, using binary encoding (multi-hot), you’d encode a text as a vector with as many dimensions as there are words in your vocabulary—with 0s almost everywhere and some 1s for dimensions that encode words present in the text. This is what we did when we worked with text data in chapters 4 and 5. Let’s try this on our task.</p>

  <p class="body"><a id="pgfId-1014920"></a>First, let’s process our raw text datasets with a <code class="fm-code-in-text">TextVectorization</code> layer so that they yield multi-hot encoded binary word vectors. Our layer will only look at single words (that is to say, <i class="fm-italics">unigrams</i>).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1014980"></a>Listing 11.3 Preprocessing our datasets with a <code class="fm-code-in-text">TextVectorization</code> layer</p>
  <pre class="programlisting"><a id="pgfId-1044009"></a>text_vectorization = TextVectorization(
<a id="pgfId-1015032"></a>    max_tokens=<span class="fm-codeblue">20000</span>,                               <span class="fm-combinumeral">❶</span>
<a id="pgfId-1015044"></a>    output_mode=<span class="fm-codegreen">"multi_hot"</span>,                        <span class="fm-combinumeral">❷</span>
<a id="pgfId-1015056"></a>)
<a id="pgfId-1015062"></a>text_only_train_ds = train_ds.map(<b class="fm-codebrown">lambda</b> x, y: x)   <span class="fm-combinumeral">❸</span>
<a id="pgfId-1015074"></a>text_vectorization.adapt(text_only_train_ds)        <span class="fm-combinumeral">❹</span>
<a id="pgfId-1048098"></a> 
<a id="pgfId-1048109"></a>binary_1gram_train_ds = train_ds.map(               <span class="fm-combinumeral">❺</span>
<a id="pgfId-1048110"></a>    <b class="fm-codebrown">lambda</b> x, y: (text_vectorization(x), y),        <span class="fm-combinumeral">❺</span>
<a id="pgfId-1048111"></a>    num_parallel_calls=<span class="fm-codeblue">4</span>)                           <span class="fm-combinumeral">❺</span>
<a id="pgfId-1048112"></a>binary_1gram_val_ds = val_ds.map(                   <span class="fm-combinumeral">❺</span>
<a id="pgfId-1048113"></a>    <b class="fm-codebrown">lambda</b> x, y: (text_vectorization(x), y),        <span class="fm-combinumeral">❺</span>
<a id="pgfId-1048114"></a>    num_parallel_calls=<span class="fm-codeblue">4</span>)                           <span class="fm-combinumeral">❺</span>
<a id="pgfId-1048115"></a>binary_1gram_test_ds = test_ds.map(                 <span class="fm-combinumeral">❺</span>
<a id="pgfId-1048116"></a>    <b class="fm-codebrown">lambda</b> x, y: (text_vectorization(x), y),        <span class="fm-combinumeral">❺</span>
<a id="pgfId-1048117"></a>    num_parallel_calls=<span class="fm-codeblue">4</span>)                           <span class="fm-combinumeral">❺</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1056454"></a><span class="fm-combinumeral">❶</span> Limit the vocabulary to the 20,000 most frequent words. Otherwise we’d be indexing every word in the training data—potentially tens of thousands of terms that only occur once or twice and thus aren’t informative. In general, 20,000 is the right vocabulary size for text classification.</p>

  <p class="fm-code-annotation"><a id="pgfId-1056475"></a><span class="fm-combinumeral">❷</span> Encode the output tokens as multi-hot binary vectors.</p>

  <p class="fm-code-annotation"><a id="pgfId-1056492"></a><span class="fm-combinumeral">❸</span> Prepare a dataset that only yields raw text inputs (no labels).</p>

  <p class="fm-code-annotation"><a id="pgfId-1056509"></a><span class="fm-combinumeral">❹</span> Use that dataset to index the dataset vocabulary via the adapt() method.</p>

  <p class="fm-code-annotation"><a id="pgfId-1048307"></a><span class="fm-combinumeral">❺</span> Prepare processed versions of our training, validation, and test dataset. Make sure to specify num_parallel_calls to leverage multiple CPU cores.</p>

  <p class="body"><a id="pgfId-1015220"></a>You can try to inspect the output of one of these datasets.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1015277"></a>Listing 11.4 Inspecting the output of our binary unigram dataset</p>
  <pre class="programlisting"><a id="pgfId-1048347"></a>&gt;&gt;&gt; <b class="fm-codebrown">for</b> inputs, targets <b class="fm-codebrown">in</b> binary_1gram_train_ds:
<a id="pgfId-1048348"></a>&gt;&gt;&gt;     <b class="fm-codebrown">print</b>(<span class="fm-codegreen">"inputs.shape:"</span>, inputs.shape)
<a id="pgfId-1048349"></a>&gt;&gt;&gt;     <b class="fm-codebrown">print</b>(<span class="fm-codegreen">"inputs.dtype:"</span>, inputs.dtype)
<a id="pgfId-1048350"></a>&gt;&gt;&gt;     <b class="fm-codebrown">print</b>(<span class="fm-codegreen">"targets.shape:"</span>, targets.shape)
<a id="pgfId-1048351"></a>&gt;&gt;&gt;     <b class="fm-codebrown">print</b>(<span class="fm-codegreen">"targets.dtype:"</span>, targets.dtype)
<a id="pgfId-1048352"></a>&gt;&gt;&gt;     <b class="fm-codebrown">print</b>(<span class="fm-codegreen">"inputs[0]:"</span>, inputs[<span class="fm-codeblue">0</span>])
<a id="pgfId-1048353"></a>&gt;&gt;&gt;     <b class="fm-codebrown">print</b>(<span class="fm-codegreen">"targets[0]:"</span>, targets[<span class="fm-codeblue">0</span>])
<a id="pgfId-1015352"></a>&gt;&gt;&gt;     <b class="fm-codebrown">break</b>
<a id="pgfId-1015358"></a>inputs.shape: (32, 20000)                                                   <span class="fm-combinumeral">❶</span>
<a id="pgfId-1044181"></a>inputs.dtype: &lt;dtype: "float32"&gt;
<a id="pgfId-1044182"></a>targets.shape: (32,)
<a id="pgfId-1044183"></a>targets.dtype: &lt;dtype: "int32"&gt;
<a id="pgfId-1015388"></a>inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)<span class="fm-combinumeral">❷</span>
<a id="pgfId-1015400"></a>targets[0]: tf.Tensor(1, shape=(), dtype=int32)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1056301"></a><span class="fm-combinumeral">❶</span> Inputs are batches of 20,000-dimensional vectors.</p>

  <p class="fm-code-annotation"><a id="pgfId-1056322"></a><span class="fm-combinumeral">❷</span> These vectors consist entirely of ones and zeros.</p>

  <p class="body"><a id="pgfId-1015442"></a>Next, let’s write a reusable model-building function that we’ll use in all of our experiments in this section.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1015499"></a>Listing 11.5 Our model-building utility</p>
  <pre class="programlisting"><a id="pgfId-1044238"></a><b class="fm-codebrown">from</b> tensorflow <b class="fm-codebrown">import</b> keras 
<a id="pgfId-1044239"></a><b class="fm-codebrown">from</b> tensorflow.keras <b class="fm-codebrown">import</b> layers
<a id="pgfId-1044240"></a>  
<a id="pgfId-1044241"></a><b class="fm-codebrown">def</b> get_model(max_tokens=<span class="fm-codeblue">20000</span>, hidden_dim=<span class="fm-codeblue">16</span>):
<a id="pgfId-1044242"></a>    inputs = keras.Input(shape=(max_tokens,))
<a id="pgfId-1044243"></a>    x = layers.Dense(hidden_dim, activation=<span class="fm-codegreen">"relu"</span>)(inputs)
<a id="pgfId-1044244"></a>    x = layers.Dropout(<span class="fm-codeblue">0.5</span>)(x)
<a id="pgfId-1044245"></a>    outputs = layers.Dense(<span class="fm-codeblue">1</span>, activation=<span class="fm-codegreen">"sigmoid"</span>)(x)
<a id="pgfId-1044246"></a>    model = keras.Model(inputs, outputs)
<a id="pgfId-1044247"></a>    model.compile(optimizer=<span class="fm-codegreen">"rmsprop"</span>,
<a id="pgfId-1044248"></a>                  loss=<span class="fm-codegreen">"binary_crossentropy"</span>,
<a id="pgfId-1044249"></a>                  metrics=[<span class="fm-codegreen">"accuracy"</span>])
<a id="pgfId-1015603"></a>    <b class="fm-codebrown">return</b> model</pre>

  <p class="body"><a id="pgfId-1015609"></a>Finally, let’s train and test our model.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1015666"></a>Listing 11.6 Training and testing the binary unigram model</p>
  <pre class="programlisting"><a id="pgfId-1044268"></a>model = get_model()
<a id="pgfId-1044269"></a>model.summary()
<a id="pgfId-1044270"></a>callbacks = [
<a id="pgfId-1044271"></a>    keras.callbacks.ModelCheckpoint(<span class="fm-codegreen">"binary_1gram.keras"</span>,
<a id="pgfId-1044272"></a>                                    save_best_only=<code class="fm-codegreen">True</code>)
<a id="pgfId-1044273"></a>]
<a id="pgfId-1015735"></a>model.fit(binary_1gram_train_ds.cache(),                   <span class="fm-combinumeral">❶</span>
<a id="pgfId-1015747"></a>          validation_data=binary_1gram_val_ds.cache(),     <span class="fm-combinumeral">❶</span>
<a id="pgfId-1044290"></a>          epochs=<span class="fm-codeblue">10</span>,
<a id="pgfId-1044291"></a>          callbacks=callbacks)
<a id="pgfId-1044292"></a>model = keras.models.load_model(<span class="fm-codegreen">"binary_1gram.keras"</span>) 
<a id="pgfId-1015777"></a><b class="fm-codebrown">print</b>(f<span class="fm-codegreen">"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}"</span>)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1056235"></a><span class="fm-combinumeral">❶</span> We call cache() on the datasets to cache them in memory: this way, we will only do the preprocessing once, during the first epoch, and we’ll reuse the preprocessed texts for the following epochs. This can only be done if the data is small enough to fit in memory.</p>

  <p class="body"><a id="pgfId-1015803"></a>This gets us to a test accuracy of 89.2%: not bad! Note that in this case, since the dataset is a balanced two-class classification dataset (there are as many positive samples as negative samples), the “naive baseline” we could reach without training an actual model would only be 50%. Meanwhile, the best score that can be achieved on this dataset without leveraging external data is around 95% test accuracy. <a id="marker-1015805"></a><a id="marker-1015808"></a><a id="marker-1015810"></a><a id="marker-1015812"></a></p>

  <p class="fm-head2"><a id="pgfId-1015818"></a>Bigrams with binary encoding</p>

  <p class="body"><a id="pgfId-1015839"></a><a id="marker-1015829"></a><a id="marker-1015831"></a><a id="marker-1015833"></a><a id="marker-1015835"></a>Of course, discarding word order is very reductive, because even atomic concepts can be expressed via multiple words: the term “United States” conveys a concept that is quite distinct from the meaning of the words “states” and “united” taken separately. For this reason, you will usually end up re-injecting local order information into your bag-of-words representation by looking at N-grams rather than single words (most commonly, bigrams).</p>

  <p class="body"><a id="pgfId-1015844"></a>With bigrams, our sentence becomes</p>
  <pre class="programlisting"><a id="pgfId-1044307"></a>{"the", "the cat", "cat", "cat sat", "sat",
<a id="pgfId-1015864"></a> "sat on", "on", "on the", "the mat", "mat"}</pre>

  <p class="body"><a id="pgfId-1015886"></a>The <code class="fm-code-in-text">TextVectorization</code> layer can be configured to return arbitrary N-grams: bigrams, trigrams, etc. Just pass an <code class="fm-code-in-text">ngrams=N</code> argument as in the following listing.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1015946"></a>Listing 11.7 Configuring the <code class="fm-code-in-text">TextVectorization</code> layer to return bigrams</p>
  <pre class="programlisting"><a id="pgfId-1044324"></a>text_vectorization = TextVectorization(
<a id="pgfId-1044325"></a>    ngrams=<span class="fm-codeblue">2</span>,
<a id="pgfId-1044326"></a>    max_tokens=<span class="fm-codeblue">20000</span>,
<a id="pgfId-1044327"></a>    output_mode=<span class="fm-codegreen">"multi_hot"</span>,
<a id="pgfId-1016016"></a>)</pre>

  <p class="body"><a id="pgfId-1016022"></a>Let’s test how our model performs when trained on such binary-encoded bags of bigrams.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1016079"></a>Listing 11.8 Training and testing the binary bigram model</p>
  <pre class="programlisting"><a id="pgfId-1044340"></a>text_vectorization.adapt(text_only_train_ds)
<a id="pgfId-1048385"></a>binary_2gram_train_ds = train_ds.map(
<a id="pgfId-1048386"></a>    <b class="fm-codebrown">lambda</b> x, y: (text_vectorization(x), y),
<a id="pgfId-1048387"></a>    num_parallel_calls=<span class="fm-codeblue">4</span>)
<a id="pgfId-1048388"></a>binary_2gram_val_ds = val_ds.map(
<a id="pgfId-1048389"></a>    <b class="fm-codebrown">lambda</b> x, y: (text_vectorization(x), y),
<a id="pgfId-1048390"></a>    num_parallel_calls=<span class="fm-codeblue">4</span>)
<a id="pgfId-1048391"></a>binary_2gram_test_ds = test_ds.map(
<a id="pgfId-1048392"></a>    <b class="fm-codebrown">lambda</b> x, y: (text_vectorization(x), y),
<a id="pgfId-1048374"></a>    num_parallel_calls=<span class="fm-codeblue">4</span>)
<a id="pgfId-1044344"></a> 
<a id="pgfId-1044345"></a>model = get_model()
<a id="pgfId-1044346"></a>model.summary()
<a id="pgfId-1044347"></a>callbacks = [
<a id="pgfId-1044348"></a>    keras.callbacks.ModelCheckpoint(<span class="fm-codegreen">"binary_2gram.keras"</span>,
<a id="pgfId-1044349"></a>                                    save_best_only=<code class="fm-codegreen">True</code>)
<a id="pgfId-1044350"></a>]
<a id="pgfId-1044351"></a>model.fit(binary_2gram_train_ds.cache(),
<a id="pgfId-1044352"></a>          validation_data=binary_2gram_val_ds.cache(),
<a id="pgfId-1044353"></a>          epochs=<span class="fm-codeblue">10</span>,
<a id="pgfId-1044354"></a>          callbacks=callbacks)
<a id="pgfId-1044355"></a>model = keras.models.load_model(<span class="fm-codegreen">"binary_2gram.keras"</span>)
<a id="pgfId-1016207"></a><b class="fm-codebrown">print</b>(f<span class="fm-codegreen">"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}"</span>)</pre>

  <p class="body"><a id="pgfId-1016213"></a>We’re now getting 90.4% test accuracy, a marked improvement! Turns out local order is pretty important. <a id="marker-1016215"></a><a id="marker-1016218"></a><a id="marker-1016220"></a><a id="marker-1016222"></a></p>

  <p class="fm-head2"><a id="pgfId-1016228"></a>Bigrams with TF-IDF encoding</p>

  <p class="body"><a id="pgfId-1016249"></a><a id="marker-1016239"></a><a id="marker-1016241"></a><a id="marker-1016243"></a><a id="marker-1016245"></a>You can also add a bit more information to this representation by counting how many times each word or N-gram occurs, that is to say, by taking the histogram of the words over the text:</p>
  <pre class="programlisting"><a id="pgfId-1044383"></a>{"the": 2, "the cat": 1, "cat": 1, "cat sat": 1, "sat": 1,
<a id="pgfId-1016268"></a> "sat on": 1, "on": 1, "on the": 1, "the mat: 1", "mat": 1}</pre>

  <p class="body"><a id="pgfId-1016274"></a>If you’re doing text classification, knowing how many times a word occurs in a sample is critical: any sufficiently long movie review may contain the word “terrible” regardless of sentiment, but a review that contains many instances of the word “terrible” is likely a negative one.</p>

  <p class="body"><a id="pgfId-1016280"></a>Here’s how you’d count bigram occurrences with the <code class="fm-code-in-text">TextVectorization</code> layer.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1016346"></a>Listing 11.9 Configuring the <code class="fm-code-in-text">TextVectorization</code> layer to return token counts</p>
  <pre class="programlisting"><a id="pgfId-1044433"></a>text_vectorization = TextVectorization(
<a id="pgfId-1044434"></a>    ngrams=<span class="fm-codeblue">2</span>,
<a id="pgfId-1044435"></a>    max_tokens=<span class="fm-codeblue">20000</span>,
<a id="pgfId-1016410"></a>    output_mode=<span class="fm-codegreen">"count"</span>
<a id="pgfId-1016416"></a>)</pre>

  <p class="body"><a id="pgfId-1016422"></a>Now, of course, some words are bound to occur more often than others no matter what the text is about. The words “the,” “a,” “is,” and “are” will always dominate your word count histograms, drowning out other words—despite being pretty much useless features in a classification context. How could we address this?</p>

  <p class="body"><a id="pgfId-1016441"></a>You already guessed it: via normalization. We could just normalize word counts by subtracting the mean and dividing by the variance (computed across the entire training dataset). That would make sense. Except most vectorized sentences consist almost entirely of zeros (our previous example features 12 non-zero entries and 19,988 zero entries), a property called “sparsity.” That’s a great property to have, as it dramatically reduces compute load and reduces the risk of overfitting. If we subtracted the mean from each feature, we’d wreck sparsity. Thus, whatever normalization scheme we use should be divide-only. What, then, should we use as the denominator? The best practice is to go with something <a id="marker-1038433"></a>called <i class="fm-italics">TF-IDF normalization</i>—TF-IDF stands for “term frequency, inverse document frequency.”</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre11" id="pgfId-1016450"></a>Understanding TF-IDF normalization</p>

    <p class="fm-sidebar-text"><a id="pgfId-1016460"></a>The more a given term appears in a document, the more important that term is for understanding what the document is about. At the same time, the frequency at which the term appears across all documents in your dataset matters too: terms that appear in almost every document (like “the” or “a”) aren’t particularly informative, while terms that appear only in a small subset of all texts (like “Herzog”) are very distinctive, and thus important. TF-IDF is a metric that fuses these two ideas. It weights a given term by taking “term frequency,” how many times the term appears in the current document, and dividing it by a measure of “document frequency,” which estimates how often the term comes up across the dataset. You’d compute it as follows:</p>
    <pre class="programlisting"><a id="pgfId-1044450"></a><b class="fm-codebrown">def</b> tfidf(term, document, dataset):
<a id="pgfId-1044451"></a>    term_freq = document.count(term)
<a id="pgfId-1044452"></a>    doc_freq = math.log(sum(doc.count(term) <b class="fm-codebrown">for</b> doc <b class="fm-codebrown">in</b> dataset) + <span class="fm-codeblue">1</span>)
<a id="pgfId-1038446"></a>    <b class="fm-codebrown">return</b> term_freq / doc_freq</pre>
  </div>

  <p class="body"><a id="pgfId-1016528"></a>TF-IDF is so common that it’s built into the <code class="fm-code-in-text">TextVectorization</code> layer. All you need to do to start using it is to switch the <code class="fm-code-in-text">output_mode</code> argument to <code class="fm-code-in-text">"tf_idf"</code>.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1016588"></a>Listing 11.10 Configuring <code class="fm-code-in-text">TextVectorization</code> to return TF-IDF-weighted outputs</p>
  <pre class="programlisting"><a id="pgfId-1044482"></a>text_vectorization = TextVectorization(
<a id="pgfId-1044483"></a>    ngrams=<span class="fm-codeblue">2</span>,
<a id="pgfId-1044484"></a>    max_tokens=<span class="fm-codeblue">20000</span>,
<a id="pgfId-1044485"></a>    output_mode=<span class="fm-codegreen">"tf_idf"</span>,
<a id="pgfId-1016658"></a>)</pre>

  <p class="body"><a id="pgfId-1016664"></a>Let’s train a new model with this scheme.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1016721"></a>Listing 11.11 Training and testing the TF-IDF bigram model</p>
  <pre class="programlisting"><a id="pgfId-1016670"></a>text_vectorization.adapt(text_only_train_ds)      <span class="fm-combinumeral">❶</span>
<a id="pgfId-1016771"></a> 
<a id="pgfId-1048527"></a>tfidf_2gram_train_ds = train_ds.map(
<a id="pgfId-1048528"></a>    <b class="fm-codebrown">lambda</b> x, y: (text_vectorization(x), y),
<a id="pgfId-1048529"></a>    num_parallel_calls=<span class="fm-codeblue">4</span>)
<a id="pgfId-1048530"></a>tfidf_2gram_val_ds = val_ds.map(
<a id="pgfId-1048531"></a>    <b class="fm-codebrown">lambda</b> x, y: (text_vectorization(x), y),
<a id="pgfId-1048532"></a>    num_parallel_calls=<span class="fm-codeblue">4</span>)
<a id="pgfId-1048533"></a>tfidf_2gram_test_ds = test_ds.map(
<a id="pgfId-1048534"></a>    <b class="fm-codebrown">lambda</b> x, y: (text_vectorization(x), y),
<a id="pgfId-1048523"></a>    num_parallel_calls=<span class="fm-codeblue">4</span>)
<a id="pgfId-1044505"></a> 
<a id="pgfId-1044506"></a>model = get_model()
<a id="pgfId-1044507"></a>model.summary()
<a id="pgfId-1044508"></a>callbacks = [
<a id="pgfId-1044509"></a>    keras.callbacks.ModelCheckpoint(<span class="fm-codegreen">"tfidf_2gram.keras"</span>,
<a id="pgfId-1044510"></a>                                    save_best_only=<code class="fm-codegreen">True</code>)
<a id="pgfId-1044511"></a>]
<a id="pgfId-1044512"></a>model.fit(tfidf_2gram_train_ds.cache(),
<a id="pgfId-1044513"></a>          validation_data=tfidf_2gram_val_ds.cache(),
<a id="pgfId-1044514"></a>          epochs=<span class="fm-codeblue">10</span>,
<a id="pgfId-1044515"></a>          callbacks=callbacks)
<a id="pgfId-1044516"></a>model = keras.models.load_model(<span class="fm-codegreen">"tfidf_2gram.keras"</span>)
<a id="pgfId-1016860"></a><b class="fm-codebrown">print</b>(f<span class="fm-codegreen">"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}"</span>)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1056180"></a><span class="fm-combinumeral">❶</span> The adapt() call will learn the TF-IDF weights in addition to the vocabulary.</p>

  <p class="body"><a id="pgfId-1016886"></a>This gets us an 89.8% test accuracy on the IMDB classification task: it doesn’t seem to be particularly helpful in this case. However, for many text-classification datasets, it would be typical to see a one-percentage-point increase when using TF-IDF compared to plain binary encoding.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre11" id="pgfId-1016892"></a>Exporting a model that processes raw strings</p>

    <p class="fm-sidebar-text"><a id="pgfId-1016902"></a>In the preceding examples, we did our text standardization, splitting, and indexing as part of the <code class="fm-code-in-text1">tf.data</code> pipeline. But if we want to export a standalone model independent of this pipeline, we should make sure that it incorporates its own text preprocessing (otherwise, you’d have to reimplement in the production environment, which can be challenging or can lead to subtle discrepancies between the training data and the production data). Thankfully, this is easy.</p>

    <p class="fm-sidebar-text"><a id="pgfId-1016917"></a>Just create a new model that reuses your <code class="fm-code-in-text1">TextVectorization</code> layer and adds to it the model you just trained:</p>
    <pre class="programlisting"><a id="pgfId-1058382"></a>inputs = keras.Input(shape=(<span class="fm-codeblue">1</span>,), dtype=<span class="fm-codegreen">"string"</span>)   <span class="fm-combinumeral">❶</span>
<a id="pgfId-1058383"></a>processed_inputs = text_vectorization(inputs)      <span class="fm-combinumeral">❷</span>
<a id="pgfId-1058384"></a>outputs = model(processed_inputs)                  <span class="fm-combinumeral">❸</span>
<a id="pgfId-1058385"></a>inference_model = keras.Model(inputs, outputs)     <span class="fm-combinumeral">❹</span></pre>

    <p class="fm-code-annotation"><a id="pgfId-1058386"></a><span class="fm-combinumeral">❶</span> One input sample would be one string.</p>

    <p class="fm-code-annotation"><a id="pgfId-1058387"></a><span class="fm-combinumeral">❷</span> Apply text preprocessing.</p>

    <p class="fm-code-annotation"><a id="pgfId-1058388"></a><span class="fm-combinumeral">❸</span> Apply the previously trained model.</p>

    <p class="fm-code-annotation"><a id="pgfId-1058389"></a><span class="fm-combinumeral">❹</span> Instantiate the end-to-end model.</p>

    <p class="fm-sidebar-text"><a id="pgfId-1058390"></a>The resulting model can process batches of raw strings:</p>
    <pre class="programlisting"><a id="pgfId-1058391"></a><b class="fm-codebrown">import</b> tensorflow <b class="fm-codebrown">as</b> tf
<a id="pgfId-1058392"></a>raw_text_data = tf.convert_to_tensor([
<a id="pgfId-1058393"></a>    [<span class="fm-codegreen">"That was an excellent movie, I loved it."</span>],
<a id="pgfId-1058394"></a>])
<a id="pgfId-1058395"></a>predictions = inference_model(raw_text_data) 
<a id="pgfId-1058402"></a><b class="fm-codebrown">print</b>(f<span class="fm-codegreen">"{float(predictions[0] * 100):.2f} percent positive"</span>)<a id="marker-1058396"></a><a id="marker-1058397"></a><a id="marker-1058398"></a><a id="marker-1058399"></a><a id="marker-1058400"></a><a id="marker-1058401"></a></pre>
  </div>

  <h3 class="fm-head1" id="heading_id_12"><a id="pgfId-1017128"></a>11.3.3 Processing words as a sequence: The sequence model approach</h3>

  <p class="body"><a id="pgfId-1017153"></a><a id="marker-1017141"></a><a id="marker-1017143"></a>These past few examples clearly show that word order matters: manual engineering of order-based features, such as bigrams, yields a nice accuracy boost. Now remember: the history of deep learning is that of a move away from manual feature engineering, toward letting models learn their own features from exposure to data alone. What if, instead of manually crafting order-based features, we exposed the model to raw word sequences and let it figure out such features on its own? This is what <i class="fm-italics">sequence models</i> are about.</p>

  <p class="body"><a id="pgfId-1017162"></a>To implement a sequence model, you’d start by representing your input samples as sequences of integer indices (one integer standing for one word). Then, you’d map each integer to a vector to obtain vector sequences. Finally, you’d feed these sequences of vectors into a stack of layers that could cross-correlate features from adjacent vectors, such as a 1D convnet, a RNN, or a Transformer.</p>

  <p class="body"><a id="pgfId-1017168"></a>For some time around 2016–2017, bidirectional RNNs (in particular, bidirectional LSTMs) were considered to be the state of the art for sequence modeling. Since you’re already familiar with this architecture, this is what we’ll use in our first sequence model examples. However, nowadays sequence modeling is almost universally done with Transformers, which we will cover shortly. Oddly, one-dimensional convnets were never very popular in NLP, even though, in my own experience, a residual stack of depthwise-separable 1D convolutions can often achieve comparable performance to a bidirectional LSTM, at a greatly reduced computational cost.</p>

  <p class="fm-head2"><a id="pgfId-1017174"></a>A first practical example</p>

  <p class="body"><a id="pgfId-1017191"></a><a id="marker-1017185"></a><a id="marker-1017187"></a>Let’s try out a first sequence model in practice. First, let’s prepare datasets that return integer sequences.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1017247"></a>Listing 11.12 Preparing integer sequence datasets</p>
  <pre class="programlisting"><a id="pgfId-1044576"></a><b class="fm-codebrown">from</b> tensorflow.keras <b class="fm-codebrown">import</b> layers
<a id="pgfId-1044577"></a>  
<a id="pgfId-1044578"></a>max_length = <span class="fm-codeblue">600</span> 
<a id="pgfId-1044579"></a>max_tokens = <span class="fm-codeblue">20000</span> 
<a id="pgfId-1044580"></a>text_vectorization = layers.TextVectorization(
<a id="pgfId-1044581"></a>    max_tokens=max_tokens,
<a id="pgfId-1044582"></a>    output_mode=<span class="fm-codegreen">"int"</span>,
<a id="pgfId-1017321"></a>    output_sequence_length=max_length,     <span class="fm-combinumeral">❶</span>
<a id="pgfId-1044599"></a>)
<a id="pgfId-1044600"></a>text_vectorization.adapt(text_only_train_ds)
<a id="pgfId-1048666"></a> 
<a id="pgfId-1048670"></a>int_train_ds = train_ds.map(
<a id="pgfId-1048671"></a>    <b class="fm-codebrown">lambda</b> x, y: (text_vectorization(x), y)),
<a id="pgfId-1048672"></a>    num_parallel_calls=<span class="fm-codeblue">4</span>)
<a id="pgfId-1048673"></a>int_val_ds = val_ds.map(
<a id="pgfId-1048674"></a>    <b class="fm-codebrown">lambda</b> x, y: (text_vectorization(x), y),
<a id="pgfId-1048675"></a>    num_parallel_calls=<span class="fm-codeblue">4</span>)
<a id="pgfId-1048676"></a>int_test_ds = test_ds.map(
<a id="pgfId-1048677"></a>    <b class="fm-codebrown">lambda</b> x, y: (text_vectorization(x), y),
<a id="pgfId-1044601"></a>    num_parallel_calls=<span class="fm-codeblue">4</span>)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1055913"></a><span class="fm-combinumeral">❶</span> In order to keep a manageable input size, we’ll truncate the inputs after the first 600 words. This is a reasonable choice, since the average review length is 233 words, and only 5% of reviews are longer than 600 words.</p>

  <p class="body"><a id="pgfId-1017388"></a>Next, let’s make a model. The simplest way to convert our integer sequences to vector sequences is to one-hot encode the integers (each dimension would represent one possible term in the vocabulary). On top of these one-hot vectors, we’ll add a simple bidirectional LSTM.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1017445"></a>Listing 11.13 A sequence model built on one-hot encoded vector sequences</p>
  <pre class="programlisting"><a id="pgfId-1044618"></a><b class="fm-codebrown">import</b> tensorflow <b class="fm-codebrown">as</b> tf
<a id="pgfId-1017484"></a>inputs = keras.Input(shape=(<code class="fm-codegreen">None</code>,), dtype=<span class="fm-codegreen">"int64"</span>)    <span class="fm-combinumeral">❶</span>
<a id="pgfId-1017496"></a>embedded = tf.one_hot(inputs, depth=max_tokens)       <span class="fm-combinumeral">❷</span>
<a id="pgfId-1017508"></a>x = layers.Bidirectional(layers.LSTM(<span class="fm-codeblue">32</span>))(embedded)   <span class="fm-combinumeral">❸</span>
<a id="pgfId-1044647"></a>x = layers.Dropout(<span class="fm-codeblue">0.5</span>)(x) 
<a id="pgfId-1017526"></a>outputs = layers.Dense(<span class="fm-codeblue">1</span>, activation=<span class="fm-codegreen">"sigmoid"</span>)(x)    <span class="fm-combinumeral">❹</span>
<a id="pgfId-1044660"></a>model = keras.Model(inputs, outputs)
<a id="pgfId-1044661"></a>model.compile(optimizer=<span class="fm-codegreen">"rmsprop"</span>,
<a id="pgfId-1044662"></a>              loss=<span class="fm-codegreen">"binary_crossentropy"</span>,
<a id="pgfId-1044663"></a>              metrics=[<span class="fm-codegreen">"accuracy"</span>])
<a id="pgfId-1017562"></a>model.summary()</pre>

  <p class="fm-code-annotation"><a id="pgfId-1055715"></a><span class="fm-combinumeral">❶</span> One input is a sequence of integers.</p>

  <p class="fm-code-annotation"><a id="pgfId-1055736"></a><span class="fm-combinumeral">❷</span> Encode the integers into binary 20,000-dimensional vectors.</p>

  <p class="fm-code-annotation"><a id="pgfId-1055753"></a><span class="fm-combinumeral">❸</span> Add a bidirectional LSTM.</p>

  <p class="fm-code-annotation"><a id="pgfId-1055770"></a><span class="fm-combinumeral">❹</span> Finally, add a classification layer.</p>

  <p class="body"><a id="pgfId-1017636"></a>Now, let’s train our model.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1017693"></a>Listing 11.14 Training a first basic sequence model</p>
  <pre class="programlisting"><a id="pgfId-1044678"></a>callbacks = [
<a id="pgfId-1044679"></a>    keras.callbacks.ModelCheckpoint(<span class="fm-codegreen">"one_hot_bidir_lstm.keras"</span>,
<a id="pgfId-1044680"></a>                                    save_best_only=<code class="fm-codegreen">True</code>)
<a id="pgfId-1044681"></a>]
<a id="pgfId-1044682"></a>model.fit(int_train_ds, validation_data=int_val_ds, epochs=<span class="fm-codeblue">10</span>,
<a id="pgfId-1050102"></a>          callbacks=callbacks)
<a id="pgfId-1044683"></a>model = keras.models.load_model(<span class="fm-codegreen">"one_hot_bidir_lstm.keras"</span>) 
<a id="pgfId-1017762"></a><b class="fm-codebrown">print</b>(f<span class="fm-codegreen">"Test acc: {model.evaluate(int_test_ds)[1]:.3f}"</span>)</pre>

  <p class="body"><a id="pgfId-1017768"></a>A first observation: this model trains very slowly, especially compared to the lightweight model of the previous section. This is because our inputs are quite large: each input sample is encoded as a matrix of size <code class="fm-code-in-text">(600,</code> <code class="fm-code-in-text">20000)</code> (600 words per sample, 20,000 possible words). That’s 12,000,000 floats for a single movie review. Our bidirectional LSTM has a lot of work to do. Second, the model only gets to 87% test accuracy—it doesn’t perform nearly as well as our (very fast) binary unigram model.</p>

  <p class="body"><a id="pgfId-1017783"></a>Clearly, using one-hot encoding to turn words into vectors, which was the simplest thing we could do, wasn’t a great idea. There’s a better way: <i class="fm-italics">word embeddings</i>. <a id="marker-1017794"></a><a id="marker-1017797"></a></p>

  <p class="fm-head2"><a id="pgfId-1017803"></a>Understanding word embeddings</p>

  <p class="body"><a id="pgfId-1017828"></a><a id="marker-1035058"></a><a id="marker-1035059"></a><a id="marker-1035060"></a>Crucially, when you encode something via one-hot encoding, you’re making a feature-engineering decision. You’re injecting into your model a fundamental assumption about the structure of your feature space. That assumption is that <i class="fm-italics">the different tokens you’re encoding are all independent from each other</i>: indeed, one-hot vectors are all orthogonal to one another. And in the case of words, that assumption is clearly wrong. Words form a structured space: they share information with each other. The words “movie” and “film” are interchangeable in most sentences, so the vector that represents “movie” should not be orthogonal to the vector that represents “film”—they should be the same vector, or close enough.</p>

  <p class="body"><a id="pgfId-1017859"></a>To get a bit more abstract, the <i class="fm-italics">geometric relationship</i> between two <a id="marker-1017848"></a>word vectors should reflect the <i class="fm-italics">semantic relationship</i> between these <a id="marker-1017864"></a>words. For instance, in a reasonable word vector space, you would expect synonyms to be embedded into similar word vectors, and in general, you would expect the geometric distance (such as the cosine distance or L2 distance) between any two word vectors to relate to the “semantic distance” between the associated words. Words that mean different things should lie far away from each other, whereas related words should be closer.</p>

  <p class="body"><a id="pgfId-1017874"></a><i class="fm-italics">Word embeddings</i> are vector representations of words that achieve exactly this: they map human language into a structured geometric space.</p>

  <p class="body"><a id="pgfId-1017887"></a>Whereas the vectors obtained through one-hot encoding are binary, sparse (mostly made of zeros), and very high-dimensional (the same dimensionality as the number of words in the vocabulary), word embeddings are low-dimensional floating-point vectors (that is, dense vectors, as opposed to sparse vectors); see figure 11.2. It’s common to see word embeddings that are 256-dimensional, 512-dimensional, or 1,024-dimensional when dealing with very large vocabularies. On the other hand, one-hot encoding words generally leads to vectors that are 20,000-dimensional or greater (capturing a vocabulary of 20,000 tokens, in this case). So, word embeddings pack more information into far fewer dimensions.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/11-02.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1059508"></a>Figure 11.2 Word representations obtained from one-hot encoding or hashing are sparse, high-dimensional, and hardcoded. Word embeddings are dense, relatively low-dimensional, and learned from data.</p>

  <p class="body"><a id="pgfId-1017955"></a>Besides being <i class="fm-italics">dense</i> representations, word embeddings <a id="marker-1017928"></a>are also <i class="fm-italics">structured</i> representations, and their <a id="marker-1017944"></a>structure is learned from data. Similar words get embedded in close locations, and further, specific <i class="fm-italics">directions</i> in the embedding space are meaningful. To make this clearer, let’s look at a concrete example.</p>

  <p class="body"><a id="pgfId-1018080"></a>In figure 11.3, four words are embedded on a 2D plane: <i class="fm-italics">cat</i>, <i class="fm-italics">dog</i>, <i class="fm-italics">wolf</i>, and <i class="fm-italics">tiger</i>. With the vector representations we chose here, some semantic relationships between these words can be encoded as geometric transformations. For instance, the same vector allows us to go from <i class="fm-italics">cat</i> to <i class="fm-italics">tiger</i> and from <i class="fm-italics">dog</i> to <i class="fm-italics">wolf</i>: this vector could be interpreted as the “from pet to wild animal” vector. Similarly, another vector lets us go from <i class="fm-italics">dog</i> to <i class="fm-italics">cat</i> and from <i class="fm-italics">wolf</i> to <i class="fm-italics">tiger</i>, which could be interpreted as a “from canine to feline” vector.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/11-03.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1059550"></a>Figure 11.3 A toy example of a word-embedding space</p>

  <p class="body"><a id="pgfId-1018099"></a>In real-world word-embedding spaces, common examples of meaningful geometric transformations are “gender” vectors and “plural” vectors. For instance, by adding a “female” vector to the vector “king,” we obtain the vector “queen.” By adding a “plural” vector, we obtain “kings.” Word-embedding spaces typically feature thousands of such interpretable and potentially useful vectors.</p>

  <p class="body"><a id="pgfId-1018119"></a>Let’s look at how to use such an embedding space in practice. There are two ways to obtain word embeddings:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018125"></a>Learn word embeddings jointly with the main task you care about (such as document classification or sentiment prediction). In this setup, you start with random word vectors and then learn word vectors in the same way you learn the weights of a neural network.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018156"></a>Load into your model word embeddings that were precomputed using a different machine learning task than the one you’re trying to solve. These are <a class="calibre11" id="marker-1018145"></a>called <i class="fm-italics1">pretrained word embeddings</i>.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1018165"></a>Let’s review each of these approaches. <a id="marker-1018167"></a><a id="marker-1018170"></a></p>

  <p class="fm-head2"><a id="pgfId-1018176"></a>Learning word embeddings with the Embedding layer</p>

  <p class="body"><a id="pgfId-1018203"></a><a id="marker-1018187"></a><a id="marker-1018189"></a><a id="marker-1018191"></a><a id="marker-1018193"></a>Is there some ideal word-embedding space that would perfectly map human language and could be used for any natural language processing task? Possibly, but we have yet to compute anything of the sort. Also, there is no such a thing as <i class="fm-italics">human language</i>—there are many different languages, and they aren’t isomorphic to one another, because a language is the reflection of a specific culture and a specific context. But more pragmatically, what makes a good word-embedding space depends heavily on your task: the perfect word-embedding space for an English-language movie-review sentiment-analysis model may look different from the perfect embedding space for an English-language legal-document classification model, because the importance of certain semantic relationships varies from task to task.</p>

  <p class="body"><a id="pgfId-1018234"></a>It’s thus reasonable to <i class="fm-italics">learn</i> a new embedding space with every new task. Fortunately, backpropagation makes this easy, and Keras makes it even easier. It’s about learning the weights of <a id="marker-1018223"></a>a layer: the <code class="fm-code-in-text">Embedding</code> layer.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1018294"></a>Listing 11.15 Instantiating an <code class="fm-code-in-text">Embedding</code> layer</p>
  <pre class="programlisting"><a id="pgfId-1018243"></a>embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=<span class="fm-codeblue">256</span>)   <span class="fm-combinumeral">❶</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1055662"></a><span class="fm-combinumeral">❶</span> The Embedding layer takes at least two arguments: the number of possible tokens and the dimensionality of the embeddings (here, 256).</p>

  <p class="body"><a id="pgfId-1018372"></a>The <code class="fm-code-in-text">Embedding</code> layer is best understood as a dictionary that maps integer indices (which stand for specific words) to dense vectors. It takes integers as input, looks up these integers in an internal dictionary, and returns the associated vectors. It’s effectively a dictionary lookup (see figure 11.4).</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/11-04.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1059592"></a>Figure 11.4 The <code class="fm-code-in-text">Embedding</code> layer</p>

  <p class="body"><a id="pgfId-1018446"></a>The <code class="fm-code-in-text">Embedding</code> layer takes as input a rank-2 tensor of integers, of shape <code class="fm-code-in-text">(batch_size,</code> <code class="fm-code-in-text">sequence_length)</code>, where each entry is a sequence of integers. The layer then returns a 3D floating-point tensor of shape <code class="fm-code-in-text">(batch_size,</code> <code class="fm-code-in-text">sequence_length,</code> <code class="fm-code-in-text">embedding_ dimensionality)</code>.</p>

  <p class="body"><a id="pgfId-1018455"></a>When you instantiate an <code class="fm-code-in-text">Embedding</code> layer, its weights (its internal dictionary of token vectors) are initially random, just as with any other layer. During training, these word vectors are gradually adjusted via backpropagation, structuring the space into something the downstream model can exploit. Once fully trained, the embedding space will show a lot of structure—a kind of structure specialized for the specific problem for which you’re training your model.</p>

  <p class="body"><a id="pgfId-1018470"></a>Let’s build a model that includes an <code class="fm-code-in-text">Embedding</code> layer and benchmark it on our task.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1018536"></a>Listing 11.16 Model that uses an <code class="fm-code-in-text">Embedding</code> layer trained from scratch</p>
  <pre class="programlisting"><a id="pgfId-1044720"></a>inputs = keras.Input(shape=(<code class="fm-codegreen">None</code>,), dtype=<span class="fm-codegreen">"int64"</span>)
<a id="pgfId-1044721"></a>embedded = layers.Embedding(input_dim=max_tokens, output_dim=<span class="fm-codeblue">256</span>)(inputs)
<a id="pgfId-1044722"></a>x = layers.Bidirectional(layers.LSTM(<span class="fm-codeblue">32</span>))(embedded)
<a id="pgfId-1044723"></a>x = layers.Dropout(<span class="fm-codeblue">0.5</span>)(x)
<a id="pgfId-1044724"></a>outputs = layers.Dense(<span class="fm-codeblue">1</span>, activation=<span class="fm-codegreen">"sigmoid"</span>)(x)
<a id="pgfId-1044725"></a>model = keras.Model(inputs, outputs)
<a id="pgfId-1044726"></a>model.compile(optimizer=<span class="fm-codegreen">"rmsprop"</span>,
<a id="pgfId-1044727"></a>              loss=<span class="fm-codegreen">"binary_crossentropy"</span>,
<a id="pgfId-1044728"></a>              metrics=[<span class="fm-codegreen">"accuracy"</span>])
<a id="pgfId-1044729"></a>model.summary()
<a id="pgfId-1044730"></a>  
<a id="pgfId-1044731"></a>callbacks = [
<a id="pgfId-1044732"></a>    keras.callbacks.ModelCheckpoint(<span class="fm-codegreen">"embeddings_bidir_gru.keras"</span>,
<a id="pgfId-1044733"></a>                                    save_best_only=<code class="fm-codegreen">True</code>)
<a id="pgfId-1044734"></a>]
<a id="pgfId-1044735"></a>model.fit(int_train_ds, validation_data=int_val_ds, epochs=<span class="fm-codeblue">10</span>,
<a id="pgfId-1050114"></a>          callbacks=callbacks)
<a id="pgfId-1044736"></a>model = keras.models.load_model(<span class="fm-codegreen">"embeddings_bidir_gru.keras"</span>) 
<a id="pgfId-1018683"></a><b class="fm-codebrown">print</b>(f<span class="fm-codegreen">"Test acc: {model.evaluate(int_test_ds)[1]:.3f}"</span>)</pre>

  <p class="body"><a id="pgfId-1018689"></a>It trains much faster than the one-hot model (since the LSTM only has to process 256-dimensional vectors instead of 20,000-dimensional), and its test accuracy is comparable (87%). However, we’re still some way off from the results of our basic bigram model. Part of the reason why is simply that the model is looking at slightly less data: the bigram model processed full reviews, while our sequence model truncates sequences after 600 words. <a id="marker-1018691"></a><a id="marker-1018694"></a><a id="marker-1018696"></a><a id="marker-1018698"></a><a id="marker-1018700"></a></p>

  <p class="fm-head2"><a id="pgfId-1018706"></a>Understanding padding and masking</p>

  <p class="body"><a id="pgfId-1018761"></a><a id="marker-1018717"></a><a id="marker-1018719"></a><a id="marker-1018721"></a><a id="marker-1018723"></a>One thing that’s slightly hurting model performance here is that our input sequences are full of zeros. This comes from our use of the <code class="fm-code-in-text">output_sequence_length=max_ length</code> option in <code class="fm-code-in-text">TextVectorization</code> (with <code class="fm-code-in-text">max_length</code> equal to 600): sentences longer than 600 tokens are truncated to a length of 600 tokens, and sentences shorter than 600 tokens are padded with zeros at the end so that they can be concatenated together with other sequences to form contiguous batches.</p>

  <p class="body"><a id="pgfId-1018770"></a>We’re using a bidirectional RNN: two RNN layers running in parallel, with one processing the tokens in their natural order, and the other processing the same tokens in reverse. The RNN that looks at the tokens in their natural order will spend its last iterations seeing only vectors that encode padding—possibly for several hundreds of iterations if the original sentence was short. The information stored in the internal state of the RNN will gradually fade out as it gets exposed to these meaningless inputs.</p>

  <p class="body"><a id="pgfId-1018776"></a>We need some way to tell the RNN that it should skip these iterations. There’s an API for that: <i class="fm-italics">masking</i>.</p>

  <p class="body"><a id="pgfId-1018847"></a>The <code class="fm-code-in-text">Embedding</code> layer is capable of generating a “mask” that corresponds to its input data. This mask is a tensor of ones and zeros (or True/False booleans), of shape <code class="fm-code-in-text">(batch_size,</code> <code class="fm-code-in-text">sequence_length)</code>, where the entry <code class="fm-code-in-text">mask[i,</code> <code class="fm-code-in-text">t]</code> indicates where timestep <code class="fm-code-in-text">t</code> of sample <code class="fm-code-in-text">i</code> should be skipped or not (the timestep will be skipped if <code class="fm-code-in-text">mask[i,</code> <code class="fm-code-in-text">t]</code> is 0 or False, and processed otherwise).</p>

  <p class="body"><a id="pgfId-1018888"></a>By default, this option isn’t active—you can turn it on by passing <code class="fm-code-in-text">mask_zero=True</code> to your <code class="fm-code-in-text">Embedding</code> layer. You can retrieve the mask with <a id="marker-1048806"></a>the <code class="fm-code-in-text">compute_mask()</code> method:</p>
  <pre class="programlisting"><a id="pgfId-1048815"></a>&gt;&gt;&gt; embedding_layer = Embedding(input_dim=<span class="fm-codeblue">10</span>, output_dim=<span class="fm-codeblue">256</span>, mask_zero=<code class="fm-codegreen">True</code>)
<a id="pgfId-1018911"></a>&gt;&gt;&gt;  some_input = [
<a id="pgfId-1049876"></a>...  [<span class="fm-codeblue">4</span>, <span class="fm-codeblue">3</span>, <span class="fm-codeblue">2</span>, <span class="fm-codeblue">1</span>, <span class="fm-codeblue">0</span>, <span class="fm-codeblue">0</span>, <span class="fm-codeblue">0</span>],
<a id="pgfId-1049877"></a>...  [<span class="fm-codeblue">5</span>, <span class="fm-codeblue">4</span>, <span class="fm-codeblue">3</span>, <span class="fm-codeblue">2</span>, <span class="fm-codeblue">1</span>, <span class="fm-codeblue">0</span>, <span class="fm-codeblue">0</span>],
<a id="pgfId-1018929"></a>...  [<span class="fm-codeblue">2</span>, <span class="fm-codeblue">1</span>, <span class="fm-codeblue">0</span>, <span class="fm-codeblue">0</span>, <span class="fm-codeblue">0</span>, <span class="fm-codeblue">0</span>, <span class="fm-codeblue">0</span>]]
<a id="pgfId-1018935"></a>&gt;&gt;&gt; mask = embedding_layer.compute_mask(some_input)
<a id="pgfId-1018941"></a>&lt;tf.Tensor: shape=(3, 7), dtype=bool, numpy=
<a id="pgfId-1044773"></a>array([[ True,  True,  True,  True, False, False, False],
<a id="pgfId-1044774"></a>       [ True,  True,  True,  True,  True, False, False],
<a id="pgfId-1018959"></a>       [ True,  True, False, False, False, False, False]])&gt;</pre>

  <p class="body"><a id="pgfId-1018965"></a>In practice, you will almost never have to manage masks by hand. Instead, Keras will automatically pass on the mask to every layer that is able to process it (as a piece of metadata attached to the sequence it represents). This mask will be used by RNN layers to skip masked steps. If your model returns an entire sequence, the mask will also be used by the loss function to skip masked steps in the output sequence.</p>

  <p class="body"><a id="pgfId-1018971"></a>Let’s try retraining our model with masking enabled.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1019028"></a>Listing 11.17 Using an <code class="fm-code-in-text">Embedding</code> layer with masking enabled</p>
  <pre class="programlisting"><a id="pgfId-1044789"></a>inputs = keras.Input(shape=(<code class="fm-codegreen">None</code>,), dtype=<span class="fm-codegreen">"int64"</span>)
<a id="pgfId-1044790"></a>embedded = layers.Embedding(
<a id="pgfId-1044791"></a>    input_dim=max_tokens, output_dim=<span class="fm-codeblue">256</span>, mask_zero=<code class="fm-codegreen">True</code>)(inputs)
<a id="pgfId-1044792"></a>x = layers.Bidirectional(layers.LSTM(<span class="fm-codeblue">32</span>))(embedded)
<a id="pgfId-1044793"></a>x = layers.Dropout(<span class="fm-codeblue">0.5</span>)(x)
<a id="pgfId-1044794"></a>outputs = layers.Dense(<span class="fm-codeblue">1</span>, activation=<span class="fm-codegreen">"sigmoid"</span>)(x)
<a id="pgfId-1044795"></a>model = keras.Model(inputs, outputs)
<a id="pgfId-1044796"></a>model.compile(optimizer=<span class="fm-codegreen">"rmsprop"</span>,
<a id="pgfId-1044797"></a>              loss=<span class="fm-codegreen">"binary_crossentropy"</span>,
<a id="pgfId-1044798"></a>              metrics=[<span class="fm-codegreen">"accuracy"</span>])
<a id="pgfId-1044799"></a>model.summary()
<a id="pgfId-1044801"></a>callbacks = [
<a id="pgfId-1044802"></a>    keras.callbacks.ModelCheckpoint(<span class="fm-codegreen">"embeddings_bidir_gru_with_masking.keras"</span>,
<a id="pgfId-1044803"></a>                                    save_best_only=<code class="fm-codegreen">True</code>)
<a id="pgfId-1044804"></a>]
<a id="pgfId-1044805"></a>model.fit(int_train_ds, validation_data=int_val_ds, epochs=<span class="fm-codeblue">10</span>,
<a id="pgfId-1050131"></a>          callbacks=callbacks)
<a id="pgfId-1044806"></a>model = keras.models.load_model(<span class="fm-codegreen">"embeddings_bidir_gru_with_masking.keras"</span>) 
<a id="pgfId-1019181"></a><b class="fm-codebrown">print</b>(f<span class="fm-codegreen">"Test acc: {model.evaluate(int_test_ds)[1]:.3f}"</span>)</pre>

  <p class="body"><a id="pgfId-1019187"></a>This time we get to 88% test accuracy—a small but noticeable improvement. <a id="marker-1019189"></a><a id="marker-1019192"></a><a id="marker-1019194"></a><a id="marker-1019196"></a></p>

  <p class="fm-head2"><a id="pgfId-1019202"></a>Using pretrained word embeddings</p>

  <p class="body"><a id="pgfId-1019221"></a><a id="marker-1019213"></a><a id="marker-1019215"></a><a id="marker-1019217"></a>Sometimes you have so little training data available that you can’t use your data alone to learn an appropriate task-specific embedding of your vocabulary. In such cases, instead of learning word embeddings jointly with the problem you want to solve, you can load embedding vectors from a precomputed embedding space that you know is highly structured and exhibits useful properties—one that captures generic aspects of language structure. The rationale behind using pretrained word embeddings in natural language processing is much the same as for using pretrained convnets in image classification: you don’t have enough data available to learn truly powerful features on your own, but you expect that the features you need are fairly generic—that is, common visual features or semantic features. In this case, it makes sense to reuse features learned on a different problem.</p>

  <p class="body"><a id="pgfId-1019226"></a>Such word embeddings are generally computed using word-occurrence statistics (observations about what words co-occur in sentences or documents), using a variety of techniques, some involving neural networks, others not. The idea of a dense, low-dimensional embedding space for words, computed in an unsupervised way, was initially explored by Bengio et al. in the early 2000s,<a id="Id-1019229"></a><a href="../Text/11.htm#pgfId-1019229"><sup class="footnotenumber">1</sup></a> but it only started to take off in research and industry applications after the release of one of the most famous and successful word-embedding schemes: the Word2Vec algorithm (<span class="fm-hyperlink"><a class="url" href="https://code.google.com/archive/p/word2vec">https://code.google.com/archive/p/word2vec</a></span>), developed by Tomas Mikolov at Google in 2013. Word2Vec dimensions capture specific semantic properties, such as gender.</p>

  <p class="body"><a id="pgfId-1019517"></a>There are various precomputed databases of word embeddings that you can download and use in a Keras <code class="fm-code-in-text">Embedding</code> layer. Word2vec is one of them. Another popular one is called Global Vectors for Word Representation (GloVe, <span class="fm-hyperlink"><a class="url" href="https://nlp.stanford.edu/projects/glove">https://nlp.stanford.edu/projects/glove</a></span>), which was developed by Stanford researchers in 2014. This embedding technique is based on factorizing a matrix of word co-occurrence statistics. Its developers have made available precomputed embeddings for millions of English tokens, obtained from Wikipedia data and Common Crawl data.</p>

  <p class="body"><a id="pgfId-1019533"></a>Let’s look at how you can get started using GloVe embeddings in a Keras model. The same method is valid for Word2Vec embeddings or any other word-embedding database. We’ll start by downloading the GloVe files and parse them. We’ll then load the word vectors into a Keras <code class="fm-code-in-text">Embedding</code> layer, which we’ll use to build a new model.</p>

  <p class="body"><a id="pgfId-1019548"></a>First, let’s download the GloVe word embeddings precomputed on the 2014 English Wikipedia dataset. It’s an 822 MB zip file containing 100-dimensional embedding vectors for 400,000 words (or non-word tokens).</p>
  <pre class="programlisting"><a id="pgfId-1019554"></a>!wget http:/ /nlp.stanford.edu/data/glove.6B.zip
<a id="pgfId-1019568"></a>!unzip -q glove.6B.zip</pre>

  <p class="body"><a id="pgfId-1019574"></a>Let’s parse the unzipped file (a .txt file) to build an index that maps words (as strings) to their vector representation.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1019631"></a>Listing 11.18 Parsing the GloVe word-embeddings file</p>
  <pre class="programlisting"><a id="pgfId-1044878"></a><b class="fm-codebrown">import</b> numpy <b class="fm-codebrown">as</b> np
<a id="pgfId-1044879"></a>path_to_glove_file = <span class="fm-codegreen">"glove.6B.100d.txt"</span> 
<a id="pgfId-1044880"></a>  
<a id="pgfId-1044881"></a>embeddings_index = {} 
<a id="pgfId-1044882"></a><b class="fm-codebrown">with</b> open(path_to_glove_file) <b class="fm-codebrown">as</b> f:
<a id="pgfId-1044883"></a>    <b class="fm-codebrown">for</b> line <b class="fm-codebrown">in</b> f:
<a id="pgfId-1044884"></a>        word, coefs = line.split(maxsplit=<span class="fm-codeblue">1</span>)
<a id="pgfId-1044885"></a>        coefs = np.fromstring(coefs, <span class="fm-codegreen">"f"</span>, sep=<span class="fm-codegreen">" "</span>)
<a id="pgfId-1044886"></a>        embeddings_index[word] = coefs
<a id="pgfId-1044887"></a>  
<a id="pgfId-1019717"></a><b class="fm-codebrown">print</b>(f<span class="fm-codegreen">"Found {len(embeddings_index)} word vectors."</span>)</pre>

  <p class="body"><a id="pgfId-1019780"></a>Next, let’s build an embedding matrix that you can load into an <code class="fm-code-in-text">Embedding</code> layer. It must be a matrix of shape <code class="fm-code-in-text">(max_words,</code> <code class="fm-code-in-text">embedding_dim)</code>, where each entry <i class="fm-italics">i</i> contains the <code class="fm-code-in-text">embedding_dim</code>-dimensional vector for the word of index <i class="fm-italics">i</i> in the reference word index (built during tokenization).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1019840"></a>Listing 11.19 Preparing the GloVe word-embeddings matrix</p>
  <pre class="programlisting"><a id="pgfId-1044908"></a>embedding_dim = <span class="fm-codeblue">100</span> 
<a id="pgfId-1044909"></a>  
<a id="pgfId-1019879"></a>vocabulary = text_vectorization.get_vocabulary()             <span class="fm-combinumeral">❶</span>
<a id="pgfId-1019896"></a>word_index = dict(zip(vocabulary, range(len(vocabulary))))   <span class="fm-combinumeral">❷</span>
<a id="pgfId-1019913"></a> 
<a id="pgfId-1019908"></a>embedding_matrix = np.zeros((max_tokens, embedding_dim))     <span class="fm-combinumeral">❸</span>
<a id="pgfId-1044964"></a><b class="fm-codebrown">for</b> word, i <b class="fm-codebrown">in</b> word_index.items():
<a id="pgfId-1044965"></a>    <b class="fm-codebrown">if</b> i &lt; max_tokens:
<a id="pgfId-1044966"></a>        embedding_vector = embeddings_index.get(word)
<a id="pgfId-1019943"></a>    <b class="fm-codebrown">if</b> embedding_vector <b class="fm-codebrown">is</b> <b class="fm-codebrown">not</b> <code class="fm-codegreen">None</code>:                         <span class="fm-combinumeral">❹</span>
<a id="pgfId-1019955"></a>        embedding_matrix[i] = embedding_vector               <span class="fm-combinumeral">❹</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1055420"></a><span class="fm-combinumeral">❶</span> Retrieve the vocabulary indexed by our previous TextVectorization layer.</p>

  <p class="fm-code-annotation"><a id="pgfId-1055441"></a><span class="fm-combinumeral">❷</span> Use it to create a mapping from words to their index in the vocabulary.</p>

  <p class="fm-code-annotation"><a id="pgfId-1055458"></a><span class="fm-combinumeral">❸</span> Prepare a matrix that we’ll fill with the GloVe vectors.</p>

  <p class="fm-code-annotation"><a id="pgfId-1055475"></a><span class="fm-combinumeral">❹</span> Fill entry i in the matrix with the word vector for index i. Words not found in the embedding index will be all zeros.</p>

  <p class="body"><a id="pgfId-1020067"></a>Finally, we use a <code class="fm-code-in-text">Constant</code> initializer to <a id="marker-1020046"></a>load the pretrained embeddings in an <code class="fm-code-in-text">Embedding</code> layer. So as not to disrupt the pretrained representations during training, we freeze the layer via <code class="fm-code-in-text">trainable=False</code>:</p>
  <pre class="programlisting"><a id="pgfId-1020076"></a>embedding_layer = layers.Embedding(
<a id="pgfId-1020090"></a>    max_tokens,
<a id="pgfId-1020096"></a>    embedding_dim,
<a id="pgfId-1020102"></a>    embeddings_initializer=keras.initializers.Constant(embedding_matrix),
<a id="pgfId-1020108"></a>    trainable=<code class="fm-codegreen">False</code>,
<a id="pgfId-1020114"></a>    mask_zero=<code class="fm-codegreen">True</code>,
<a id="pgfId-1020120"></a>)</pre>

  <p class="body"><a id="pgfId-1020126"></a>We’re now ready to train a new model—identical to our previous model, but leveraging the 100-dimensional pretrained GloVe embeddings instead of 128-dimensional learned embeddings.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1020183"></a>Listing 11.20 Model that uses a pretrained Embedding layer</p>
  <pre class="programlisting"><a id="pgfId-1045066"></a>inputs = keras.Input(shape=(<code class="fm-codegreen">None</code>,), dtype=<span class="fm-codegreen">"int64"</span>)
<a id="pgfId-1045067"></a>embedded = embedding_layer(inputs)
<a id="pgfId-1045068"></a>x = layers.Bidirectional(layers.LSTM(<span class="fm-codeblue">32</span>))(embedded)
<a id="pgfId-1045069"></a>x = layers.Dropout(<span class="fm-codeblue">0.5</span>)(x)
<a id="pgfId-1045070"></a>outputs = layers.Dense(<span class="fm-codeblue">1</span>, activation=<span class="fm-codegreen">"sigmoid"</span>)(x)
<a id="pgfId-1045071"></a>model = keras.Model(inputs, outputs)
<a id="pgfId-1045072"></a>model.compile(optimizer=<span class="fm-codegreen">"rmsprop"</span>,
<a id="pgfId-1045073"></a>              loss=<span class="fm-codegreen">"binary_crossentropy"</span>,
<a id="pgfId-1045074"></a>              metrics=[<span class="fm-codegreen">"accuracy"</span>])
<a id="pgfId-1045075"></a>model.summary()
<a id="pgfId-1045076"></a>  
<a id="pgfId-1045077"></a>callbacks = [
<a id="pgfId-1045078"></a>    keras.callbacks.ModelCheckpoint(<span class="fm-codegreen">"glove_embeddings_sequence_model.keras"</span>,
<a id="pgfId-1045079"></a>                                    save_best_only=<code class="fm-codegreen">True</code>)
<a id="pgfId-1045080"></a>]
<a id="pgfId-1045081"></a>model.fit(int_train_ds, validation_data=int_val_ds, epochs=<span class="fm-codeblue">10</span>,
<a id="pgfId-1050145"></a>          callbacks=callbacks)
<a id="pgfId-1045082"></a>model = keras.models.load_model(<span class="fm-codegreen">"glove_embeddings_sequence_model.keras"</span>)
<a id="pgfId-1020317"></a><b class="fm-codebrown">print</b>(f<span class="fm-codegreen">"Test acc: {model.evaluate(int_test_ds)[1]:.3f}"</span>)</pre>

  <p class="body"><a id="pgfId-1020323"></a>You’ll find that on this particular task, pretrained embeddings aren’t very helpful, because the dataset contains enough samples that it is possible to learn a specialized enough embedding space from scratch. However, leveraging pretrained embeddings can be very helpful when you’re working with a smaller dataset. <a id="marker-1045055"></a><a id="marker-1045056"></a><a id="marker-1045057"></a><a id="marker-1045059"></a><a id="marker-1045060"></a></p>

  <h2 class="fm-head" id="heading_id_13"><a id="pgfId-1020342"></a>11.4 The Transformer architecture</h2>

  <p class="body"><a id="pgfId-1020361"></a><a id="marker-1020355"></a><a id="marker-1020357"></a>Starting in 2017, a new model architecture started overtaking recurrent neural networks across most natural language processing tasks: the Transformer.</p>

  <p class="body"><a id="pgfId-1020366"></a>Transformers were introduced in the seminal paper “Attention is all you need” by Vaswani et al.<a id="Id-1020369"></a><a href="../Text/11.htm#pgfId-1020369"><sup class="footnotenumber">2</sup></a> The gist of the paper is right there in the title: as it turned out, a simple mechanism called “neural attention” could be used to build powerful sequence models that didn’t feature any recurrent layers or convolution layers.</p>

  <p class="body"><a id="pgfId-1020386"></a>This finding unleashed nothing short of a revolution in natural language processing—and beyond. Neural attention has fast become one of the most influential ideas in deep learning. In this section, you’ll get an in-depth explanation of how it works and why it has proven so effective for sequence data. We’ll then leverage self-attention to create a Transformer encoder, one of the basic components of the Transformer architecture, and we’ll apply it to the IMDB movie review classification task.</p>

  <h3 class="fm-head1" id="heading_id_14"><a id="pgfId-1020392"></a>11.4.1 Understanding self-attention</h3>

  <p class="body"><a id="pgfId-1020411"></a><a id="marker-1020403"></a><a id="marker-1020405"></a><a id="marker-1020407"></a>As you’re going through this book, you may be skimming some parts and attentively reading others, depending on what your goals or interests are. What if your models did the same? It’s a simple yet powerful idea: not all input information seen by a model is equally important to the task at hand, so models should “pay more attention” to some features and “pay less attention” to other features.</p>

  <p class="body"><a id="pgfId-1020416"></a>Does that sound familiar? You’ve already encountered a similar concept twice in this book:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1020422"></a>Max pooling in convnets looks at a pool of features in a spatial region and selects just one feature to keep. That’s an “all or nothing” form of attention: keep the most important feature and discard the rest.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1020436"></a>TF-IDF normalization assigns importance scores to tokens based on how much information different tokens are likely to carry. Important tokens get boosted while irrelevant tokens get faded out. That’s a continuous form of attention.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1020446"></a>There are many different forms of attention you could imagine, but they all start by computing importance scores for a set of features, with higher scores for more relevant features and lower scores for less relevant ones (see figure 11.5). How these scores should be computed, and what you should do with them, will vary from approach to approach.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/11-05.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1059644"></a>Figure 11.5 The general concept of “attention” in deep learning: input features get assigned “attention scores,” which can be used to inform the next representation of the input.</p>

  <p class="body"><a id="pgfId-1020462"></a>Crucially, this kind of attention mechanism can be used for more than just highlighting or erasing certain features. It can be used to make features <i class="fm-italics">context-aware</i>. You’ve just learned about word embeddings—vector spaces that capture the “shape” of the semantic relationships between different words. In an embedding space, a single word has a fixed position—a fixed set of relationships with every other word in the space. But that’s not quite how language works: the meaning of a word is usually context-specific. When you mark the date, you’re not talking about the same “date” as when you go on a date, nor is it the kind of date you’d buy at the market. When you say, “I’ll see you soon,” the meaning of the word “see” is subtly different from the “see” in “I’ll see this project to its end” or “I see what you mean.” And, of course, the meaning of pronouns like “he,” “it,” “in,” etc., is entirely sentence-specific and can even change multiple times within a single sentence.</p>

  <p class="body"><a id="pgfId-1020491"></a>Clearly, a smart embedding space would provide a different vector representation for a word depending on the other words surrounding it. That’s where <i class="fm-italics">self-attention</i> comes in. The purpose of self-attention is to modulate the representation of a token by using the representations of related tokens in the sequence. This produces context-aware token representations. Consider an example sentence: “The train left the station on time.” Now, consider one word in the sentence: station. What kind of station are we talking about? Could it be a radio station? Maybe the International Space Station? Let’s figure it out algorithmically via self-attention (see figure 11.6).</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/11-06.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1059693"></a>Figure 11.6 Self-attention: attention scores are computed between “station” and every other word in the sequence, and they are then used to weight a sum of word vectors that becomes the new “station” vector.</p>

  <p class="body"><a id="pgfId-1020516"></a>Step 1 is to compute relevancy scores between the vector for “station” and every other word in the sentence. These are our “attention scores.” We’re simply going to use the dot product between two word vectors as a measure of the strength of their relationship. It’s a very computationally efficient distance function, and it was already the standard way to relate two word embeddings to each other long before Transformers. In practice, these scores will also go through a scaling function and a softmax, but for now, that’s just an implementation detail.</p>

  <p class="body"><a id="pgfId-1020536"></a>Step 2 is to compute the sum of all word vectors in the sentence, weighted by our relevancy scores. Words closely related to “station” will contribute more to the sum (including the word “station” itself), while irrelevant words will contribute almost nothing. The resulting vector is our new representation for “station”: a representation that incorporates the surrounding context. In particular, it includes part of the “train” vector, clarifying that it is, in fact, a “train station.”</p>

  <p class="body"><a id="pgfId-1020542"></a>You’d repeat this process for every word in the sentence, producing a new sequence of vectors encoding the sentence. Let’s see it in NumPy-like pseudocode:</p>
  <pre class="programlisting"><a id="pgfId-1045129"></a><b class="fm-codebrown">def</b> self_attention(input_sequence):
<a id="pgfId-1045130"></a>    output = np.zeros(shape=input_sequence.shape)
<a id="pgfId-1020568"></a>    <b class="fm-codebrown">for</b> i, pivot_vector <b class="fm-codebrown">in</b> enumerate(input_sequence):            <span class="fm-combinumeral">❶</span>
<a id="pgfId-1045151"></a>        scores = np.zeros(shape=(len(input_sequence),))
<a id="pgfId-1045152"></a>        <b class="fm-codebrown">for</b> j, vector <b class="fm-codebrown">in</b> enumerate(input_sequence):
<a id="pgfId-1020592"></a>            scores[j] = np.dot(pivot_vector, vector.T)           <span class="fm-combinumeral">❷</span>
<a id="pgfId-1020604"></a>        scores /= np.sqrt(input_sequence.shape[<span class="fm-codeblue">1</span>])               <span class="fm-combinumeral">❸</span>
<a id="pgfId-1020616"></a>        scores = softmax(scores)                                 <span class="fm-combinumeral">❸</span>
<a id="pgfId-1045193"></a>        new_pivot_representation = np.zeros(shape=pivot_vector.shape)
<a id="pgfId-1045194"></a>        <b class="fm-codebrown">for</b> j, vector <b class="fm-codebrown">in</b> enumerate(input_sequence):
<a id="pgfId-1020640"></a>            new_pivot_representation += vector * scores[j]       <span class="fm-combinumeral">❹</span>
<a id="pgfId-1020652"></a>        output[i] = new_pivot_representation                     <span class="fm-combinumeral">❺</span>
<a id="pgfId-1020664"></a>    <b class="fm-codebrown">return</b> output</pre>

  <p class="fm-code-annotation"><a id="pgfId-1055072"></a><span class="fm-combinumeral">❶</span> Iterate over each token in the input sequence.</p>

  <p class="fm-code-annotation"><a id="pgfId-1055096"></a><span class="fm-combinumeral">❷</span> Compute the dot product (attention score) between the token and every other token.</p>

  <p class="fm-code-annotation"><a id="pgfId-1055113"></a><span class="fm-combinumeral">❸</span> Scale by a normalization factor, and apply a softmax.</p>

  <p class="fm-code-annotation"><a id="pgfId-1055130"></a><span class="fm-combinumeral">❹</span> Take the sum of all tokens weighted by the attention scores.</p>

  <p class="fm-code-annotation"><a id="pgfId-1055147"></a><span class="fm-combinumeral">❺</span> That sum is our output.</p>

  <p class="body"><a id="pgfId-1020767"></a>Of course, in practice you’d use a vectorized implementation. Keras has a built-in layer to handle <a id="marker-1020756"></a>it: the <code class="fm-code-in-text">MultiHeadAttention</code> layer. Here’s how you would use it:</p>
  <pre class="programlisting"><a id="pgfId-1045233"></a>num_heads = <span class="fm-codeblue">4</span> 
<a id="pgfId-1045234"></a>embed_dim = <span class="fm-codeblue">256</span> 
<a id="pgfId-1045235"></a>mha_layer = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
<a id="pgfId-1020802"></a>outputs = mha_layer(inputs, inputs, inputs)</pre>

  <p class="body"><a id="pgfId-1020808"></a>Reading this, you’re probably wondering</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1020814"></a>Why are we passing the inputs to the layer <i class="fm-italics1">three</i> times? That seems redundant.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1020837"></a>What are these “multiple heads” we’re referring to? That sounds intimidating—do they also grow back if you cut them?</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1020847"></a>Both of these questions have simple answers. Let’s take a look.</p>

  <p class="fm-head2"><a id="pgfId-1020853"></a>Generalized self-attention: The query-key-value model</p>

  <p class="body"><a id="pgfId-1020873"></a><a id="marker-1020864"></a>So far, we have only considered one input sequence. However, the Transformer architecture was originally developed for machine translation, where you have to deal with two input sequences: the source sequence you’re currently translating (such as “How’s the weather today?”), and the target sequence you’re converting it to (such as “¿Qué tiempo hace hoy?”). A Transformer is a <i class="fm-italics">sequence-to-sequence</i> model: it was <a id="marker-1020878"></a>designed to convert one sequence into another. You’ll learn about sequence-to-sequence models in depth later in this chapter.</p>

  <p class="body"><a id="pgfId-1020888"></a>Now let’s take a step back. The self-attention mechanism as we’ve introduced it performs the following, schematically:</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/11-06-UN01.png"/></p>

  <p class="body"><a id="pgfId-1020938"></a>This means “for each token in <code class="fm-code-in-text">inputs</code> (A), compute how much the token is related to every token in <code class="fm-code-in-text">inputs</code> (B), and use these scores to weight a sum of tokens from <code class="fm-code-in-text">inputs</code> (C).” Crucially, there’s nothing that requires A, B, and C to refer to the same input sequence. In the general case, you could be doing this with three different sequences. We’ll call them “query,” “keys,” and “values.” The operation becomes “for each element in the query, compute how much the element is related to every key, and use these scores to weight a sum of values”:</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/11-06-UN02.png"/></p>

  <p class="body"><a id="pgfId-1020947"></a>This terminology comes from search engines and recommender systems (see figure 11.7). Imagine that you’re typing up a query to retrieve a photo from your collection—“dogs on the beach.” Internally, each of your pictures in the database is described by a set of keywords—“cat,” “dog,” “party,” etc. We’ll call those “keys.” The search engine will start by comparing your query to the keys in the database. “Dog” yields a match of 1, and “cat” yields a match of 0. It will then rank those keys by strength of match—relevance—and it will return the pictures associated with the top <i class="fm-italics">N</i> matches, in order of relevance.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/11-07.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1059769"></a>Figure 11.7 Retrieving images from a database: the “query” is compared to a set of “keys,” and the match scores are used to rank “values” (images).</p>

  <p class="body"><a id="pgfId-1020981"></a>Conceptually, this is what Transformer-style attention is doing. You’ve got a reference sequence that describes something you’re looking for: the query. You’ve got a body of knowledge that you’re trying to extract information from: the values. Each value is assigned a key that describes the value in a format that can be readily compared to a query. You simply match the query to the keys. Then you return a weighted sum of values.</p>

  <p class="body"><a id="pgfId-1021001"></a>In practice, the keys and the values are often the same sequence. In machine translation, for instance, the query would be the target sequence, and the source sequence would play the roles of both keys and values: for each element of the target (like “tiempo”), you want to go back to the source (“How’s the weather today?”) and identify the different bits that are related to it (“tiempo” and “weather” should have a strong match). And naturally, if you’re just doing sequence classification, then query, keys, and values are all the same: you’re comparing a sequence to itself, to enrich each token with context from the whole sequence.</p>

  <p class="body"><a id="pgfId-1021023"></a>That explains why we needed to pass <code class="fm-code-in-text">inputs</code> three times to our <code class="fm-code-in-text">MultiHeadAttention</code> layer. But why “multi-head” attention?<a id="marker-1021028"></a><a id="marker-1021031"></a><a id="marker-1021033"></a><a id="marker-1021035"></a></p>

  <h3 class="fm-head1" id="heading_id_15"><a id="pgfId-1021041"></a>11.4.2 Multi-head attention</h3>

  <p class="body"><a id="pgfId-1021060"></a><a id="marker-1035613"></a><a id="marker-1035614"></a><a id="marker-1035615"></a>“Multi-head attention” is an extra tweak to the self-attention mechanism, introduced in “Attention is all you need.” The “multi-head” moniker refers to the fact that the output space of the self-attention layer gets factored into a set of independent subspaces, learned separately: the initial query, key, and value are sent through three independent sets of dense projections, resulting in three separate vectors. Each vector is processed via neural attention, and the three outputs are concatenated back together into a single output sequence. Each such subspace is called a “head.” The full picture is shown in figure 11.8.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/11-08.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1059814"></a>Figure 11.8 The <code class="fm-code-in-text">MultiHeadAttention</code> layer</p>

  <p class="body"><a id="pgfId-1021090"></a>The presence of the learnable dense projections enables the layer to actually learn something, as opposed to being a purely stateless transformation that would require additional layers before or after it to be useful. In addition, having independent heads helps the layer learn different groups of features for each token, where features within one group are correlated with each other but are mostly independent from features in a different group.</p>

  <p class="body"><a id="pgfId-1021123"></a>This is similar in principle to what makes depthwise separable convolutions work: in a depthwise separable convolution, the output space of the convolution is factored into many subspaces (one per input channel) that get learned independently. The “Attention is all you need” paper was written at a time when the idea of factoring feature spaces into independent subspaces had been shown to provide great benefits for computer vision models—both in the case of depthwise separable convolutions, and in the case of a closely related <a id="marker-1021112"></a>approach, <i class="fm-italics">grouped convolutions</i>. Multi-head attention is simply the application of the same idea to self-attention. <a id="marker-1021128"></a><a id="marker-1021131"></a><a id="marker-1021133"></a></p>

  <h3 class="fm-head1" id="heading_id_16"><a id="pgfId-1021139"></a>11.4.3 The Transformer encoder</h3>

  <p class="body"><a id="pgfId-1021156"></a><a id="marker-1021150"></a><a id="marker-1021152"></a>If adding extra dense projections is so useful, why don’t we also apply one or two to the output of the attention mechanism? Actually, that’s a great idea—let’s do that. And our model is starting to do a lot, so we might want to add residual connections to make sure we don’t destroy any valuable information along the way—you learned in chapter 9 that they’re a must for any sufficiently deep architecture. And there’s another thing you learned in chapter 9: normalization layers are supposed to help gradients flow better during backpropagation. Let’s add those too.</p>

  <p class="body"><a id="pgfId-1021161"></a>That’s roughly the thought process that I imagine unfolded in the minds of the inventors of the Transformer architecture at the time. Factoring outputs into multiple independent spaces, adding residual connections, adding normalization layers—all of these are standard architecture patterns that one would be wise to leverage in any complex model. Together, these bells and whistles form the <i class="fm-italics">Transformer encoder</i>—one of two critical parts that make up the Transformer architecture (see figure 11.9).</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/11-09.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1059862"></a>Figure 11.9 The <code class="fm-code-in-text">TransformerEncoder</code> chains a <code class="fm-code-in-text">MultiHeadAttention</code> layer with a dense projection and adds normalization as well as residual connections.</p>

  <p class="body"><a id="pgfId-1021235"></a>The original Transformer architecture consists of two parts: a <i class="fm-italics">Transformer encoder</i> that processes the source sequence, and a <i class="fm-italics">Transformer decoder</i> that uses the source sequence to generate a translated version. You’ll learn about about the decoder part in a minute.</p>

  <p class="body"><a id="pgfId-1021244"></a>Crucially, the encoder part can be used for text classification—it’s a very generic module that ingests a sequence and learns to turn it into a more useful representation. Let’s implement a Transformer encoder and try it on the movie review sentiment classification task.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1021301"></a>Listing 11.21 Transformer encoder implemented as a subclassed <code class="fm-code-in-text">Layer</code></p>
  <pre class="programlisting"><a id="pgfId-1045254"></a><b class="fm-codebrown">import</b> tensorflow <b class="fm-codebrown">as</b> tf
<a id="pgfId-1045255"></a><b class="fm-codebrown">from</b> tensorflow <b class="fm-codebrown">import</b> keras
<a id="pgfId-1045256"></a><b class="fm-codebrown">from</b> tensorflow.keras <b class="fm-codebrown">import</b> layers
<a id="pgfId-1045257"></a>  
<a id="pgfId-1045258"></a><b class="fm-codebrown">class</b> TransformerEncoder(layers.Layer):
<a id="pgfId-1045259"></a>    <b class="fm-codebrown">def</b> __init__(self, embed_dim, dense_dim, num_heads, **kwargs):
<a id="pgfId-1045260"></a>        super().__init__(**kwargs)
<a id="pgfId-1021383"></a>        self.embed_dim = embed_dim                         <span class="fm-combinumeral">❶</span>
<a id="pgfId-1021395"></a>        self.dense_dim = dense_dim                         <span class="fm-combinumeral">❷</span>
<a id="pgfId-1021407"></a>        self.num_heads = num_heads                         <span class="fm-combinumeral">❸</span>
<a id="pgfId-1045301"></a>        self.attention = layers.MultiHeadAttention(
<a id="pgfId-1045302"></a>            num_heads=num_heads, key_dim=embed_dim)
<a id="pgfId-1045303"></a>        self.dense_proj = keras.Sequential(
<a id="pgfId-1045304"></a>            [layers.Dense(dense_dim, activation=<span class="fm-codegreen">"relu"</span>),
<a id="pgfId-1045305"></a>             layers.Dense(embed_dim),]
<a id="pgfId-1045306"></a>        )
<a id="pgfId-1045307"></a>        self.layernorm_1 = layers.LayerNormalization()
<a id="pgfId-1045308"></a>        self.layernorm_2 = layers.LayerNormalization()
<a id="pgfId-1021467"></a>    <b class="fm-codebrown">def</b> call(self, inputs, mask=<code class="fm-codegreen">None</code>):                    <span class="fm-combinumeral">❹</span>
<a id="pgfId-1021484"></a>        <b class="fm-codebrown">if</b> mask <b class="fm-codebrown">is</b> <b class="fm-codebrown">not</b> <code class="fm-codegreen">None</code>:                              <span class="fm-combinumeral">❺</span>
<a id="pgfId-1021496"></a>            mask = mask[:, tf.newaxis, :]                 <span class="fm-combinumeral">❺</span>
<a id="pgfId-1045405"></a>        attention_output = self.attention(
<a id="pgfId-1045406"></a>            inputs, inputs, attention_mask=mask)
<a id="pgfId-1045407"></a>        proj_input = self.layernorm_1(inputs + attention_output)
<a id="pgfId-1045408"></a>        proj_output = self.dense_proj(proj_input)
<a id="pgfId-1045409"></a>        <b class="fm-codebrown">return</b> self.layernorm_2(proj_input + proj_output)
<a id="pgfId-1045410"></a>  
<a id="pgfId-1021538"></a>    <b class="fm-codebrown">def</b> get_config(self):                                 <span class="fm-combinumeral">❻</span>
<a id="pgfId-1045423"></a>        config = super().get_config()
<a id="pgfId-1045424"></a>        config.update({
<a id="pgfId-1045425"></a>            <span class="fm-codegreen">"embed_dim"</span>: self.embed_dim,
<a id="pgfId-1045426"></a>            <span class="fm-codegreen">"num_heads"</span>: self.num_heads,
<a id="pgfId-1045427"></a>            <span class="fm-codegreen">"dense_dim"</span>: self.dense_dim,
<a id="pgfId-1045428"></a>        })
<a id="pgfId-1021591"></a>        <b class="fm-codebrown">return</b> config</pre>

  <p class="fm-code-annotation"><a id="pgfId-1054642"></a><span class="fm-combinumeral">❶</span> Size of the input token vectors</p>

  <p class="fm-code-annotation"><a id="pgfId-1054663"></a><span class="fm-combinumeral">❷</span> Size of the inner dense layer</p>

  <p class="fm-code-annotation"><a id="pgfId-1054680"></a><span class="fm-combinumeral">❸</span> Number of attention heads</p>

  <p class="fm-code-annotation"><a id="pgfId-1054697"></a><span class="fm-combinumeral">❹</span> Computation goes in call().</p>

  <p class="fm-code-annotation"><a id="pgfId-1054714"></a><span class="fm-combinumeral">❺</span> The mask that will be generated by the Embedding layer will be 2D, but the attention layer expects to be 3D or 4D, so we expand its rank.</p>

  <p class="fm-code-annotation"><a id="pgfId-1054731"></a><span class="fm-combinumeral">❻</span> Implement serialization so we can save the model.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title"><a class="calibre11" id="pgfId-1022874"></a>Saving custom layers</p>

    <p class="fm-sidebar-text"><a id="pgfId-1022897"></a>When you write custom layers, make sure to implement <a id="marker-1049003"></a>the <code class="fm-code-in-text1">get_config</code> method: this enables the layer to be reinstantiated from its config dict, which is useful during model saving and loading. The method should return a Python dict that contains the values of the constructor arguments used to create the layer.</p>

    <p class="fm-sidebar-text"><a id="pgfId-1022906"></a>All Keras layers can be serialized and deserialized as follows:</p>
    <pre class="programlisting"><a id="pgfId-1022912"></a>config = layer.get_config()
<a id="pgfId-1022930"></a>new_layer = layer.__class__.from_config(config)   <span class="fm-combinumeral">❶</span></pre>

    <p class="fm-code-annotation"><a id="pgfId-1054580"></a><span class="fm-combinumeral">❶</span> The config does not contain weight values, so all weights in the layer get initialized from scratch.</p>

    <p class="fm-sidebar-text"><a id="pgfId-1022962"></a>For instance:</p>
    <pre class="programlisting"><a id="pgfId-1022968"></a>layer = PositionalEmbedding(sequence_length, input_dim, output_dim)
<a id="pgfId-1022986"></a>config = layer.get_config()
<a id="pgfId-1022992"></a>new_layer = PositionalEmbedding.from_config(config)</pre>

    <p class="fm-sidebar-text"><a id="pgfId-1022998"></a>When saving a model that contains custom layers, the savefile will contain these config dicts. When loading the model from the file, you should provide the custom layer classes to the loading process, so that it can make sense of the config objects:</p>
    <pre class="programlisting"><a id="pgfId-1045774"></a>model = keras.models.load_model(
<a id="pgfId-1023022"></a>    filename, custom_objects={<span class="fm-codegreen">"PositionalEmbedding"</span>: PositionalEmbedding})<a id="marker-1058121"></a><a id="marker-1058122"></a></pre>
  </div>

  <p class="body"><a id="pgfId-1021729"></a>You’ll note that the normalization layers we’re using here aren’t <code class="fm-code-in-text">BatchNormalization</code> layers like <a id="marker-1021708"></a>those we’ve used before in image models. That’s because <code class="fm-code-in-text">BatchNormalization</code> doesn’t work well for sequence data. Instead, we’re using the <code class="fm-code-in-text">LayerNormalization</code> layer, which normalizes <a id="marker-1021734"></a>each sequence independently from other sequences in the batch. Like this, in NumPy-like pseudocode:</p>
  <pre class="programlisting"><a id="pgfId-1021744"></a><b class="fm-codebrown">def</b> layer_normalization(batch_of_sequences):                       <span class="fm-combinumeral">❶</span>
<a id="pgfId-1021764"></a>    mean = np.mean(batch_of_sequences, keepdims=<code class="fm-codegreen">True</code>, axis=-<span class="fm-codeblue">1</span>)     <span class="fm-combinumeral">❷</span>
<a id="pgfId-1021776"></a>    variance = np.var(batch_of_sequences, keepdims=<code class="fm-codegreen">True</code>, axis=-<span class="fm-codeblue">1</span>)  <span class="fm-combinumeral">❷</span>
<a id="pgfId-1021788"></a>    <b class="fm-codebrown">return</b> (batch_of_sequences - mean) / variance</pre>

  <p class="fm-code-annotation"><a id="pgfId-1054439"></a><span class="fm-combinumeral">❶</span> Input shape: (batch_size, sequence_length, embedding_dim)</p>

  <p class="fm-code-annotation"><a id="pgfId-1054460"></a><span class="fm-combinumeral">❷</span> To compute mean and variance, we only pool data over the last axis (axis -1).</p>

  <p class="body"><a id="pgfId-1021830"></a>Compare to <code class="fm-code-in-text">BatchNormalization</code> (during training):</p>
  <pre class="programlisting"><a id="pgfId-1021845"></a><b class="fm-codebrown">def</b> batch_normalization(batch_of_images):                              <span class="fm-combinumeral">❶</span>
<a id="pgfId-1021865"></a>    mean = np.mean(batch_of_images, keepdims=<code class="fm-codegreen">True</code>, axis=(<span class="fm-codeblue">0</span>, <span class="fm-codeblue">1</span>, <span class="fm-codeblue">2</span>))     <span class="fm-combinumeral">❷</span>
<a id="pgfId-1021877"></a>    variance = np.var(batch_of_images, keepdims=<code class="fm-codegreen">True</code>, axis=(<span class="fm-codeblue">0</span>, <span class="fm-codeblue">1</span>, <span class="fm-codeblue">2</span>))  <span class="fm-combinumeral">❷</span>
<a id="pgfId-1021889"></a>    <b class="fm-codebrown">return</b> (batch_of_images - mean) / variance</pre>

  <p class="fm-code-annotation"><a id="pgfId-1054289"></a><span class="fm-combinumeral">❶</span> Input shape: (batch_size, height, width, channels)</p>

  <p class="fm-code-annotation"><a id="pgfId-1054306"></a><span class="fm-combinumeral">❷</span> Pool data over the batch axis (axis 0), which creates interactions between samples in a batch.</p>

  <p class="body"><a id="pgfId-1021947"></a>While <code class="fm-code-in-text">BatchNormalization</code> collects information from many samples to obtain accurate statistics for the feature means and variances, <code class="fm-code-in-text">LayerNormalization</code> pools data within each sequence separately, which is more appropriate for sequence data.</p>

  <p class="body"><a id="pgfId-1021956"></a>Now that we’ve implemented our <code class="fm-code-in-text">TransformerEncoder</code>, we can use it to assemble a text-classification model similar to the GRU-based one you’ve seen previously.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1022022"></a>Listing 11.22 Using the Transformer encoder for text classification</p>
  <pre class="programlisting"><a id="pgfId-1045557"></a>vocab_size = <span class="fm-codeblue">20000</span> 
<a id="pgfId-1045558"></a>embed_dim = <span class="fm-codeblue">256</span> 
<a id="pgfId-1045559"></a>num_heads = <span class="fm-codeblue">2</span> 
<a id="pgfId-1045560"></a>dense_dim = <span class="fm-codeblue">32</span> 
<a id="pgfId-1045561"></a>  
<a id="pgfId-1045562"></a>inputs = keras.Input(shape=(<code class="fm-codegreen">None</code>,), dtype=<span class="fm-codegreen">"int64"</span>)
<a id="pgfId-1045563"></a>x = layers.Embedding(vocab_size, embed_dim)(inputs)
<a id="pgfId-1045564"></a>x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)
<a id="pgfId-1022102"></a>x = layers.GlobalMaxPooling1D()(x)                          <span class="fm-combinumeral">❶</span>
<a id="pgfId-1045581"></a>x = layers.Dropout(<span class="fm-codeblue">0.5</span>)(x)
<a id="pgfId-1045582"></a>outputs = layers.Dense(<span class="fm-codeblue">1</span>, activation=<span class="fm-codegreen">"sigmoid"</span>)(x)
<a id="pgfId-1045583"></a>model = keras.Model(inputs, outputs)
<a id="pgfId-1045584"></a>model.compile(optimizer=<span class="fm-codegreen">"rmsprop"</span>,
<a id="pgfId-1045585"></a>              loss=<span class="fm-codegreen">"binary_crossentropy"</span>,
<a id="pgfId-1045586"></a>              metrics=[<span class="fm-codegreen">"accuracy"</span>])
<a id="pgfId-1022150"></a>model.summary()</pre>

  <p class="fm-code-annotation"><a id="pgfId-1054210"></a><span class="fm-combinumeral">❶</span> Since TransformerEncoder returns full sequences, we need to reduce each sequence to a single vector for classification via a global pooling layer.</p>

  <p class="body"><a id="pgfId-1022176"></a>Let’s train it. It gets to 87.5% test accuracy—slightly worse than the GRU model.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1022233"></a>Listing 11.23 Training and evaluating the Transformer encoder based model</p>
  <pre class="programlisting"><a id="pgfId-1045601"></a>callbacks = [
<a id="pgfId-1045602"></a>    keras.callbacks.ModelCheckpoint(<span class="fm-codegreen">"transformer_encoder.keras"</span>,
<a id="pgfId-1045603"></a>                                    save_best_only=<code class="fm-codegreen">True</code>)
<a id="pgfId-1045604"></a>]
<a id="pgfId-1045605"></a>model.fit(int_train_ds, validation_data=int_val_ds, epochs=<span class="fm-codeblue">20</span>,
<a id="pgfId-1050171"></a>          callbacks=callbacks)
<a id="pgfId-1045606"></a>model = keras.models.load_model(
<a id="pgfId-1045607"></a>    <span class="fm-codegreen">"transformer_encoder.keras"</span>,
<a id="pgfId-1022308"></a>    custom_objects={<span class="fm-codegreen">"TransformerEncoder"</span>: TransformerEncoder})   <span class="fm-combinumeral">❶</span>
<a id="pgfId-1022320"></a><b class="fm-codebrown">print</b>(f<span class="fm-codegreen">"Test acc: {model.evaluate(int_test_ds)[1]:.3f}"</span>)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1054149"></a><span class="fm-combinumeral">❶</span> Provide the custom TransformerEncoder class to the model-loading process.</p>

  <p class="body"><a id="pgfId-1022346"></a>At this point, you should start to feel a bit uneasy. Something’s off here. Can you tell what it is?</p>

  <p class="body"><a id="pgfId-1022352"></a>This section is ostensibly about “sequence models.” I started off by highlighting the importance of word order. I said that Transformer was a sequence-processing architecture, originally developed for machine translation. And yet . . . the Transformer encoder you just saw in action wasn’t a sequence model at all. Did you notice? It’s composed of dense layers that process sequence tokens independently from each other, and an attention layer that looks at the tokens <i class="fm-italics">as a set</i>. You could change the order of the tokens in a sequence, and you’d get the exact same pairwise attention scores and the exact same context-aware representations. If you were to completely scramble the words in every movie review, the model wouldn’t notice, and you’d still get the exact same accuracy. Self-attention is a set-processing mechanism, focused on the relationships between pairs of sequence elements (see figure 11.10)—it’s blind to whether these elements occur at the beginning, at the end, or in the middle of a sequence. So why do we say that Transformer is a sequence model? And how could it possibly be good for machine translation if it doesn’t look at word order?</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/11-10.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1059911"></a>Figure 11.10 Features of different types of NLP models</p>

  <p class="body"><a id="pgfId-1022377"></a>I hinted at the solution earlier in the chapter: I mentioned in passing that Transformer was a hybrid approach that is technically order-agnostic, but that manually injects order information in the representations it processes. This is the missing ingredient! It’s called <i class="fm-italics">positional encoding</i>. Let’s take a look.</p>

  <p class="fm-head2"><a id="pgfId-1022406"></a>Using positional encoding to re-inject order information</p>

  <p class="body"><a id="pgfId-1022416"></a><a id="marker-1022417"></a>The idea behind positional encoding is very simple: to give the model access to word-order information, we’re going to add the word’s position in the sentence to each word embedding. Our input word embeddings will have two components: the usual word vector, which represents the word independently of any specific context, and a position vector, which represents the position of the word in the current sentence. Hopefully, the model will then figure out how to best leverage this additional information.</p>

  <p class="body"><a id="pgfId-1022425"></a>The simplest scheme you could come up with would be to concatenate the word’s position to its embedding vector. You’d add a “position” axis to the vector and fill it with 0 for the first word in the sequence, 1 for the second, and so on.</p>

  <p class="body"><a id="pgfId-1022431"></a>That may not be ideal, however, because the positions can potentially be very large integers, which will disrupt the range of values in the embedding vector. As you know, neural networks don’t like very large input values, or discrete input distributions.</p>

  <p class="body"><a id="pgfId-1022437"></a>The original “Attention is all you need” paper used an interesting trick to encode word positions: it added to the word embeddings a vector containing values in the range <code class="fm-code-in-text">[-1,</code> <code class="fm-code-in-text">1]</code> that varied cyclically depending on the position (it used cosine functions to achieve this). This trick offers a way to uniquely characterize any integer in a large range via a vector of small values. It’s clever, but it’s not what we’re going to use in our case. We’ll do something simpler and more effective: we’ll learn position-embedding vectors the same way we learn to embed word indices. We’ll then proceed to add our position embeddings to the corresponding word embeddings, to obtain a position-aware word embedding. This technique is called “positional embedding.” Let’s implement it.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1022503"></a>Listing 11.24 Implementing positional embedding as a subclassed layer</p>
  <pre class="programlisting"><a id="pgfId-1045638"></a><b class="fm-codebrown">class</b> PositionalEmbedding(layers.Layer):
<a id="pgfId-1022542"></a>    <b class="fm-codebrown">def</b> __init__(self, sequence_length, input_dim, output_dim, **kwargs):  <span class="fm-combinumeral">❶</span>
<a id="pgfId-1045657"></a>        super().__init__(**kwargs)
<a id="pgfId-1022560"></a>        self.token_embeddings = layers.Embedding(                          <span class="fm-combinumeral">❷</span>
<a id="pgfId-1045674"></a>            input_dim=input_dim, output_dim=output_dim)
<a id="pgfId-1045675"></a>        self.position_embeddings = layers.Embedding(
<a id="pgfId-1022584"></a>            input_dim=sequence_length, output_dim=output_dim)              <span class="fm-combinumeral">❸</span>
<a id="pgfId-1045688"></a>        self.sequence_length = sequence_length
<a id="pgfId-1045689"></a>        self.input_dim = input_dim
<a id="pgfId-1045690"></a>        self.output_dim = output_dim
<a id="pgfId-1045691"></a>  
<a id="pgfId-1045692"></a>    <b class="fm-codebrown">def</b> call(self, inputs):
<a id="pgfId-1045693"></a>        length = tf.shape(inputs)[-<span class="fm-codeblue">1</span>]
<a id="pgfId-1045694"></a>        positions = tf.range(start=<span class="fm-codeblue">0</span>, limit=length, delta=<span class="fm-codeblue">1</span>)
<a id="pgfId-1045695"></a>        embedded_tokens = self.token_embeddings(inputs)
<a id="pgfId-1045696"></a>        embedded_positions = self.position_embeddings(positions)
<a id="pgfId-1022649"></a>        <b class="fm-codebrown">return</b> embedded_tokens + embedded_positions                        <span class="fm-combinumeral">❹</span>
<a id="pgfId-1022666"></a> 
<a id="pgfId-1022661"></a>    <b class="fm-codebrown">def</b> compute_mask(self, inputs, mask=<code class="fm-codegreen">None</code>):                             <span class="fm-combinumeral">❺</span>
<a id="pgfId-1022678"></a>        <b class="fm-codebrown">return</b> tf.math.not_equal(inputs, <span class="fm-codeblue">0</span>)                                <span class="fm-combinumeral">❺</span>
<a id="pgfId-1022695"></a> 
<a id="pgfId-1022690"></a>    <b class="fm-codebrown">def</b> get_config(self):                                                  <span class="fm-combinumeral">❻</span>
<a id="pgfId-1045754"></a>        config = super().get_config()
<a id="pgfId-1045755"></a>        config.update({
<a id="pgfId-1045756"></a>            <span class="fm-codegreen">"output_dim"</span>: self.output_dim,
<a id="pgfId-1045757"></a>            <span class="fm-codegreen">"sequence_length"</span>: self.sequence_length,
<a id="pgfId-1045758"></a>            <span class="fm-codegreen">"input_dim"</span>: self.input_dim,
<a id="pgfId-1045759"></a>        })
<a id="pgfId-1022743"></a>        <b class="fm-codebrown">return</b> config</pre>

  <p class="fm-code-annotation"><a id="pgfId-1053656"></a><span class="fm-combinumeral">❶</span> A downside of position embeddings is that the sequence length needs to be known in advance.</p>

  <p class="fm-code-annotation"><a id="pgfId-1053677"></a><span class="fm-combinumeral">❷</span> Prepare an Embedding layer for the token indices.</p>

  <p class="fm-code-annotation"><a id="pgfId-1053694"></a><span class="fm-combinumeral">❸</span> And another one for the token positions</p>

  <p class="fm-code-annotation"><a id="pgfId-1053711"></a><span class="fm-combinumeral">❹</span> Add both embedding vectors together.</p>

  <p class="fm-code-annotation"><a id="pgfId-1053733"></a><span class="fm-combinumeral">❺</span> Like the Embedding layer, this layer should be able to generate a mask so we can ignore padding 0s in the inputs. The compute_mask method will called automatically by the framework, and the mask will get propagated to the next layer.</p>

  <p class="fm-code-annotation"><a id="pgfId-1053757"></a><span class="fm-combinumeral">❻</span> Implement serialization so we can save the model.</p>

  <p class="body"><a id="pgfId-1022865"></a>You would use this <code class="fm-code-in-text">PositionEmbedding</code> layer just like a regular <code class="fm-code-in-text">Embedding</code> layer. Let’s see it in action!</p>

  <p class="fm-head2"><a id="pgfId-1023030"></a>Putting it all together: A text-classification Transformer</p>

  <p class="body"><a id="pgfId-1023040"></a>All you have to do to start taking word order into account is swap the old <code class="fm-code-in-text">Embedding</code> layer with our position-aware version<a id="marker-1050427"></a>.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1023106"></a>Listing 11.25 Combining the Transformer encoder with positional embedding</p>
  <pre class="programlisting"><a id="pgfId-1045803"></a>vocab_size = <span class="fm-codeblue">20000</span> 
<a id="pgfId-1045804"></a>sequence_length = <span class="fm-codeblue">600</span> 
<a id="pgfId-1045805"></a>embed_dim = <span class="fm-codeblue">256</span> 
<a id="pgfId-1045806"></a>num_heads = <span class="fm-codeblue">2</span> 
<a id="pgfId-1045807"></a>dense_dim = <span class="fm-codeblue">32</span> 
<a id="pgfId-1045808"></a>  
<a id="pgfId-1045809"></a>inputs = keras.Input(shape=(<code class="fm-codegreen">None</code>,), dtype=<span class="fm-codegreen">"int64"</span>)
<a id="pgfId-1023180"></a>x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)   <span class="fm-combinumeral">❶</span>
<a id="pgfId-1045826"></a>x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)
<a id="pgfId-1045827"></a>x = layers.GlobalMaxPooling1D()(x)
<a id="pgfId-1045828"></a>x = layers.Dropout(<span class="fm-codeblue">0.5</span>)(x)
<a id="pgfId-1045829"></a>outputs = layers.Dense(<span class="fm-codeblue">1</span>, activation=<span class="fm-codegreen">"sigmoid"</span>)(x)
<a id="pgfId-1045830"></a>model = keras.Model(inputs, outputs)
<a id="pgfId-1045831"></a>model.compile(optimizer=<span class="fm-codegreen">"rmsprop"</span>,
<a id="pgfId-1045832"></a>              loss=<span class="fm-codegreen">"binary_crossentropy"</span>,
<a id="pgfId-1045833"></a>              metrics=[<span class="fm-codegreen">"accuracy"</span>])
<a id="pgfId-1045834"></a>model.summary()
<a id="pgfId-1045835"></a>  
<a id="pgfId-1045836"></a>callbacks = [
<a id="pgfId-1045837"></a>    keras.callbacks.ModelCheckpoint(<span class="fm-codegreen">"full_transformer_encoder.keras"</span>,
<a id="pgfId-1045838"></a>                                    save_best_only=<code class="fm-codegreen">True</code>)
<a id="pgfId-1045839"></a>] 
<a id="pgfId-1045840"></a>model.fit(int_train_ds, validation_data=int_val_ds, epochs=<span class="fm-codeblue">20</span>, callbacks=callbacks)
<a id="pgfId-1045841"></a>model = keras.models.load_model(
<a id="pgfId-1045842"></a>    <span class="fm-codegreen">"full_transformer_encoder.keras"</span>,
<a id="pgfId-1045843"></a>    custom_objects={<span class="fm-codegreen">"TransformerEncoder"</span>: TransformerEncoder,
<a id="pgfId-1045844"></a>                    <span class="fm-codegreen">"PositionalEmbedding"</span>: PositionalEmbedding}) 
<a id="pgfId-1023305"></a><b class="fm-codebrown">print</b>(f<span class="fm-codegreen">"Test acc: {model.evaluate(int_test_ds)[1]:.3f}"</span>)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1053603"></a><span class="fm-combinumeral">❶</span> Look here!</p>

  <p class="body"><a id="pgfId-1023331"></a>We get to 88.3% test accuracy, a solid improvement that clearly demonstrates the value of word order information for text classification. This is our best sequence model so far! However, it’s still one notch below the bag-of-words approach. <a id="marker-1023333"></a><a id="marker-1023336"></a></p>

  <h3 class="fm-head1" id="heading_id_17"><a id="pgfId-1023342"></a>11.4.4 When to use sequence models over bag-of-words models</h3>

  <p class="body"><a id="pgfId-1023369"></a><a id="marker-1023353"></a><a id="marker-1023355"></a><a id="marker-1023357"></a><a id="marker-1023359"></a>You may sometimes hear that bag-of-words methods are outdated, and that Transformer-based sequence models are the way to go, no matter what task or dataset you’re looking at. This is definitely not the case: a small stack of <code class="fm-code-in-text">Dense</code> layers on top of a bag-of-bigrams remains a perfectly valid and relevant approach in many cases. In fact, among the various techniques that we’ve tried on the IMDB dataset throughout this chapter, the best performing so far was the bag-of-bigrams!</p>

  <p class="body"><a id="pgfId-1023378"></a>So, when should you prefer one approach over the other?</p>

  <p class="body"><a id="pgfId-1023384"></a>In 2017, my team and I ran a systematic analysis of the performance of various text-classification techniques across many different types of text datasets, and we discovered a remarkable and surprising rule of thumb for deciding whether to go with a bag-of-words model or a sequence model (<span class="fm-hyperlink"><a class="url" href="https://mng.bz/AOzK">http://mng.bz/AOzK</a></span>)—a golden constant of sorts.</p>

  <p class="body"><a id="pgfId-1023391"></a>It turns out that when approaching a new text-classification task, you should pay close attention to the ratio between the number of samples in your training data and the mean number of words per sample (see figure 11.11). If that ratio is small—less than 1,500—then the bag-of-bigrams model will perform better (and as a bonus, it will be much faster to train and to iterate on too). If that ratio is higher than 1,500, then you should go with a sequence model. In other words, sequence models work best when lots of training data is available and when each sample is relatively short.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/11-11.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1059953"></a>Figure 11.11 A simple heuristic for selecting a text-classification model: the ratio between the number of training samples and the mean number of words per sample</p>

  <p class="body"><a id="pgfId-1023407"></a>So if you’re classifying 1,000-word long documents, and you have 100,000 of them (a ratio of 100), you should go with a bigram model. If you’re classifying tweets that are 40 words long on average, and you have 50,000 of them (a ratio of 1,250), you should also go with a bigram model. But if you increase your dataset size to 500,000 tweets (a ratio of 12,500), go with a Transformer encoder. What about the IMDB movie review classification task? We had 20,000 training samples and an average word count of 233, so our rule of thumb points toward a bigram model, which confirms what we found in practice.</p>

  <p class="body"><a id="pgfId-1023427"></a>This intuitively makes sense: the input of a sequence model represents a richer and more complex space, and thus it takes more data to map out that space; meanwhile, a plain set of terms is a space so simple that you can train a logistic regression on top using just a few hundreds or thousands of samples. In addition, the shorter a sample is, the less the model can afford to discard any of the information it contains—in particular, word order becomes more important, and discarding it can create ambiguity. The sentences “this movie is the bomb” and “this movie was a bomb” have very close unigram representations, which could confuse a bag-of-words model, but a sequence model could tell which one is negative and which one is positive. With a longer sample, word statistics would become more reliable and the topic or sentiment would be more apparent from the word histogram alone.</p>

  <p class="body"><a id="pgfId-1023433"></a>Now, keep in mind that this heuristic rule was developed specifically for text classification. It may not necessarily hold for other NLP tasks—when it comes to machine translation, for instance, Transformer shines especially for very long sequences, compared to RNNs. Our heuristic is also just a rule of thumb, rather than a scientific law, so expect it to work most of the time, but not necessarily every time. <a id="marker-1049171"></a><a id="marker-1049172"></a><a id="marker-1049173"></a><a id="marker-1049174"></a><a id="marker-1049176"></a><a id="marker-1049177"></a></p>

  <h2 class="fm-head" id="heading_id_18"><a id="pgfId-1023454"></a>11.5 Beyond text classification: Sequence-to-sequence learning</h2>

  <p class="body"><a id="pgfId-1023483"></a><a id="marker-1050311"></a><a id="marker-1050312"></a><a id="marker-1050313"></a><a id="marker-1050314"></a>You now possess all of the tools you will need to tackle most natural language processing tasks. However, you’ve only seen these tools in action on a single problem: text classification. This is an extremely popular use case, but there’s a lot more to NLP than classification. In this section, you’ll deepen your expertise by learning about <i class="fm-italics">sequence-to-sequence models</i>.</p>

  <p class="body"><a id="pgfId-1023492"></a>A sequence-to-sequence model takes a sequence as input (often a sentence or paragraph) and translates it into a different sequence. This is the task at the heart of many of the most successful applications of NLP:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1023498"></a><i class="fm-italics1">Machine translation</i>—Convert a <a class="calibre11" id="marker-1023515"></a>paragraph in a source language to its equivalent in a target language.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1023525"></a><i class="fm-italics1">Text summarization</i>—Convert a <a class="calibre11" id="marker-1023538"></a>long document to a shorter version that retains the most important information.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1023548"></a><i class="fm-italics1">Question answering</i>—Convert an input question into its answer.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1023565"></a><i class="fm-italics1">Chatbots</i>—Convert a <a class="calibre11" id="marker-1023578"></a>dialogue prompt into a reply to this prompt, or convert the history of a conversation into the next reply in the conversation.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1023588"></a><i class="fm-italics1">Text generation</i>—Convert a <a class="calibre11" id="marker-1023601"></a>text prompt into a paragraph that completes the prompt.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1023611"></a>Etc.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1023621"></a>The general template behind sequence-to-sequence models is described in figure 11.12. During training,</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1023627"></a>An <i class="fm-italics1">encoder</i> model turns <a class="calibre11" id="marker-1041300"></a>the source sequence into an intermediate representation.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1023702"></a>A <i class="fm-italics1">decoder</i> is trained <a class="calibre11" id="marker-1041302"></a>to predict the next token <code class="fm-code-in-text">i</code> in the target sequence by looking at both previous tokens (<code class="fm-code-in-text">0</code> to <code class="fm-code-in-text">i</code> <code class="fm-code-in-text">-</code> <code class="fm-code-in-text">1</code>) and the encoded source sequence.</p>
    </li>
  </ul>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/11-12.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1059999"></a>Figure 11.12 Sequence-to-sequence learning: the source sequence is processed by the encoder and is then sent to the decoder. The decoder looks at the target sequence so far and predicts the target sequence offset by one step in the future. During inference, we generate one target token at a time and feed it back into the decoder.</p>

  <p class="body"><a id="pgfId-1023721"></a>During inference, we don’t have access to the target sequence—we’re trying to predict it from scratch. We’ll have to generate it one token at a time:</p>

  <ol class="calibre14">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1023741"></a>We obtain the encoded source sequence from the encoder.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1023755"></a>The decoder starts by looking at the encoded source sequence as well as an initial “seed” token (such as the string <code class="fm-code-in-text">"[start]"</code>), and uses them to predict the first real token in the sequence.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1023774"></a>The predicted sequence so far is fed back into the decoder, which generates the next token, and so on, until it generates a stop token (such as the string <code class="fm-code-in-text">"[end]"</code>).</p>
    </li>
  </ol>

  <p class="body"><a id="pgfId-1023793"></a>Everything you’ve learned so far can be repurposed to build this new kind of model. Let’s dive in.</p>

  <h3 class="fm-head1" id="heading_id_19"><a id="pgfId-1023799"></a>11.5.1 A machine translation example</h3>

  <p class="body"><a id="pgfId-1023816"></a><a id="marker-1023810"></a><a id="marker-1023812"></a>We’ll demonstrate sequence-to-sequence modeling on a machine translation task. Machine translation is precisely what Transformer was developed for! We’ll start with a recurrent sequence model, and we’ll follow up with the full Transformer architecture.</p>

  <p class="body"><a id="pgfId-1023821"></a>We’ll be working with an English-to-Spanish translation dataset available at <span class="fm-hyperlink"><a class="url" href="https://www.manythings.org/anki/">www.manythings.org/anki/</a></span>. Let’s download it:</p>
  <pre class="programlisting"><a id="pgfId-1023827"></a>!wget http:/ /storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip
<a id="pgfId-1023841"></a>!unzip -q spa-eng.zip</pre>

  <p class="body"><a id="pgfId-1023847"></a>The text file contains one example per line: an English sentence, followed by a tab character, followed by the corresponding Spanish sentence. Let’s parse this file.</p>
  <pre class="programlisting"><a id="pgfId-1045859"></a>text_file = <span class="fm-codegreen">"spa-eng/spa.txt"</span> 
<a id="pgfId-1045860"></a><b class="fm-codebrown">with</b> open(text_file) <b class="fm-codebrown">as</b> f:
<a id="pgfId-1045861"></a>    lines = f.read().split(<span class="fm-codegreen">"\n"</span>)[:-<span class="fm-codeblue">1</span>]
<a id="pgfId-1045862"></a>text_pairs = [] 
<a id="pgfId-1023885"></a><b class="fm-codebrown">for</b> line <b class="fm-codebrown">in</b> lines:                              <span class="fm-combinumeral">❶</span>
<a id="pgfId-1023897"></a>    english, spanish = line.split(<span class="fm-codegreen">"\t"</span>)         <span class="fm-combinumeral">❷</span>
<a id="pgfId-1023909"></a>    spanish = <span class="fm-codegreen">"[start] "</span> + spanish + <span class="fm-codegreen">" [end]"</span>   <span class="fm-combinumeral">❸</span>
<a id="pgfId-1023921"></a>    text_pairs.append((english, spanish))</pre>

  <p class="fm-code-annotation"><a id="pgfId-1053433"></a><span class="fm-combinumeral">❶</span> Iterate over the lines in the file.</p>

  <p class="fm-code-annotation"><a id="pgfId-1053456"></a><span class="fm-combinumeral">❷</span> Each line contains an English phrase and its Spanish translation, tab-separated.</p>

  <p class="fm-code-annotation"><a id="pgfId-1053477"></a><span class="fm-combinumeral">❸</span> We prepend "[start]" and append "[end]" to the Spanish sentence, to match the template from figure 11.12.</p>

  <p class="body"><a id="pgfId-1023979"></a>Our <code class="fm-code-in-text">text_pairs</code> look like this:</p>
  <pre class="programlisting"><a id="pgfId-1049219"></a>&gt;&gt;&gt; <b class="fm-codebrown">import</b> random
<a id="pgfId-1024008"></a>&gt;&gt;&gt; <b class="fm-codebrown">print</b>(random.choice(text_pairs))
<a id="pgfId-1045917"></a>("Soccer is more popular than tennis.",
<a id="pgfId-1024020"></a> "[start] El fútbol es más popular que el tenis. [end]")</pre>

  <p class="body"><a id="pgfId-1024026"></a>Let’s shuffle them and split them into the usual training, validation, and test sets:</p>
  <pre class="programlisting"><a id="pgfId-1045934"></a><b class="fm-codebrown">import</b> random
<a id="pgfId-1045935"></a>random.shuffle(text_pairs)
<a id="pgfId-1045936"></a>num_val_samples = int(<span class="fm-codeblue">0.15</span> * len(text_pairs))
<a id="pgfId-1045937"></a>num_train_samples = len(text_pairs) - <span class="fm-codeblue">2</span> * num_val_samples
<a id="pgfId-1045938"></a>train_pairs = text_pairs[:num_train_samples]
<a id="pgfId-1045939"></a>val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]
<a id="pgfId-1024076"></a>test_pairs = text_pairs[num_train_samples + num_val_samples:]</pre>

  <p class="body"><a id="pgfId-1024082"></a>Next, let’s prepare two separate <code class="fm-code-in-text">TextVectorization</code> layers: one for English and one for Spanish. We’re going to need to customize the way strings are preprocessed:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1024151"></a>We need to preserve the <code class="fm-code-in-text">"[start]"</code> and <code class="fm-code-in-text">"[end]"</code> tokens that we’ve inserted. By default, the characters <code class="fm-code-in-text">[</code> and <code class="fm-code-in-text">]</code> would be stripped, but we want to keep them around so we can tell apart the word “start” and the start token <code class="fm-code-in-text">"[start]"</code>.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1024180"></a>Punctuation is different from language to language! In the Spanish <code class="fm-code-in-text">TextVectorization</code> layer, if we’re going to strip punctuation characters, we need to also strip the character <code class="fm-code-in-text">¿</code>.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1024189"></a>Note that for a non-toy translation model, we would treat punctuation characters as separate tokens rather than stripping them, since we would want to be able to generate correctly punctuated sentences. In our case, for simplicity, we’ll get rid of all punctuation.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1024246"></a>Listing 11.26 Vectorizing the English and Spanish text pairs</p>
  <pre class="programlisting"><a id="pgfId-1045954"></a><b class="fm-codebrown">import</b> tensorflow <b class="fm-codebrown">as</b> tf 
<a id="pgfId-1045955"></a><b class="fm-codebrown">import</b> string
<a id="pgfId-1045956"></a><b class="fm-codebrown">import</b> re
<a id="pgfId-1045957"></a>  
<a id="pgfId-1024297"></a>strip_chars = string.punctuation + <span class="fm-codegreen">"¿"</span>                  <span class="fm-combinumeral">❶</span>
<a id="pgfId-1024314"></a>strip_chars = strip_chars.replace(<span class="fm-codegreen">"["</span>, <span class="fm-codegreen">""</span>)              <span class="fm-combinumeral">❶</span>
<a id="pgfId-1024326"></a>strip_chars = strip_chars.replace(<span class="fm-codegreen">"]"</span>, <span class="fm-codegreen">""</span>)              <span class="fm-combinumeral">❶</span>
<a id="pgfId-1024343"></a> 
<a id="pgfId-1024338"></a><b class="fm-codebrown">def</b> custom_standardization(input_string):               <span class="fm-combinumeral">❶</span>
<a id="pgfId-1024355"></a>    lowercase = tf.strings.lower(input_string)          <span class="fm-combinumeral">❶</span>
<a id="pgfId-1024367"></a>    <b class="fm-codebrown">return</b> tf.strings.regex_replace(                    <span class="fm-combinumeral">❶</span>
<a id="pgfId-1024379"></a>        lowercase, f<span class="fm-codegreen">"[{re.escape(strip_chars)}]"</span>, <span class="fm-codegreen">""</span>)   <span class="fm-combinumeral">❶</span>
<a id="pgfId-1024385"></a>vocab_size = <span class="fm-codeblue">15000</span>                                      <span class="fm-combinumeral">❷</span>
<a id="pgfId-1024402"></a>sequence_length = <span class="fm-codeblue">20</span>                                    <span class="fm-combinumeral">❷</span>
<a id="pgfId-1024419"></a> 
<a id="pgfId-1024414"></a>source_vectorization = layers.TextVectorization(        <span class="fm-combinumeral">❸</span>
<a id="pgfId-1046097"></a>    max_tokens=vocab_size,
<a id="pgfId-1046098"></a>    output_mode=<span class="fm-codegreen">"int"</span>,
<a id="pgfId-1046099"></a>    output_sequence_length=sequence_length,
<a id="pgfId-1046100"></a>)
<a id="pgfId-1024455"></a>target_vectorization = layers.TextVectorization(        <span class="fm-combinumeral">❹</span>
<a id="pgfId-1046113"></a>    max_tokens=vocab_size,
<a id="pgfId-1046114"></a>    output_mode=<span class="fm-codegreen">"int"</span>,
<a id="pgfId-1024479"></a>    output_sequence_length=sequence_length + <span class="fm-codeblue">1</span>,         <span class="fm-combinumeral">❺</span>
<a id="pgfId-1046127"></a>    standardize=custom_standardization,
<a id="pgfId-1046128"></a>)
<a id="pgfId-1046129"></a>train_english_texts = [pair[<span class="fm-codeblue">0</span>] <b class="fm-codebrown">for</b> pair <b class="fm-codebrown">in</b> train_pairs]
<a id="pgfId-1046130"></a>train_spanish_texts = [pair[<span class="fm-codeblue">1</span>] <b class="fm-codebrown">for</b> pair <b class="fm-codebrown">in</b> train_pairs]
<a id="pgfId-1024515"></a>source_vectorization.adapt(train_english_texts)         <span class="fm-combinumeral">❻</span>
<a id="pgfId-1024527"></a>target_vectorization.adapt(train_spanish_texts)         <span class="fm-combinumeral">❻</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1052909"></a><span class="fm-combinumeral">❶</span> Prepare a custom string standardization function for the Spanish TextVectorization layer: it preserves [ and ] but strips ¿ (as well as all other characters from strings.punctuation).</p>

  <p class="fm-code-annotation"><a id="pgfId-1052930"></a><span class="fm-combinumeral">❷</span> To keep things simple, we’ll only look at the top 15,000 words in each language, and we’ll restrict sentences to 20 words.</p>

  <p class="fm-code-annotation"><a id="pgfId-1052947"></a><span class="fm-combinumeral">❸</span> The English layer</p>

  <p class="fm-code-annotation"><a id="pgfId-1052964"></a><span class="fm-combinumeral">❹</span> The Spanish layer</p>

  <p class="fm-code-annotation"><a id="pgfId-1052981"></a><span class="fm-combinumeral">❺</span> Generate Spanish sentences that have one extra token, since we’ll need to offset the sentence by one step during training.</p>

  <p class="fm-code-annotation"><a id="pgfId-1052998"></a><span class="fm-combinumeral">❻</span> Learn the vocabulary of each language.</p>

  <p class="body"><a id="pgfId-1024675"></a>Finally, we can turn our data into a <code class="fm-code-in-text">tf.data</code> pipeline. We want it to return a tuple <code class="fm-code-in-text">(inputs,</code> <code class="fm-code-in-text">target)</code> where <code class="fm-code-in-text">inputs</code> is a dict with two keys, “encoder_inputs” (the English sentence) and “decoder_inputs” (the Spanish sentence), and <code class="fm-code-in-text">target</code> is the Spanish sentence offset by one step ahead.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1024735"></a>Listing 11.27 Preparing datasets for the translation task</p>
  <pre class="programlisting"><a id="pgfId-1046145"></a>batch_size = <span class="fm-codeblue">64</span> 
<a id="pgfId-1046146"></a>  
<a id="pgfId-1046147"></a><b class="fm-codebrown">def</b> format_dataset(eng, spa):
<a id="pgfId-1046148"></a>    eng = source_vectorization(eng)
<a id="pgfId-1046149"></a>    spa = target_vectorization(spa)
<a id="pgfId-1046150"></a>    <b class="fm-codebrown">return</b> ({
<a id="pgfId-1046151"></a>        <span class="fm-codegreen">"english"</span>: eng,
<a id="pgfId-1024809"></a>        <span class="fm-codegreen">"spanish"</span>: spa[:, :-<span class="fm-codeblue">1</span>],                                <span class="fm-combinumeral">❶</span>
<a id="pgfId-1024821"></a>    }, spa[:, <span class="fm-codeblue">1</span>:])                                             <span class="fm-combinumeral">❷</span>
<a id="pgfId-1024838"></a> 
<a id="pgfId-1046180"></a><b class="fm-codebrown">def</b> make_dataset(pairs):
<a id="pgfId-1046181"></a>    eng_texts, spa_texts = zip(*pairs)
<a id="pgfId-1046182"></a>    eng_texts = list(eng_texts)
<a id="pgfId-1046183"></a>    spa_texts = list(spa_texts)
<a id="pgfId-1046184"></a>    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))
<a id="pgfId-1046185"></a>    dataset = dataset.batch(batch_size)
<a id="pgfId-1046186"></a>    dataset = dataset.map(format_dataset, num_parallel_calls=<span class="fm-codeblue">4</span>)
<a id="pgfId-1024880"></a>    <b class="fm-codebrown">return</b> dataset.shuffle(<span class="fm-codeblue">2048</span>).prefetch(<span class="fm-codeblue">16</span>).cache()          <span class="fm-combinumeral">❸</span>
<a id="pgfId-1024897"></a> 
<a id="pgfId-1046199"></a>train_ds = make_dataset(train_pairs)
<a id="pgfId-1024903"></a>val_ds = make_dataset(val_pairs)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1052690"></a><span class="fm-combinumeral">❶</span> The input Spanish sentence doesn’t include the last token to keep inputs and targets at the same length.</p>

  <p class="fm-code-annotation"><a id="pgfId-1052711"></a><span class="fm-combinumeral">❷</span> The target Spanish sentence is one step ahead. Both are still the same length (20 words).</p>

  <p class="fm-code-annotation"><a id="pgfId-1052728"></a><span class="fm-combinumeral">❸</span> Use in-memory caching to speed up preprocessing.</p>

  <p class="body"><a id="pgfId-1024961"></a>Here’s what our dataset outputs look like:</p>
  <pre class="programlisting"><a id="pgfId-1049259"></a>&gt;&gt;&gt; <b class="fm-codebrown">for</b> inputs, targets <b class="fm-codebrown">in</b> train_ds.take(<span class="fm-codeblue">1</span>):
<a id="pgfId-1049260"></a>&gt;&gt;&gt;     <b class="fm-codebrown">print</b>(f<span class="fm-codegreen">"inputs['english'].shape: {inputs['english'].shape}"</span>)
<a id="pgfId-1049261"></a>&gt;&gt;&gt;     <b class="fm-codebrown">print</b>(f<span class="fm-codegreen">"inputs['spanish'].shape: {inputs['spanish'].shape}"</span>)
<a id="pgfId-1024993"></a>&gt;&gt;&gt;     <b class="fm-codebrown">print</b>(f<span class="fm-codegreen">"targets.shape: {targets.shape}"</span>)
<a id="pgfId-1046214"></a>inputs["encoder_inputs"].shape: (64, 20)
<a id="pgfId-1046215"></a>inputs["decoder_inputs"].shape: (64, 20)
<a id="pgfId-1025011"></a>targets.shape: (64, 20)</pre>

  <p class="body"><a id="pgfId-1025017"></a>The data is now ready—time to build some models. We’ll start with a recurrent sequence-to-sequence model before moving on to a Transformer. <a id="marker-1025019"></a><a id="marker-1025022"></a></p>

  <h3 class="fm-head1" id="heading_id_20"><a id="pgfId-1025028"></a>11.5.2 Sequence-to-sequence learning with RNNs</h3>

  <p class="body"><a id="pgfId-1025045"></a><a id="marker-1025039"></a><a id="marker-1025041"></a>Recurrent neural networks dominated sequence-to-sequence learning from 2015–2017 before being overtaken by Transformer. They were the basis for many real-world machine-translation systems—as mentioned in chapter 10, Google Translate circa 2017 was powered by a stack of seven large LSTM layers. It’s still worth learning about this approach today, as it provides an easy entry point to understanding sequence-to-sequence models.</p>

  <p class="body"><a id="pgfId-1025050"></a>The simplest, naive way to use RNNs to turn a sequence into another sequence is to keep the output of the RNN at each time step. In Keras, it would look like this:</p>
  <pre class="programlisting"><a id="pgfId-1046257"></a>inputs = keras.Input(shape=(sequence_length,), dtype=<span class="fm-codegreen">"int64"</span>)
<a id="pgfId-1046258"></a>x = layers.Embedding(input_dim=vocab_size, output_dim=<span class="fm-codeblue">128</span>)(inputs)
<a id="pgfId-1046259"></a>x = layers.LSTM(<span class="fm-codeblue">32</span>, return_sequences=<code class="fm-codegreen">True</code>)(x)
<a id="pgfId-1046260"></a>outputs = layers.Dense(vocab_size, activation=<span class="fm-codegreen">"softmax"</span>)(x)
<a id="pgfId-1025088"></a>model = keras.Model(inputs, outputs)</pre>

  <p class="body"><a id="pgfId-1025094"></a>However, there are two major issues with this approach:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1025100"></a>The target sequence must always be the same length as the source sequence. In practice, this is rarely the case. Technically, this isn’t critical, as you could always pad either the source sequence or the target sequence to make their lengths match.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1025114"></a>Due to the step-by-step nature of RNNs, the model will only be looking at tokens 0...<i class="fm-italics1">N</i> in the source sequence in order to predict token <i class="fm-italics1">N</i> in the target sequence. This constraint makes this setup unsuitable for most tasks, and particularly translation. Consider translating “The weather is nice today” to French—that would be “Il fait beau aujourd’hui.” You’d need to be able to predict “Il” from just “The,” “Il fait” from just “The weather,” etc., which is simply impossible.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1025124"></a>If you’re a human translator, you’d start by reading the entire source sentence before starting to translate it. This is especially important if you’re dealing with languages that have wildly different word ordering, like English and Japanese. And that’s exactly what standard sequence-to-sequence models do.</p>

  <p class="body"><a id="pgfId-1025130"></a>In a proper sequence-to-sequence setup (see figure 11.13), you would first use an RNN (the encoder) to turn the entire source sequence into a single vector (or set of vectors). This could be the last output of the RNN, or alternatively, its final internal state vectors. Then you would use this vector (or vectors) as the <i class="fm-italics">initial state</i> of another <a id="marker-1025141"></a>RNN (the decoder), which would look at elements 0...<i class="fm-italics">N</i> in the target sequence, and try to predict step <i class="fm-italics">N</i>+1 in the target sequence.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/11-13.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1060041"></a>Figure 11.13 A sequence-to-sequence RNN: an RNN encoder is used to produce a vector that encodes the entire source sequence, which is used as the initial state for an RNN decoder.</p>

  <p class="body"><a id="pgfId-1025161"></a>Let’s implement this in Keras with GRU-based encoders and decoders. The choice of GRU rather than LSTM makes things a bit simpler, since GRU only has a single state vector, whereas LSTM has multiple. Let’s start with the encoder.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1025232"></a>Listing 11.28 GRU-based encoder</p>
  <pre class="programlisting"><a id="pgfId-1046273"></a><b class="fm-codebrown">from</b> tensorflow <b class="fm-codebrown">import</b> keras 
<a id="pgfId-1046274"></a><b class="fm-codebrown">from</b> tensorflow.keras <b class="fm-codebrown">import</b> layers
<a id="pgfId-1046275"></a>  
<a id="pgfId-1046276"></a>embed_dim = <span class="fm-codeblue">256</span> 
<a id="pgfId-1047296"></a>latent_dim = <span class="fm-codeblue">1024</span>  
<a id="pgfId-1047311"></a> 
<a id="pgfId-1025294"></a>source = keras.Input(shape=(<code class="fm-codegreen">None</code>,), dtype=<span class="fm-codegreen">"int64"</span>, name=<span class="fm-codegreen">"english"</span>)  <span class="fm-combinumeral">❶</span>
<a id="pgfId-1025311"></a>x = layers.Embedding(vocab_size, embed_dim, mask_zero=<code class="fm-codegreen">True</code>)(source) <span class="fm-combinumeral">❷</span>
<a id="pgfId-1046307"></a>encoded_source = layers.Bidirectional(
<a id="pgfId-1025329"></a>    layers.GRU(latent_dim), merge_mode=<span class="fm-codegreen">"sum"</span>)(x)                    <span class="fm-combinumeral">❸</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1052521"></a><span class="fm-combinumeral">❶</span> The English source sentence goes here. Specifying the name of the input enables us to fit() the model with a dict of inputs.</p>

  <p class="fm-code-annotation"><a id="pgfId-1052542"></a><span class="fm-combinumeral">❷</span> Don’t forget masking: it’s critical in this setup.</p>

  <p class="fm-code-annotation"><a id="pgfId-1052559"></a><span class="fm-combinumeral">❸</span> Our encoded source sentence is the last output of a bidirectional GRU.</p>

  <p class="body"><a id="pgfId-1025393"></a>Next, let’s add the decoder—a simple GRU layer that takes as its initial state the encoded source sentence. On top of it, we add a <code class="fm-code-in-text">Dense</code> layer that <a id="marker-1025404"></a>produces for each output step a probability distribution over the Spanish vocabulary.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1025465"></a>Listing 11.29 GRU-based decoder and the end-to-end model</p>
  <pre class="programlisting"><a id="pgfId-1025414"></a>past_target = keras.Input(shape=(<code class="fm-codegreen">None</code>,), dtype=<span class="fm-codegreen">"int64"</span>, name=<span class="fm-codegreen">"spanish"</span>)   <span class="fm-combinumeral">❶</span>
<a id="pgfId-1025510"></a>x = layers.Embedding(vocab_size, embed_dim, mask_zero=<code class="fm-codegreen">True</code>)(past_target)  <span class="fm-combinumeral">❷</span>
<a id="pgfId-1025522"></a>decoder_gru = layers.GRU(latent_dim, return_sequences=<code class="fm-codegreen">True</code>)
<a id="pgfId-1025528"></a>x = decoder_gru(x, initial_state=encoded_source)                          <span class="fm-combinumeral">❸</span>
<a id="pgfId-1046378"></a>x = layers.Dropout(<span class="fm-codeblue">0.5</span>)(x)
<a id="pgfId-1025546"></a>target_next_step = layers.Dense(vocab_size, activation=<span class="fm-codegreen">"softmax"</span>)(x)      <span class="fm-combinumeral">❹</span>
<a id="pgfId-1025558"></a>seq2seq_rnn = keras.Model([source, past_target], target_next_step)        <span class="fm-combinumeral">❺</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1052249"></a><span class="fm-combinumeral">❶</span> The Spanish target sentence goes here.</p>

  <p class="fm-code-annotation"><a id="pgfId-1052270"></a><span class="fm-combinumeral">❷</span> Don’t forget masking.</p>

  <p class="fm-code-annotation"><a id="pgfId-1052287"></a><span class="fm-combinumeral">❸</span> The encoded source sentence serves as the initial state of the decoder GRU.</p>

  <p class="fm-code-annotation"><a id="pgfId-1052304"></a><span class="fm-combinumeral">❹</span> Predicts the next token</p>

  <p class="fm-code-annotation"><a id="pgfId-1052321"></a><span class="fm-combinumeral">❺</span> End-to-end model: maps the source sentence and the target sentence to the target sentence one step in the future</p>

  <p class="body"><a id="pgfId-1025654"></a>During training, the decoder takes as input the entire target sequence, but thanks to the step-by-step nature of RNNs, it only looks at tokens 0...<i class="fm-italics">N</i> in the input to predict token <i class="fm-italics">N</i> in the output (which corresponds to the next token in the sequence, since the output is intended to be offset by one step). This means we only use information from the past to predict the future, as we should; otherwise we’d be cheating, and our model would not work at inference time.</p>

  <p class="body"><a id="pgfId-1025660"></a>Let’s start training.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1025717"></a>Listing 11.30 Training our recurrent sequence-to-sequence model</p>
  <pre class="programlisting"><a id="pgfId-1046424"></a>seq2seq_rnn.compile(
<a id="pgfId-1046425"></a>    optimizer=<span class="fm-codegreen">"rmsprop"</span>,
<a id="pgfId-1046426"></a>    loss=<span class="fm-codegreen">"sparse_categorical_crossentropy"</span>,
<a id="pgfId-1046427"></a>    metrics=[<span class="fm-codegreen">"accuracy"</span>])
<a id="pgfId-1025774"></a>seq2seq_rnn.fit(train_ds, epochs=<span class="fm-codeblue">15</span>, validation_data=val_ds)</pre>

  <p class="body"><a id="pgfId-1025780"></a>We picked accuracy as a crude way to monitor validation-set performance during training. We get to 64% accuracy: on average, the model predicts the next word in the Spanish sentence correctly 64% of the time. However, in practice, next-token accuracy isn’t a great metric for machine translation models, in particular because it makes the assumption that the correct target tokens from 0 to <i class="fm-italics">N</i> are already known when predicting token <i class="fm-italics">N</i>+1. In reality, during inference, you’re generating the target sentence from scratch, and you can’t rely on previously generated tokens being 100% correct. If you work on a real-world machine translation system, you will likely use “BLEU scores” to evaluate your models—a metric that looks at entire generated sequences and that seems to correlate well with human perception of translation quality.</p>

  <p class="body"><a id="pgfId-1025802"></a>At last, let’s use our model for inference. We’ll pick a few sentences in the test set and check how our model translates them. We’ll start from the seed token, <code class="fm-code-in-text">"[start]"</code>, and feed it into the decoder model, together with the encoded English source sentence. We’ll retrieve a next-token prediction, and we’ll re-inject it into the decoder repeatedly, sampling one new target token at each iteration, until we get to <code class="fm-code-in-text">"[end]"</code> or reach the maximum sentence length.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1025862"></a>Listing 11.31 Translating new sentences with our RNN encoder and decoder</p>
  <pre class="programlisting"><a id="pgfId-1046442"></a><b class="fm-codebrown">import</b> numpy <b class="fm-codebrown">as</b> np
<a id="pgfId-1025901"></a>spa_vocab = target_vectorization.get_vocabulary()                          <span class="fm-combinumeral">❶</span>
<a id="pgfId-1025913"></a>spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))             <span class="fm-combinumeral">❶</span>
<a id="pgfId-1025925"></a>max_decoded_sentence_length = <span class="fm-codeblue">20</span>
<a id="pgfId-1025936"></a> 
<a id="pgfId-1046490"></a><b class="fm-codebrown">def</b> decode_sequence(input_sentence):
<a id="pgfId-1046491"></a>    tokenized_input_sentence = source_vectorization([input_sentence])
<a id="pgfId-1025948"></a>    decoded_sentence = <span class="fm-codegreen">"[start]"</span>                                           <span class="fm-combinumeral">❷</span>
<a id="pgfId-1046526"></a>    <b class="fm-codebrown">for</b> i <b class="fm-codebrown">in</b> range(max_decoded_sentence_length):
<a id="pgfId-1046527"></a>        tokenized_target_sentence = target_vectorization([decoded_sentence])
<a id="pgfId-1025972"></a>        next_token_predictions = seq2seq_rnn.predict(                      <span class="fm-combinumeral">❸</span>
<a id="pgfId-1025984"></a>            [tokenized_input_sentence, tokenized_target_sentence])         <span class="fm-combinumeral">❸</span>
<a id="pgfId-1025996"></a>        sampled_token_index = np.argmax(next_token_predictions[0, i, :])   <span class="fm-combinumeral">❸</span>
<a id="pgfId-1026008"></a>        sampled_token = spa_index_lookup[sampled_token_index]              <span class="fm-combinumeral">❹</span>
<a id="pgfId-1026020"></a>        decoded_sentence += <span class="fm-codegreen">" "</span> + sampled_token                            <span class="fm-combinumeral">❹</span>
<a id="pgfId-1026032"></a>        <b class="fm-codebrown">if</b> sampled_token == <span class="fm-codegreen">"[end]"</span>:                                       <span class="fm-combinumeral">❺</span>
<a id="pgfId-1046568"></a>            <b class="fm-codebrown">break</b>
<a id="pgfId-1046569"></a>    <b class="fm-codebrown">return</b> decoded_sentence
<a id="pgfId-1046570"></a>  
<a id="pgfId-1046571"></a>test_eng_texts = [pair[<span class="fm-codeblue">0</span>] <b class="fm-codebrown">for</b> pair <b class="fm-codebrown">in</b> test_pairs] 
<a id="pgfId-1046572"></a><b class="fm-codebrown">for</b> _ <b class="fm-codebrown">in</b> range(<span class="fm-codeblue">20</span>):
<a id="pgfId-1046573"></a>    input_sentence = random.choice(test_eng_texts)
<a id="pgfId-1046574"></a>    <b class="fm-codebrown">print</b>(<span class="fm-codegreen">"-"</span>)
<a id="pgfId-1046575"></a>    <b class="fm-codebrown">print</b>(input_sentence)
<a id="pgfId-1026091"></a>    <b class="fm-codebrown">print</b>(decode_sequence(input_sentence))</pre>

  <p class="fm-code-annotation"><a id="pgfId-1051737"></a><span class="fm-combinumeral">❶</span> Prepare a dict to convert token index predictions to string tokens.</p>

  <p class="fm-code-annotation"><a id="pgfId-1051758"></a><span class="fm-combinumeral">❷</span> Seed token</p>

  <p class="fm-code-annotation"><a id="pgfId-1051775"></a><span class="fm-combinumeral">❸</span> Sample the next token.</p>

  <p class="fm-code-annotation"><a id="pgfId-1051792"></a><span class="fm-combinumeral">❹</span> Convert the next token prediction to a string and append it to the generated sentence.</p>

  <p class="fm-code-annotation"><a id="pgfId-1051809"></a><span class="fm-combinumeral">❺</span> Exit condition: either hit max length or sample a stop character</p>

  <p class="body"><a id="pgfId-1026181"></a>Note that this inference setup, while very simple, is rather inefficient, since we reprocess the entire source sentence and the entire generated target sentence every time we sample a new word. In a practical application, you’d factor the encoder and the decoder as two separate models, and your decoder would only run a single step at each token-sampling iteration, reusing its previous internal state.</p>

  <p class="body"><a id="pgfId-1026187"></a>Here are our translation results. Our model works decently well for a toy model, though it still makes many basic mistakes.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1026244"></a>Listing 11.32 Some sample results from the recurrent translation model</p>
  <pre class="programlisting"><a id="pgfId-1046593"></a>Who is in this room?
<a id="pgfId-1046594"></a>[start] quién está en esta habitación [end]
<a id="pgfId-1046595"></a>-
<a id="pgfId-1046596"></a>That doesn't sound too dangerous.
<a id="pgfId-1046597"></a>[start] eso no es muy difícil [end]
<a id="pgfId-1046598"></a>-
<a id="pgfId-1046599"></a>No one will stop me.
<a id="pgfId-1046600"></a>[start] nadie me va a hacer [end]
<a id="pgfId-1046601"></a>-
<a id="pgfId-1046602"></a>Tom is friendly.
<a id="pgfId-1026337"></a>[start] tom es un buen [UNK] [end]</pre>

  <p class="body"><a id="pgfId-1026343"></a>There are many ways this toy model could be improved: We could use a deep stack of recurrent layers for both the encoder and the decoder (note that for the decoder, this makes state management a bit more involved). We could use an LSTM instead of a GRU. And so on. Beyond such tweaks, however, the RNN approach to sequence-to-sequence learning has a few fundamental limitations:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1026349"></a>The source sequence representation has to be held entirely in the encoder state vector(s), which puts significant limitations on the size and complexity of the sentences you can translate. It’s a bit as if a human were translating a sentence entirely from memory, without looking twice at the source sentence while producing the translation.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1026363"></a>RNNs have trouble dealing with very long sequences, since they tend to progressively forget about the past—by the time you’ve reached the 100th token in either sequence, little information remains about the start of the sequence. That means RNN-based models can’t hold onto long-term context, which can be essential for translating long documents.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1026373"></a>These limitations are what has led the machine learning community to embrace the Transformer architecture for sequence-to-sequence problems. Let’s take a look. <a id="marker-1026375"></a><a id="marker-1026378"></a></p>

  <h3 class="fm-head1" id="heading_id_21"><a id="pgfId-1026384"></a>11.5.3 Sequence-to-sequence learning with Transformer</h3>

  <p class="body"><a id="pgfId-1026403"></a><a id="marker-1026395"></a><a id="marker-1026397"></a><a id="marker-1026399"></a>Sequence-to-sequence learning is the task where Transformer really shines. Neural attention enables Transformer models to successfully process sequences that are considerably longer and more complex than those RNNs can handle.</p>

  <p class="body"><a id="pgfId-1026408"></a>As a human translating English to Spanish, you’re not going to read the English sentence one word at a time, keep its meaning in memory, and then generate the Spanish sentence one word at a time. That may work for a five-word sentence, but it’s unlikely to work for an entire paragraph. Instead, you’ll probably want to go back and forth between the source sentence and your translation in progress, and pay attention to different words in the source as you’re writing down different parts of your translation.</p>

  <p class="body"><a id="pgfId-1026414"></a>That’s exactly what you can achieve with neural attention and Transformers. You’re already familiar with the Transformer encoder, which uses self-attention to produce context-aware representations of each token in an input sequence. In a sequence-to-sequence Transformer, the Transformer encoder would naturally play the role of the encoder, which reads the source sequence and produces an encoded representation of it. Unlike our previous RNN encoder, though, the Transformer encoder keeps the encoded representation in a sequence format: it’s a sequence of context-aware embedding vectors.</p>

  <p class="body"><a id="pgfId-1026429"></a>The second half of the model is the <i class="fm-italics">Transformer decoder</i>. Just like the RNN decoder, it reads tokens 0...<i class="fm-italics">N</i> in the target sequence and tries to predict token <i class="fm-italics">N</i>+1. Crucially, while doing this, it uses neural attention to identify which tokens in the encoded source sentence are most closely related to the target token it’s currently trying to predict—perhaps not unlike what a human translator would do. Recall the query-key-value model: in a Transformer decoder, the target sequence serves as an attention “query” that is used to to pay closer attention to different parts of the source sequence (the source sequence plays the roles of both keys and values).</p>

  <p class="fm-head2"><a id="pgfId-1026444"></a>The Transformer decoder</p>

  <p class="body"><a id="pgfId-1026454"></a><a id="marker-1026455"></a>Figure 11.14 shows the full sequence-to-sequence Transformer. Look at the decoder internals: you’ll recognize that it looks very similar to the Transformer encoder, except that an extra attention block is inserted between the self-attention block applied to the target sequence and the dense layers of the exit block.</p>

  <p class="body"><a id="pgfId-1026548"></a>Let’s implement it. Like for the <code class="fm-code-in-text">TransformerEncoder</code>, we’ll use a <code class="fm-code-in-text">Layer</code> subclass. Before we <a id="marker-1026537"></a>focus on the <code class="fm-code-in-text">call()</code>, method, where the <a id="marker-1026553"></a>action happens, let’s start by defining the class constructor, containing the layers we’re going to need.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1026614"></a>Listing 11.33 The <code class="fm-code-in-text">TransformerDecoder</code></p>
  <pre class="programlisting"><a id="pgfId-1046621"></a><b class="fm-codebrown">class</b> TransformerDecoder(layers.Layer):
<a id="pgfId-1046622"></a>    <b class="fm-codebrown">def</b> __init__(self, embed_dim, dense_dim, num_heads, **kwargs):
<a id="pgfId-1046623"></a>        super().__init__(**kwargs)
<a id="pgfId-1046624"></a>        self.embed_dim = embed_dim
<a id="pgfId-1046625"></a>        self.dense_dim = dense_dim
<a id="pgfId-1046626"></a>        self.num_heads = num_heads
<a id="pgfId-1046627"></a>        self.attention_1 = layers.MultiHeadAttention(
<a id="pgfId-1046628"></a>            num_heads=num_heads, key_dim=embed_dim)
<a id="pgfId-1046629"></a>        self.attention_2 = layers.MultiHeadAttention(
<a id="pgfId-1046630"></a>            num_heads=num_heads, key_dim=embed_dim)
<a id="pgfId-1046631"></a>        self.dense_proj = keras.Sequential(
<a id="pgfId-1046632"></a>            [layers.Dense(dense_dim, activation=<span class="fm-codegreen">"relu"</span>),
<a id="pgfId-1046633"></a>             layers.Dense(embed_dim),]
<a id="pgfId-1046634"></a>        )
<a id="pgfId-1046635"></a>        self.layernorm_1 = layers.LayerNormalization()
<a id="pgfId-1046636"></a>        self.layernorm_2 = layers.LayerNormalization()
<a id="pgfId-1046637"></a>        self.layernorm_3 = layers.LayerNormalization()
<a id="pgfId-1026757"></a>        self.supports_masking = <code class="fm-codegreen">True</code>                     <span class="fm-combinumeral">❶</span>
<a id="pgfId-1046654"></a>  
<a id="pgfId-1046655"></a>    <b class="fm-codebrown">def</b> get_config(self):
<a id="pgfId-1046656"></a>        config = super().get_config()
<a id="pgfId-1046657"></a>        config.update({
<a id="pgfId-1046658"></a>            <span class="fm-codegreen">"embed_dim"</span>: self.embed_dim,
<a id="pgfId-1046659"></a>            <span class="fm-codegreen">"num_heads"</span>: self.num_heads,
<a id="pgfId-1046660"></a>            <span class="fm-codegreen">"dense_dim"</span>: self.dense_dim,
<a id="pgfId-1046661"></a>        })
<a id="pgfId-1026816"></a>        <b class="fm-codebrown">return</b> config</pre>

  <p class="fm-code-annotation"><a id="pgfId-1051667"></a><span class="fm-combinumeral">❶</span> This attribute ensures that the layer will propagate its input mask to its outputs; masking in Keras is explicitly opt-in. If you pass a mask to a layer that doesn’t implement compute_mask() and that doesn’t expose this supports_masking attribute, that’s an error.</p>

  <p class="body"><a id="pgfId-1026874"></a>The <code class="fm-code-in-text">call()</code> method is almost a straightforward rendering of the connectivity diagram from figure 11.14. But there’s an additional detail we need to take into account: <i class="fm-italics">causal padding</i>. Causal padding <a id="marker-1026863"></a>is absolutely critical to successfully training a sequence-to-sequence Transformer. Unlike an RNN, which looks at its input one step at a time, and thus will only have access to steps 0...<i class="fm-italics">N</i> to generate output step <i class="fm-italics">N</i> (which is token <i class="fm-italics">N</i>+1 in the target sequence), the <code class="fm-code-in-text">TransformerDecoder</code> is order-agnostic: it looks at the entire target sequence at once. If it were allowed to use its entire input, it would simply learn to copy input step <i class="fm-italics">N</i>+1 to location <i class="fm-italics">N</i> in the output. The model would thus achieve perfect training accuracy, but of course, when running inference, it would be completely useless, since input steps beyond <i class="fm-italics">N</i> aren’t available.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/11-14.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1060083"></a>Figure 11.14 The <code class="fm-code-in-text">TransformerDecoder</code> is similar to the <code class="fm-code-in-text">TransformerEncoder</code>, except it features an additional attention block where the keys and values are the source sequence encoded by the <code class="fm-code-in-text">TransformerEncoder</code>. Together, the encoder and the decoder form an end-to-end Transformer.</p>

  <p class="body"><a id="pgfId-1026909"></a>The fix is simple: we’ll mask the upper half of the pairwise attention matrix to prevent the model from paying any attention to information from the future—only information from tokens 0...<i class="fm-italics">N</i> in the target sequence should be used when generating target token <i class="fm-italics">N</i>+1. To do this, we’ll add a <code class="fm-code-in-text">get_causal_attention_mask(self,</code> <code class="fm-code-in-text">inputs)</code> method to our <code class="fm-code-in-text">TransformerDecoder</code> to retrieve an attention mask that we can pass to our <code class="fm-code-in-text">MultiHeadAttention</code> layers.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1026969"></a>Listing 11.34 <code class="fm-code-in-text">TransformerDecoder</code> method that generates a causal mask</p>
  <pre class="programlisting"><a id="pgfId-1046723"></a>    <b class="fm-codebrown">def</b> get_causal_attention_mask(self, inputs):
<a id="pgfId-1046724"></a>        input_shape = tf.shape(inputs)
<a id="pgfId-1046725"></a>        batch_size, sequence_length = input_shape[<span class="fm-codeblue">0</span>], input_shape[<span class="fm-codeblue">1</span>]
<a id="pgfId-1046726"></a>        i = tf.range(sequence_length)[:, tf.newaxis]
<a id="pgfId-1046727"></a>        j = tf.range(sequence_length)
<a id="pgfId-1027041"></a>        mask = tf.cast(i &gt;= j, dtype=<span class="fm-codegreen">"int32"</span>)                           <span class="fm-combinumeral">❶</span>
<a id="pgfId-1027053"></a>        mask = tf.reshape(mask, (<span class="fm-codeblue">1</span>, input_shape[<span class="fm-codeblue">1</span>], input_shape[<span class="fm-codeblue">1</span>]))    <span class="fm-combinumeral">❷</span>
<a id="pgfId-1027065"></a>        mult = tf.concat(                                               <span class="fm-combinumeral">❷</span>
<a id="pgfId-1027077"></a>            [tf.expand_dims(batch_size, -<span class="fm-codeblue">1</span>),                            <span class="fm-combinumeral">❷</span>
<a id="pgfId-1027089"></a>             tf.constant([<span class="fm-codeblue">1</span>, <span class="fm-codeblue">1</span>], dtype=tf.int32)], axis=<span class="fm-codeblue">0</span>)              <span class="fm-combinumeral">❷</span>
<a id="pgfId-1027101"></a>        <b class="fm-codebrown">return</b> tf.tile(mask, mult)          3                           <span class="fm-combinumeral">❷</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1051478"></a><span class="fm-combinumeral">❶</span> Generate matrix of shape (sequence_length, sequence_length) with 1s in one half and 0s in the other.</p>

  <p class="fm-code-annotation"><a id="pgfId-1051499"></a><span class="fm-combinumeral">❷</span> Replicate it along the batch axis to get a matrix of shape (batch_size, sequence_length, sequence_length)</p>

  <p class="body"><a id="pgfId-1027149"></a>Now we can write down the full <code class="fm-code-in-text">call()</code> method implementing the forward pass of the decoder.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1027215"></a>Listing 11.35 The forward pass of the <code class="fm-code-in-text">TransformerDecoder</code></p>
  <pre class="programlisting"><a id="pgfId-1046856"></a>    <b class="fm-codebrown">def</b> call(self, inputs, encoder_outputs, mask=<code class="fm-codegreen">None</code>):
<a id="pgfId-1027262"></a>        causal_mask = self.get_causal_attention_mask(inputs)       <span class="fm-combinumeral">❶</span>
<a id="pgfId-1027274"></a>        <b class="fm-codebrown">if</b> mask <b class="fm-codebrown">is</b> <b class="fm-codebrown">not</b> <code class="fm-codegreen">None</code>:                                       <span class="fm-combinumeral">❷</span>
<a id="pgfId-1027286"></a>            padding_mask = tf.cast(                                <span class="fm-combinumeral">❷</span>
<a id="pgfId-1027298"></a>                mask[:, tf.newaxis, :], dtype=<span class="fm-codegreen">"int32"</span>)             <span class="fm-combinumeral">❷</span>
<a id="pgfId-1027310"></a>            padding_mask = tf.minimum(padding_mask, causal_mask)   <span class="fm-combinumeral">❸</span>
<a id="pgfId-1046921"></a>        attention_output_1 = self.attention_1(
<a id="pgfId-1046922"></a>            query=inputs,
<a id="pgfId-1046923"></a>            value=inputs,
<a id="pgfId-1046924"></a>            key=inputs,
<a id="pgfId-1027346"></a>            attention_mask=causal_mask)                            <span class="fm-combinumeral">❹</span>
<a id="pgfId-1046937"></a>        attention_output_1 = self.layernorm_1(inputs + attention_output_1)
<a id="pgfId-1046938"></a>        attention_output_2 = self.attention_2(
<a id="pgfId-1046939"></a>            query=attention_output_1,
<a id="pgfId-1046940"></a>            value=encoder_outputs,
<a id="pgfId-1046941"></a>            key=encoder_outputs,
<a id="pgfId-1027388"></a>            attention_mask=padding_mask,                           <span class="fm-combinumeral">❺</span>
<a id="pgfId-1046954"></a>        )
<a id="pgfId-1046955"></a>        attention_output_2 = self.layernorm_2(
<a id="pgfId-1046956"></a>            attention_output_1 + attention_output_2)
<a id="pgfId-1046957"></a>        proj_output = self.dense_proj(attention_output_2)
<a id="pgfId-1027424"></a>        <b class="fm-codebrown">return</b> self.layernorm_3(attention_output_2 + proj_output)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1051130"></a><span class="fm-combinumeral">❶</span> Retrieve the causal mask.</p>

  <p class="fm-code-annotation"><a id="pgfId-1051151"></a><span class="fm-combinumeral">❷</span> Prepare the input mask (that describes padding locations in the target sequence).</p>

  <p class="fm-code-annotation"><a id="pgfId-1051168"></a><span class="fm-combinumeral">❸</span> Merge the two masks together.</p>

  <p class="fm-code-annotation"><a id="pgfId-1051185"></a><span class="fm-combinumeral">❹</span> Pass the causal mask to the first attention layer, which performs self-attention over the target sequence.</p>

  <p class="fm-code-annotation"><a id="pgfId-1051202"></a><span class="fm-combinumeral">❺</span> Pass the combined mask to the second attention layer, which relates the source sequence to the target sequence. <a id="marker-1051207"></a></p>

  <p class="fm-head2"><a id="pgfId-1027517"></a>Putting it all together: A Transformer for machine translation</p>

  <p class="body"><a id="pgfId-1027580"></a>The end-to-end Transformer is the model we’ll be training. It maps the source sequence and the target sequence to the target sequence one step in the future. It straightforwardly combines the <a id="marker-1027529"></a>pieces we’ve built so far: <code class="fm-code-in-text">PositionalEmbedding</code> layers, the <code class="fm-code-in-text">TransformerEncoder</code>, and the <code class="fm-code-in-text">TransformerDecoder</code>. Note that both the <code class="fm-code-in-text">TransformerEncoder</code> and the <code class="fm-code-in-text">TransformerDecoder</code> are shape-invariant, so you could be stacking many of them to create a more powerful encoder or decoder. In our example, we’ll stick to a single instance of each.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1027640"></a>Listing 11.36 End-to-end Transformer</p>
  <pre class="programlisting"><a id="pgfId-1046972"></a>embed_dim = <span class="fm-codeblue">256</span> 
<a id="pgfId-1046973"></a>dense_dim = <span class="fm-codeblue">2048</span> 
<a id="pgfId-1046974"></a>num_heads = <span class="fm-codeblue">8</span> 
<a id="pgfId-1046975"></a>  
<a id="pgfId-1046976"></a>encoder_inputs = keras.Input(shape=(<code class="fm-codegreen">None</code>,), dtype=<span class="fm-codegreen">"int64"</span>, name=<span class="fm-codegreen">"english"</span>)
<a id="pgfId-1046977"></a>x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)
<a id="pgfId-1027708"></a>encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)     <span class="fm-combinumeral">❶</span>
<a id="pgfId-1027725"></a> 
<a id="pgfId-1047000"></a>decoder_inputs = keras.Input(shape=(<code class="fm-codegreen">None</code>,), dtype=<span class="fm-codegreen">"int64"</span>, name=<span class="fm-codegreen">"spanish"</span>)
<a id="pgfId-1047001"></a>x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)
<a id="pgfId-1027737"></a>x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)  <span class="fm-combinumeral">❷</span>
<a id="pgfId-1047019"></a>x = layers.Dropout(<span class="fm-codeblue">0.5</span>)(x)
<a id="pgfId-1027755"></a>decoder_outputs = layers.Dense(vocab_size, activation=<span class="fm-codegreen">"softmax"</span>)(x)        <span class="fm-combinumeral">❸</span>
<a id="pgfId-1027767"></a>transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1050971"></a><span class="fm-combinumeral">❶</span> Encode the source sentence.</p>

  <p class="fm-code-annotation"><a id="pgfId-1050992"></a><span class="fm-combinumeral">❷</span> Encode the target sentence and combine it with the encoded source sentence.</p>

  <p class="fm-code-annotation"><a id="pgfId-1051009"></a><span class="fm-combinumeral">❸</span> Predict a word for each output position.</p>

  <p class="body"><a id="pgfId-1027825"></a>We’re now ready to train our model—we get to 67% accuracy, a good deal above the GRU-based model.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1027882"></a>Listing 11.37 Training the sequence-to-sequence Transformer</p>
  <pre class="programlisting"><a id="pgfId-1047059"></a>transformer.compile(
<a id="pgfId-1047060"></a>    optimizer=<span class="fm-codegreen">"rmsprop"</span>,
<a id="pgfId-1047061"></a>    loss=<span class="fm-codegreen">"sparse_categorical_crossentropy"</span>,
<a id="pgfId-1047062"></a>    metrics=[<span class="fm-codegreen">"accuracy"</span>])
<a id="pgfId-1027939"></a>transformer.fit(train_ds, epochs=<span class="fm-codeblue">30</span>, validation_data=val_ds)</pre>

  <p class="body"><a id="pgfId-1027945"></a>Finally, let’s try using our model to translate never-seen-before English sentences from the test set. The setup is identical to what we used for the sequence-to-sequence RNN model.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1028002"></a>Listing 11.38 Translating new sentences with our Transformer model</p>
  <pre class="programlisting"><a id="pgfId-1047079"></a><b class="fm-codebrown">import</b> numpy <b class="fm-codebrown">as</b> np
<a id="pgfId-1047080"></a>spa_vocab = target_vectorization.get_vocabulary()
<a id="pgfId-1047081"></a>spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))
<a id="pgfId-1047082"></a>max_decoded_sentence_length = <span class="fm-codeblue">20</span> 
<a id="pgfId-1047083"></a>  
<a id="pgfId-1047084"></a><b class="fm-codebrown">def</b> decode_sequence(input_sentence):
<a id="pgfId-1047085"></a>    tokenized_input_sentence = source_vectorization([input_sentence])
<a id="pgfId-1047086"></a>    decoded_sentence = <span class="fm-codegreen">"[start]"</span> 
<a id="pgfId-1047087"></a>    <b class="fm-codebrown">for</b> i <b class="fm-codebrown">in</b> range(max_decoded_sentence_length):
<a id="pgfId-1047088"></a>        tokenized_target_sentence = target_vectorization(
<a id="pgfId-1047089"></a>            [decoded_sentence])[:, :-<span class="fm-codeblue">1</span>]
<a id="pgfId-1047090"></a>        predictions = transformer(
<a id="pgfId-1028106"></a>            [tokenized_input_sentence, tokenized_target_sentence])  <span class="fm-combinumeral">❶</span>
<a id="pgfId-1028118"></a>        sampled_token_index = np.argmax(predictions[<span class="fm-codeblue">0</span>, i, :])       <span class="fm-combinumeral">❶</span>
<a id="pgfId-1028130"></a>        sampled_token = spa_index_lookup[sampled_token_index]       <span class="fm-combinumeral">❷</span>
<a id="pgfId-1028142"></a>        decoded_sentence += <span class="fm-codegreen">" "</span> + sampled_token                     <span class="fm-combinumeral">❷</span>
<a id="pgfId-1028154"></a>        <b class="fm-codebrown">if</b> sampled_token == <span class="fm-codegreen">"[end]"</span>:                                <span class="fm-combinumeral">❸</span>
<a id="pgfId-1028166"></a>            <b class="fm-codebrown">break</b>                                                   <span class="fm-combinumeral">❸</span>
<a id="pgfId-1047186"></a>    <b class="fm-codebrown">return</b> decoded_sentence
<a id="pgfId-1047187"></a>  
<a id="pgfId-1047188"></a>test_eng_texts = [pair[<span class="fm-codeblue">0</span>] <b class="fm-codebrown">for</b> pair <b class="fm-codebrown">in</b> test_pairs] 
<a id="pgfId-1047189"></a><b class="fm-codebrown">for</b> _ <b class="fm-codebrown">in</b> range(<span class="fm-codeblue">20</span>):
<a id="pgfId-1047190"></a>    input_sentence = random.choice(test_eng_texts)
<a id="pgfId-1047191"></a>    <b class="fm-codebrown">print</b>(<span class="fm-codegreen">"-"</span>)
<a id="pgfId-1047192"></a>    <b class="fm-codebrown">print</b>(input_sentence)
<a id="pgfId-1028219"></a>    <b class="fm-codebrown">print</b>(decode_sequence(input_sentence))</pre>

  <p class="fm-code-annotation"><a id="pgfId-1050705"></a><span class="fm-combinumeral">❶</span> Sample the next token.</p>

  <p class="fm-code-annotation"><a id="pgfId-1050726"></a><span class="fm-combinumeral">❷</span> Convert the next token prediction to a string, and append it to the generated sentence.</p>

  <p class="fm-code-annotation"><a id="pgfId-1050750"></a><span class="fm-combinumeral">❸</span> Exit condition</p>

  <p class="body"><a id="pgfId-1028277"></a>Subjectively, the Transformer seems to perform significantly better than the GRU-based translation model. It’s still a toy model, but it’s a better toy model.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1028334"></a>Listing 11.39 Some sample results from the Transformer translation model</p>
  <pre class="programlisting"><a id="pgfId-1047213"></a>This is a song I learned when I was a kid.
<a id="pgfId-1028373"></a>[start] esta es una canción que aprendí cuando era chico [end]    <span class="fm-combinumeral">❶</span>
<a id="pgfId-1028385"></a>-
<a id="pgfId-1047230"></a>She can play the piano.
<a id="pgfId-1047231"></a>[start] ella puede tocar piano [end]
<a id="pgfId-1047232"></a>-
<a id="pgfId-1047233"></a>I'm not who you think I am.
<a id="pgfId-1047234"></a>[start] no soy la persona que tú creo que soy [end]
<a id="pgfId-1047235"></a>-
<a id="pgfId-1047236"></a>It may have rained a little last night.
<a id="pgfId-1028433"></a>[start] puede que llueve un poco el pasado [end]</pre>

  <p class="fm-code-annotation"><a id="pgfId-1050631"></a><span class="fm-combinumeral">❶</span> While the source sentence wasn’t gendered, this translation assumes a male speaker. Keep in mind that translation models will often make unwarranted assumptions about their input data, which leads to algorithmic bias. In the worst cases, a model might hallucinate memorized information that has nothing to do with the data it’s currently processing.</p>

  <p class="body"><a id="pgfId-1028459"></a>That concludes this chapter on natural language processing—you just went from the very basics to a fully fledged Transformer that can translate from English to Spanish. Teaching machines to make sense of language is the latest superpower you can add to your collection. <a id="marker-1028461"></a><a id="marker-1028464"></a><a id="marker-1028466"></a><a id="marker-1028470"></a><a id="marker-1028472"></a></p>

  <h2 class="fm-head" id="heading_id_22"><a id="pgfId-1028478"></a>Summary</h2>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1028522"></a>There are two kinds of NLP models: <i class="fm-italics1">bag-of-words models</i> that process sets of words or N-grams without taking into account their order, and <i class="fm-italics1">sequence models</i> that process word order. A bag-of-words model is made of <code class="fm-code-in-text">Dense</code> layers, while a sequence model could be an RNN, a 1D convnet, or a Transformer.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1028531"></a>When it comes to text classification, the ratio between the number of samples in your training data and the mean number of words per sample can help you determine whether you should use a bag-of-words model or a sequence model.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1028541"></a><i class="fm-italics1">Word embeddings</i> are vector spaces where semantic relationships between words are modeled as distance relationships between vectors that represent those words.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1028558"></a><i class="fm-italics1">Sequence-to-sequence learning</i> is a generic, powerful learning framework that can be applied to solve many NLP problems, including machine translation. A sequence-to-sequence model is made of an encoder, which processes a source sequence, and a decoder, which tries to predict future tokens in target sequence by looking at past tokens, with the help of the encoder-processed source sequence.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1028575"></a><i class="fm-italics1">Neural attention</i> is a way to create context-aware word representations. It’s the basis for the Transformer architecture.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1028632"></a>The <i class="fm-italics1">Transformer</i> architecture, which consists of a <code class="fm-code-in-text">TransformerEncoder</code> and a <code class="fm-code-in-text">TransformerDecoder</code>, yields excellent results on sequence-to-sequence tasks. The first half, the <code class="fm-code-in-text">TransformerEncoder</code>, can also be used for text classification or any sort of single-input NLP task.</p>
    </li>
  </ul>
  <hr class="calibre15"/>

  <p class="fm-footnote"><a href="../Text/11.htm#Id-1019229"><sup class="footnotenumber1">1</sup></a> <a id="pgfId-1019229"></a>Yoshua Bengio et al., “A Neural Probabilistic Language Model,” <i class="fm-italics">Journal of Machine Learning Research</i> (2003).</p>

  <p class="fm-footnote"><a href="../Text/11.htm#Id-1020369"><sup class="footnotenumber1">2</sup></a> <a id="pgfId-1020369"></a>Ashish Vaswani et al., “Attention is all you need” (2017), <span class="fm-hyperlink"><a class="url" href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></span>.</p>
</body>
</html>
