<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>12</title>
    
  <link href="../../stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="../../page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
  <h1 class="tochead" id="heading_id_2"><a id="pgfId-998407"></a><a id="pgfId-1031687"></a>12 Generative deep learning</h1>

  <p class="co-summary-head"><a id="pgfId-1011754"></a>This chapter covers</p>

  <ul class="calibre10">
    <li class="co-summary-bullet"><a class="calibre11" id="pgfId-1011760"></a>Text generation</li>

    <li class="co-summary-bullet"><a class="calibre11" id="pgfId-1011774"></a>DeepDream</li>

    <li class="co-summary-bullet"><a class="calibre11" id="pgfId-1011784"></a>Neural style transfer</li>

    <li class="co-summary-bullet"><a class="calibre11" id="pgfId-1011794"></a>Variational autoencoders</li>

    <li class="co-summary-bullet"><a class="calibre11" id="pgfId-1011804"></a>Generative adversarial networks</li>
  </ul>

  <p class="body"><a id="pgfId-1011814"></a>The potential of artificial intelligence to emulate human thought processes goes beyond passive tasks such as object recognition and mostly reactive tasks such as driving a car. It extends well into creative activities. When I first made the claim that in a not-so-distant future, most of the cultural content that we consume will be created with substantial help from AIs, I was met with utter disbelief, even from long-time machine learning practitioners. That was in 2014. Fast-forward a few years, and the disbelief had receded at an incredible speed. In the summer of 2015, we were entertained by Google’s DeepDream algorithm turning an image into a psychedelic mess of dog eyes and pareidolic artifacts; in 2016, we started using smartphone applications to turn photos into paintings of various styles. In the summer of 2016, an experimental short movie, <i class="fm-italics">Sunspring</i>, was directed using a script written by a Long Short-Term Memory. Maybe you’ve recently listened to music that was tentatively generated by a neural network.</p>

  <p class="body"><a id="pgfId-1011855"></a>Granted, the artistic productions we’ve seen from AI so far have been fairly low quality. AI isn’t anywhere close to rivaling human screenwriters, painters, and composers. But replacing humans was always beside the point: artificial intelligence isn’t about replacing our own intelligence with something else, it’s about bringing into our lives and work <i class="fm-italics">more</i> intelligence—intelligence of a different kind. In many fields, but especially in creative ones, AI will be used by humans as a tool to augment their own capabilities: more <i class="fm-italics">augmented intelligence</i> than <i class="fm-italics">artificial intelligence</i>.</p>

  <p class="body"><a id="pgfId-1011880"></a>A large part of artistic creation consists of simple pattern recognition and technical skill. And that’s precisely the part of the process that many find less attractive or even dispensable. That’s where AI comes in. Our perceptual modalities, our language, and our artwork all have statistical structure. Learning this structure is what deep learning algorithms excel at. Machine learning models can learn the statistical <i class="fm-italics">latent space</i> of images, music, and stories, and they can then <i class="fm-italics">sample</i> from this space, creating new artworks with characteristics similar to those the model has seen in its training data. Naturally, such sampling is hardly an act of artistic creation in itself. It’s a mere mathematical operation: the algorithm has no grounding in human life, human emotions, or our experience of the world; instead, it learns from an experience that has little in common with ours. It’s only our interpretation, as human spectators, that will give meaning to what the model generates. But in the hands of a skilled artist, algorithmic generation can be steered to become meaningful—and beautiful. Latent space sampling can become a brush that empowers the artist, augments our creative affordances, and expands the space of what we can imagine. What’s more, it can make artistic creation more accessible by eliminating the need for technical skill and practice—setting up a new medium of pure expression, factoring art apart from craft.</p>

  <p class="body"><a id="pgfId-1011889"></a>Iannis Xenakis, a visionary pioneer of electronic and algorithmic music, beautifully expressed this same idea in the 1960s, in the context of the application of automation technology to music composition:<a id="Id-1011892"></a><a href="#pgfId-1011892"><sup class="footnotenumber">1</sup></a></p>

  <p class="fm-quote"><i class="calibre8"><a class="calibre11" id="pgfId-1011914"></a>Freed from tedious calculations, the composer is able to devote himself to the general problems that the new musical form poses and to explore the nooks and crannies of this form while modifying the values of the input data. For example, he may test all instrumental combinations from soloists, to chamber orchestras, to large orchestras. With the aid of electronic computers the composer becomes a sort of pilot: he presses the buttons, introduces coordinates, and supervises the controls of a cosmic vessel sailing in the space of sound, across sonic constellations and galaxies that he could formerly glimpse only as a distant dream</i>.</p>

  <p class="body"><a id="pgfId-1011924"></a>In this chapter, we’ll explore from various angles the potential of deep learning to augment artistic creation. We’ll review sequence data generation (which can be used to generate text or music), DeepDream, and image generation using both variational autoencoders and generative adversarial networks. We’ll get your computer to dream up content never seen before; and maybe we’ll get you to dream, too, about the fantastic possibilities that lie at the intersection of technology and art. Let’s get started.</p>

  <h2 class="fm-head" id="heading_id_3"><a id="pgfId-1011930"></a>12.1 Text generation</h2>

  <p class="body"><a id="pgfId-1011949"></a><a id="marker-1011943"></a><a id="marker-1011945"></a>In this section, we’ll explore how recurrent neural networks can be used to generate sequence data. We’ll use text generation as an example, but the exact same techniques can be generalized to any kind of sequence data: you could apply it to sequences of musical notes in order to generate new music, to timeseries of brushstroke data (perhaps recorded while an artist paints on an iPad) to generate paintings stroke by stroke, and so on.</p>

  <p class="body"><a id="pgfId-1011954"></a>Sequence data generation is in no way limited to artistic content generation. It has been successfully applied to speech synthesis and to dialogue generation for chatbots. The Smart Reply feature that Google released in 2016, capable of automatically generating a selection of quick replies to emails or text messages, is powered by similar techniques.</p>

  <h3 class="fm-head1" id="heading_id_4"><a id="pgfId-1011960"></a>12.1.1 A brief history of generative deep learning for sequence generation</h3>

  <p class="body"><a id="pgfId-1011977"></a><a id="marker-1011971"></a><a id="marker-1011973"></a>In late 2014, few people had ever seen the initials LSTM, even in the machine learning community. Successful applications of sequence data generation with recurrent networks only began to appear in the mainstream in 2016. But these techniques have a fairly long history, starting with the development of the LSTM algorithm in 1997 (discussed in chapter 10). This new algorithm was used early on to generate text character by character.</p>

  <p class="body"><a id="pgfId-1011982"></a>In 2002, Douglas Eck, then at Schmidhuber’s lab in Switzerland, applied LSTM to music generation for the first time, with promising results. Eck is now a researcher at Google Brain, and in 2016 he started a new research group there, called Magenta, focused on applying modern deep learning techniques to produce engaging music. Sometimes good ideas take 15 years to get started.</p>

  <p class="body"><a id="pgfId-1012009"></a>In the late 2000s and early 2010s, Alex Graves did important pioneering work on using recurrent networks for sequence data generation. In particular, his 2013 work on applying recurrent mixture density networks to generate human-like handwriting using timeseries of pen positions is seen by some as a turning point.<a id="Id-1011991"></a><a href="#pgfId-1011991"><sup class="footnotenumber">2</sup></a> This specific application of neural networks at that specific moment in time captured for me the notion of <i class="fm-italics">machines that dream</i> and was a significant inspiration around the time I started developing Keras. Graves left a similar commented-out remark hidden in a 2013 LaTeX file uploaded to the preprint server arXiv: “Generating sequential data is the closest computers get to dreaming.” Several years later, we take a lot of these developments for granted, but at the time it was difficult to watch Graves’s demonstrations and not walk away awe-inspired by the possibilities. Between 2015 and 2017, recurrent neural networks were successfully used for text and dialogue generation, music generation, and speech synthesis.</p>

  <p class="body"><a id="pgfId-1012031"></a>Then around 2017–2018, the Transformer architecture started taking over recurrent neural networks, not just for supervised natural language processing tasks, but also for generative sequence <a id="marker-1012020"></a>models—in particular <i class="fm-italics">language modeling</i> (word-level text generation). The best-known example of a generative Transformer would be GPT-3, a 175 billion parameter text-generation model trained by the startup OpenAI on an astoundingly large text corpus, including most digitally available books, Wikipedia, and a large fraction of a crawl of the entire internet. GPT-3 made headlines in 2020 due to its capability to generate plausible-sounding text paragraphs on virtually any topic, a prowess that has fed a short-lived hype wave worthy of the most torrid AI summer. <a id="marker-1012036"></a><a id="marker-1012039"></a></p>

  <h3 class="fm-head1" id="heading_id_5"><a id="pgfId-1012045"></a>12.1.2 How do you generate sequence data?</h3>

  <p class="body"><a id="pgfId-1012084"></a><a id="marker-1012056"></a><a id="marker-1012058"></a>The universal way to generate sequence data in deep learning is to train a model (usually a Transformer or an RNN) to predict the next token or next few tokens in a sequence, using the previous tokens as input. For instance, given the input “the cat is on the,” the model is trained to predict the target “mat,” the next word. As usual when working with text data, tokens are typically words or characters, and any network that can model the probability of the next token given the previous ones is called a <i class="fm-italics">language model</i>. A language model captures <a id="marker-1012073"></a>the <i class="fm-italics">latent space</i> of language: its statistical structure.</p>

  <p class="body"><a id="pgfId-1012119"></a>Once you have such a trained language model, you can <i class="fm-italics">sample</i> from it (generate new sequences): you feed it an initial string of text (called <i class="fm-italics">conditioning data</i>), ask it to generate the next character or the next word (you can even generate several tokens at once), add the generated output back to the input data, and repeat the process many times (see figure 12.1). This loop allows you to generate sequences of arbitrary length that reflect the structure of the data on which the model was trained: sequences that look <i class="fm-italics">almost</i> like human-written sentences. <a id="marker-1012124"></a><a id="marker-1012127"></a></p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/12-01.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1062901"></a>Figure 12.1 The process of word-by-word text generation using a language model</p>

  <h3 class="fm-head1" id="heading_id_6"><a id="pgfId-1012143"></a>12.1.3 The importance of the sampling strategy</h3>

  <p class="body"><a id="pgfId-1012222"></a><a id="marker-1012168"></a><a id="marker-1012170"></a>When generating text, the way you choose the next token is crucially important. A naive approach is <i class="fm-italics">greedy sampling</i>, consisting of <a id="marker-1012185"></a>always choosing the most likely next character. But such an approach results in repetitive, predictable strings that don’t look like coherent language. A more interesting approach makes slightly more surprising choices: it introduces randomness in the sampling process by sampling from the probability distribution for the next character. This is <a id="marker-1012191"></a>called <i class="fm-italics">stochastic sampling</i> (recall that <i class="fm-italics">stochasticity</i> is what we call <i class="fm-italics">randomness</i> in this field). In such a setup, if a word has probability 0.3 of being next in the sentence according to the model, you’ll choose it 30% of the time. Note that greedy sampling can also be cast as sampling from a probability distribution: one where a certain word has probability 1 and all others have probability 0.</p>

  <p class="body"><a id="pgfId-1012231"></a>Sampling probabilistically from the softmax output of the model is neat: it allows even unlikely words to be sampled some of the time, generating more interesting-looking sentences and sometimes showing creativity by coming up with new, realistic-sounding sentences that didn’t occur in the training data. But there’s one issue with this strategy: it doesn’t offer a way to <i class="fm-italics">control the amount of randomness</i> in the sampling process.</p>

  <p class="body"><a id="pgfId-1012246"></a>Why would you want more or less randomness? Consider an extreme case: pure random sampling, where you draw the next word from a uniform probability distribution, and every word is equally likely. This scheme has maximum randomness; in other words, this probability distribution has maximum entropy. Naturally, it won’t produce anything interesting. At the other extreme, greedy sampling doesn’t produce anything interesting, either, and has no randomness: the corresponding probability distribution has minimum entropy. Sampling from the “real” probability distribution—the distribution that is output by the model’s softmax function—constitutes an intermediate point between these two extremes. But there are many other intermediate points of higher or lower entropy that you may want to explore. Less entropy will give the generated sequences a more predictable structure (and thus they will potentially be more realistic looking), whereas more entropy will result in more surprising and creative sequences. When sampling from generative models, it’s always good to explore different amounts of randomness in the generation process. Because we—humans—are the ultimate judges of how interesting the generated data is, interestingness is highly subjective, and there’s no telling in advance where the point of optimal entropy lies.</p>

  <p class="body"><a id="pgfId-1012274"></a>In order to control the amount of stochasticity in the sampling process, we’ll introduce a parameter called the <i class="fm-italics">softmax temperature</i>, which characterizes <a id="marker-1012263"></a>the entropy of the probability distribution used for sampling: it characterizes how surprising or predictable the choice of the next word will be. Given a <code class="fm-code-in-text">temperature</code> value, a new probability distribution is computed from the original one (the softmax output of the model) by reweighting it in the following way.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1012334"></a>Listing 12.1 Reweighting a probability distribution to a different temperature</p>
  <pre class="programlisting"><a id="pgfId-1012427"></a><b class="fm-codebrown">import</b> numpy <b class="fm-codebrown">as</b> np 
<a id="pgfId-1036514"></a><b class="fm-codebrown">def</b> reweight_distribution(original_distribution, temperature=<span class="fm-codeblue">0.5</span>):   <span class="fm-combinumeral">❶</span>
<a id="pgfId-1036525"></a>    distribution = np.log(original_distribution) / temperature
<a id="pgfId-1036532"></a>    distribution = np.exp(distribution)
<a id="pgfId-1036539"></a>    <b class="fm-codebrown">return</b> distribution / np.sum(distribution)                       <span class="fm-combinumeral">❷</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1061265"></a><span class="fm-combinumeral">❶</span> original_distribution is a 1D NumPy array of probability values that must sum to 1. temperature is a factor quantifying the entropy of the output distribution.</p>

  <p class="fm-code-annotation"><a id="pgfId-1061286"></a><span class="fm-combinumeral">❷</span> Returns a reweighted version of the original distribution. The sum of the distribution may no longer be 1, so you divide it by its sum to obtain the new distribution.</p>

  <p class="body"><a id="pgfId-1012469"></a>Higher temperatures result in sampling distributions of higher entropy that will generate more surprising and unstructured generated data, whereas a lower temperature will result in less randomness and much more predictable generated data (see figure 12.2).</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/12-02.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1062943"></a> <a id="marker-1012471"></a><a id="marker-1012474"></a>Figure 12.2 Different reweightings of one probability distribution. Low temperature = more deterministic, high temperature = more random.</p>

  <h3 class="fm-head1" id="heading_id_7"><a id="pgfId-1012490"></a>12.1.4 Implementing text generation with Keras</h3>

  <p class="body"><a id="pgfId-1012529"></a><a id="marker-1012515"></a><a id="marker-1012517"></a><a id="marker-1012519"></a>Let’s put these ideas into practice in a Keras implementation. The first thing you need is a lot of text data that you can use to learn a language model. You can use any sufficiently large text file or set of text files—Wikipedia, <i class="fm-italics">The Lord of the Rings</i>, and so on.</p>

  <p class="body"><a id="pgfId-1012538"></a>In this example, we’ll keep working with the IMDB movie review dataset from the last chapter, and we’ll learn to generate never-read-before movie reviews. As such, our language model will be a model of the style and topics of these movie reviews specifically, rather than a general model of the English language.</p>

  <p class="fm-head2"><a id="pgfId-1012544"></a>Preparing the data</p>

  <p class="body"><a id="pgfId-1012563"></a><a id="marker-1012555"></a><a id="marker-1012557"></a><a id="marker-1012559"></a>Just like in the previous chapter, let’s download and uncompress the IMDB movie reviews dataset.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1012619"></a>Listing 12.2 Downloading and uncompressing the IMDB movie reviews dataset</p>
  <pre class="programlisting"><a id="pgfId-1012568"></a>!wget https:/ /ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz
<a id="pgfId-1036546"></a>!tar -xf aclImdb_v1.tar.gz</pre>

  <p class="body"><a id="pgfId-1012677"></a>You’re already familiar with the structure of the data: we get a folder named aclImdb containing two subfolders, one for negative-sentiment movie reviews, and one for positive-sentiment reviews. There’s one text file per <a id="marker-1012656"></a>review. We’ll call <code class="fm-code-in-text">text_dataset_ from_directory</code> with <code class="fm-code-in-text">label_mode=None</code> to create a dataset that reads from these files and yields the text content of each file.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1012737"></a>Listing 12.3 Creating a dataset from text files (one file = one sample)</p>
  <pre class="programlisting"><a id="pgfId-1012860"></a><b class="fm-codebrown">import</b> tensorflow <b class="fm-codebrown">as</b> tf 
<a id="pgfId-1036553"></a><b class="fm-codebrown">from</b> tensorflow <b class="fm-codebrown">import</b> keras
<a id="pgfId-1036560"></a>dataset = keras.utils.text_dataset_from_directory(
<a id="pgfId-1036567"></a>    directory=<span class="fm-codegreen">"aclImdb"</span>, label_mode=<code class="fm-codegreen">None</code>, batch_size=<span class="fm-codeblue">256</span>)
<a id="pgfId-1036574"></a>dataset = dataset.map(<b class="fm-codebrown">lambda</b> x: tf.strings.regex_replace(x, <span class="fm-codegreen">"&lt;br /&gt;"</span>, <span class="fm-codegreen">" "</span>))<span class="fm-combinumeral">❶</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1061212"></a><span class="fm-combinumeral">❶</span> Strip the &lt;br /&gt; HTML tag that occurs in many of the reviews. This did not matter much for text classification, but we wouldn’t want to generate &lt;br /&gt; tags in this example!</p>

  <p class="body"><a id="pgfId-1012918"></a>Now let’s use a <code class="fm-code-in-text">TextVectorization</code> layer to <a id="marker-1012897"></a>compute the vocabulary we’ll be working with. We’ll only use the first <code class="fm-code-in-text">sequence_length</code> words of each review: our <code class="fm-code-in-text">TextVectorization</code> layer will cut off anything beyond that when vectorizing a text.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1012978"></a>Listing 12.4 Preparing a <code class="fm-code-in-text">TextVectorization</code> layer</p>
  <pre class="programlisting"><a id="pgfId-1013104"></a><b class="fm-codebrown">from</b> tensorflow.keras.layers <b class="fm-codebrown">import</b> TextVectorization
<a id="pgfId-1036581"></a>  
<a id="pgfId-1036588"></a>sequence_length = <span class="fm-codeblue">100</span> 
<a id="pgfId-1036595"></a>vocab_size = <span class="fm-codeblue">15000</span>                            <span class="fm-combinumeral">❶</span>
<a id="pgfId-1036602"></a>text_vectorization = TextVectorization(
<a id="pgfId-1036609"></a>    max_tokens=vocab_size,                
<a id="pgfId-1036616"></a>    output_mode=<span class="fm-codegreen">"int"</span>,                        <span class="fm-combinumeral">❷</span>
<a id="pgfId-1036623"></a>    output_sequence_length=sequence_length,   <span class="fm-combinumeral">❸</span>
<a id="pgfId-1036630"></a>)
<a id="pgfId-1036637"></a>text_vectorization.adapt(dataset)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1061053"></a><span class="fm-combinumeral">❶</span> We’ll only consider the top 15,000 most common words—anything else will be treated as the out-of-vocabulary token, "[UNK]".</p>

  <p class="fm-code-annotation"><a id="pgfId-1061070"></a><span class="fm-combinumeral">❷</span> We want to return integer word index sequences.</p>

  <p class="fm-code-annotation"><a id="pgfId-1061087"></a><span class="fm-combinumeral">❸</span> We’ll work with inputs and targets of length 100 (but since we’ll offset the targets by 1, the model will actually see sequences of length 99).</p>

  <p class="body"><a id="pgfId-1013165"></a>Let’s use the layer to create a language modeling dataset where input samples are vectorized texts, and corresponding targets are the same texts offset by one word.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1013222"></a>Listing 12.5 Setting up a language modeling dataset</p>
  <pre class="programlisting"><a id="pgfId-1013315"></a><b class="fm-codebrown">def</b> prepare_lm_dataset(text_batch):
<a id="pgfId-1036644"></a>    vectorized_sequences = text_vectorization(text_batch)    <span class="fm-combinumeral">❶</span>
<a id="pgfId-1036651"></a>    x = vectorized_sequences[:, :-<span class="fm-codeblue">1</span>]                         <span class="fm-combinumeral">❷</span>
<a id="pgfId-1036658"></a>    y = vectorized_sequences[:, <span class="fm-codeblue">1</span>:]                          <span class="fm-combinumeral">❸</span>
<a id="pgfId-1036665"></a>    <b class="fm-codebrown">return</b> x, y
<a id="pgfId-1036672"></a>  
<a id="pgfId-1036679"></a>lm_dataset = dataset.map(prepare_lm_dataset, num_parallel_calls=<span class="fm-codeblue">4</span>)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1060856"></a><span class="fm-combinumeral">❶</span> Convert a batch of texts (strings) to a batch of integer sequences.</p>

  <p class="fm-code-annotation"><a id="pgfId-1060877"></a><span class="fm-combinumeral">❷</span> Create inputs by cutting off the last word of the sequences.</p>

  <p class="fm-code-annotation"><a id="pgfId-1060894"></a><span class="fm-combinumeral">❸</span> Create targets by offsetting the sequences by 1. <a id="marker-1060899"></a><a id="marker-1060900"></a><a id="marker-1060901"></a></p>

  <p class="fm-head2"><a id="pgfId-1013383"></a>A Transformer-based sequence-to-sequence model</p>

  <p class="body"><a id="pgfId-1013404"></a><a id="marker-1013394"></a><a id="marker-1013396"></a><a id="marker-1013398"></a><a id="marker-1013400"></a>We’ll train a model to predict a probability distribution over the next word in a sentence, given a number of initial words. When the model is trained, we’ll feed it with a prompt, sample the next word, add that word back to the prompt, and repeat, until we’ve generated a short paragraph.</p>

  <p class="body"><a id="pgfId-1013409"></a>Like we did for temperature forecasting in chapter 10, we could train a model that takes as input a sequence of <i class="fm-italics">N</i> words and simply predicts word <i class="fm-italics">N</i>+1. However, there are several issues with this setup in the context of sequence generation.</p>

  <p class="body"><a id="pgfId-1013415"></a>First, the model would only learn to produce predictions when <i class="fm-italics">N</i> words were available, but it would be useful to be able to start predicting with fewer than <i class="fm-italics">N</i> words. Otherwise we’d be constrained to only use relatively long prompts (in our implementation, <i class="fm-italics">N</i>=100 words). We didn’t have this need in chapter 10.</p>

  <p class="body"><a id="pgfId-1013421"></a>Second, many of our training sequences will be mostly overlapping. Consider <code class="fm-code-in-text">N</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">4</code>. The text “A complete sentence must have, at minimum, three things: a subject, verb, and an object” would be used to generate the following training sequences:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1013436"></a>“A complete sentence must”</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1013450"></a>“complete sentence must have”</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1013460"></a>“sentence must have at”</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1013470"></a>and so on, until “verb and an object”</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1013480"></a>A model that treats each such sequence as an independent sample would have to do a lot of redundant work, re-encoding multiple times subsequences that it has largely seen before. In chapter 10, this wasn’t much of a problem, because we didn’t have that many training samples in the first place, and we needed to benchmark dense and convolutional models, for which redoing the work every time is the only option. We could try to alleviate this redundancy problem by using <i class="fm-italics">strides</i> to sample our sequences—skipping a few words between two consecutive samples. But that would reduce our number of training samples while only providing a partial solution.</p>

  <p class="body"><a id="pgfId-1013587"></a>To address these two issues, we’ll use a <i class="fm-italics">sequence-to-sequence model</i>: we’ll feed <a id="marker-1050865"></a>sequences of <i class="fm-italics">N</i> words (indexed from 0 to <i class="fm-italics">N</i>) into our model, and we’ll predict the sequence offset by one (from 1 to <code class="fm-code-in-text">N+1</code>). We’ll use causal masking to make sure that, for any <code class="fm-code-in-text">i</code>, the model will only be using words from 0 to <code class="fm-code-in-text">i</code> in order to predict the word <code class="fm-code-in-text">i</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">1</code>. This means that we’re simultaneously training the model to solve <i class="fm-italics">N</i> mostly overlapping but different problems: predicting the next words given a sequence of <code class="fm-code-in-text">1</code> <code class="fm-code-in-text">&lt;=</code> <code class="fm-code-in-text">i</code> <code class="fm-code-in-text">&lt;=</code> <code class="fm-code-in-text">N</code> prior words (see figure 12.3). At generation time, even if you only prompt the model with a single word, it will be able to give you a probability distribution for the next possible words.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/12-03.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1062985"></a>Figure 12.3 Compared to plain next-word prediction, sequence-to-sequence modeling simultaneously optimizes for multiple prediction problems.</p>

  <p class="body"><a id="pgfId-1013606"></a>Note that we could have used a similar sequence-to-sequence setup on our temperature forecasting problem in chapter 10: given a sequence of 120 hourly data points, learn to generate a sequence of 120 temperatures offset by 24 hours in the future. You’d be not only solving the initial problem, but also solving the 119 related problems of forecasting temperature in 24 hours, given <code class="fm-code-in-text">1</code> <code class="fm-code-in-text">&lt;=</code> <code class="fm-code-in-text">i</code> <code class="fm-code-in-text">&lt;</code> <code class="fm-code-in-text">120</code> prior hourly data points. If you try to retrain the RNNs from chapter 10 in a sequence-to-sequence setup, you’ll find that you get similar but incrementally worse results, because the constraint of solving these additional 119 related problems with the same model interferes slightly with the task we actually do care about.</p>

  <p class="body"><a id="pgfId-1013651"></a>In the previous chapter, you learned about the setup you can use for sequence-to-sequence learning in the general case: feed the source sequence into an encoder, and then feed both the encoded sequence and the target sequence into a decoder that tries to predict the same target sequence offset by one step. When you’re doing text generation, there is no source sequence: you’re just trying to predict the next tokens in the target sequence given past tokens, which we can do using only the decoder. And thanks to causal padding, the decoder will only look at words <code class="fm-code-in-text">0...N</code> to predict the word <code class="fm-code-in-text">N+1</code>.</p>

  <p class="body"><a id="pgfId-1013676"></a>Let’s implement our model—we’re going to reuse the building blocks we created in chapter 11: <code class="fm-code-in-text">PositionalEmbedding</code> and <code class="fm-code-in-text">TransformerDecoder</code>.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1013736"></a>Listing 12.6 A simple Transformer-based language model</p>
  <pre class="programlisting"><a id="pgfId-1013859"></a><b class="fm-codebrown">from</b> tensorflow.keras <b class="fm-codebrown">import</b> layers
<a id="pgfId-1036686"></a>embed_dim = <span class="fm-codeblue">256</span> 
<a id="pgfId-1036693"></a>latent_dim = <span class="fm-codeblue">2048</span> 
<a id="pgfId-1036700"></a>num_heads = <span class="fm-codeblue">2</span> 
<a id="pgfId-1036707"></a>  
<a id="pgfId-1036714"></a>inputs = keras.Input(shape=(<code class="fm-codegreen">None</code>,), dtype=<span class="fm-codegreen">"int64"</span>)
<a id="pgfId-1036721"></a>x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)
<a id="pgfId-1036728"></a>x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, x)
<a id="pgfId-1036735"></a>outputs = layers.Dense(vocab_size, activation=<span class="fm-codegreen">"softmax"</span>)(x)       <span class="fm-combinumeral">❶</span>
<a id="pgfId-1036742"></a>model = keras.Model(inputs, outputs)
<a id="pgfId-1036749"></a>model.compile(loss=<span class="fm-codegreen">"sparse_categorical_crossentropy"</span>, optimizer=<span class="fm-codegreen">"rmsprop"</span>)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1060801"></a><span class="fm-combinumeral">❶</span> Softmax over possible vocabulary words, computed for each output sequence timestep.</p>

  <h3 class="fm-head1" id="heading_id_8"><a id="pgfId-1013903"></a>12.1.5 A text-generation callback with variable-temperature sampling</h3>

  <p class="body"><a id="pgfId-1013924"></a><a id="marker-1013914"></a><a id="marker-1013916"></a><a id="marker-1013918"></a><a id="marker-1013920"></a>We’ll use a callback to generate text using a range of different temperatures after every epoch. This allows you to see how the generated text evolves as the model begins to converge, as well as the impact of temperature in the sampling strategy. To seed text generation, we’ll use the prompt “this movie”: all of our generated texts will start with this.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1013980"></a>Listing 12.7 The text-generation callback</p>
  <pre class="programlisting"><a id="pgfId-1014453"></a><b class="fm-codebrown">import</b> numpy <b class="fm-codebrown">as</b> np
<a id="pgfId-1036756"></a>  
<a id="pgfId-1036763"></a>tokens_index = dict(enumerate(text_vectorization.get_vocabulary()))    <span class="fm-combinumeral">❶</span>
<a id="pgfId-1036770"></a>  
<a id="pgfId-1036777"></a><b class="fm-codebrown">def</b> sample_next(predictions, temperature=<span class="fm-codeblue">1.0</span>):                         <span class="fm-combinumeral">❷</span>
<a id="pgfId-1036784"></a>    predictions = np.asarray(predictions).astype(<span class="fm-codegreen">"float64"</span>)
<a id="pgfId-1036791"></a>    predictions = np.log(predictions) / temperature
<a id="pgfId-1036798"></a>    exp_preds = np.exp(predictions)
<a id="pgfId-1036805"></a>    predictions = exp_preds / np.sum(exp_preds)
<a id="pgfId-1036812"></a>    probas = np.random.multinomial(<span class="fm-codeblue">1</span>, predictions, <span class="fm-codeblue">1</span>)
<a id="pgfId-1036819"></a>    <b class="fm-codebrown">return</b> np.argmax(probas)
<a id="pgfId-1036826"></a>  
<a id="pgfId-1036833"></a><b class="fm-codebrown">class</b> TextGenerator(keras.callbacks.Callback):
<a id="pgfId-1036840"></a>    <b class="fm-codebrown">def</b> __init__(self,
<a id="pgfId-1036847"></a>                 prompt,                                               <span class="fm-combinumeral">❸</span>
<a id="pgfId-1036854"></a>                 generate_length,                                      <span class="fm-combinumeral">❹</span>
<a id="pgfId-1036861"></a>                 model_input_length,
<a id="pgfId-1036868"></a>                 temperatures=(<span class="fm-codeblue">1.</span>,),                                   <span class="fm-combinumeral">❺</span>
<a id="pgfId-1036875"></a>                 print_freq=<span class="fm-codeblue">1</span>):
<a id="pgfId-1036882"></a>        self.prompt = prompt
<a id="pgfId-1036889"></a>        self.generate_length = generate_length
<a id="pgfId-1036896"></a>        self.model_input_length = model_input_length
<a id="pgfId-1036903"></a>        self.temperatures = temperatures
<a id="pgfId-1036910"></a>        self.print_freq = print_freq
<a id="pgfId-1036917"></a>  
<a id="pgfId-1036924"></a>    <b class="fm-codebrown">def</b> on_epoch_end(self, epoch, logs=<code class="fm-codegreen">None</code>):
<a id="pgfId-1036931"></a>        <b class="fm-codebrown">if</b> (epoch + <span class="fm-codeblue">1</span>) % self.print_freq != <span class="fm-codeblue">0</span>:
<a id="pgfId-1036938"></a>            <b class="fm-codebrown">return</b>
<a id="pgfId-1051019"></a>        <b class="fm-codebrown">for</b> temperature <b class="fm-codebrown">in</b> self.temperatures:
<a id="pgfId-1036945"></a>            <b class="fm-codebrown">print</b>(<span class="fm-codegreen">"== Generating with temperature"</span>, temperature)
<a id="pgfId-1036952"></a>            sentence = self.prompt                                     <span class="fm-combinumeral">❻</span>
<a id="pgfId-1036959"></a>            <b class="fm-codebrown">for</b> i <b class="fm-codebrown">in</b> range(self.generate_length):
<a id="pgfId-1036966"></a>                tokenized_sentence = text_vectorization([sentence])    <span class="fm-combinumeral">❼</span>
<a id="pgfId-1036973"></a>                predictions = self.model(tokenized_sentence)           <span class="fm-combinumeral">❼</span>
<a id="pgfId-1036980"></a>                next_token = sample_next(predictions[<span class="fm-codeblue">0</span>, i, :])         <span class="fm-combinumeral">❽</span>
<a id="pgfId-1036987"></a>                sampled_token = tokens_index[next_token]               <span class="fm-combinumeral">❽</span>
<a id="pgfId-1036994"></a>                sentence += <span class="fm-codegreen">" "</span> + sampled_token                        <span class="fm-combinumeral">❾</span>
<a id="pgfId-1037001"></a>            <b class="fm-codebrown">print</b>(sentence)
<a id="pgfId-1037008"></a>  
<a id="pgfId-1037015"></a>prompt = <span class="fm-codegreen">"This movie"</span> 
<a id="pgfId-1037022"></a>text_gen_callback = TextGenerator(
<a id="pgfId-1037029"></a>    prompt,
<a id="pgfId-1037036"></a>    generate_length=<span class="fm-codeblue">50</span>,
<a id="pgfId-1037043"></a>    model_input_length=sequence_length,
<a id="pgfId-1037050"></a>    temperatures=(<span class="fm-codeblue">0.2</span>, <span class="fm-codeblue">0.5</span>, <span class="fm-codeblue">0.7</span>, <span class="fm-codeblue">1.</span>, <span class="fm-codeblue">1.5</span>))                             <span class="fm-combinumeral">❿</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1060035"></a><span class="fm-combinumeral">❶</span> Dict that maps word indices back to strings, to be used for text decoding</p>

  <p class="fm-code-annotation"><a id="pgfId-1060056"></a><span class="fm-combinumeral">❷</span> Implements variable-temperature sampling from a probability distribution</p>

  <p class="fm-code-annotation"><a id="pgfId-1060076"></a><span class="fm-combinumeral">❸</span> Prompt that we use to seed text generation</p>

  <p class="fm-code-annotation"><a id="pgfId-1060093"></a><span class="fm-combinumeral">❹</span> How many words to generate</p>

  <p class="fm-code-annotation"><a id="pgfId-1060110"></a><span class="fm-combinumeral">❺</span> Range of temperatures to use for sampling</p>

  <p class="fm-code-annotation"><a id="pgfId-1060127"></a><span class="fm-combinumeral">❻</span> When generating text, we start from our prompt.</p>

  <p class="fm-code-annotation"><a id="pgfId-1060144"></a><span class="fm-combinumeral">❼</span> Feed the current sequence into our model.</p>

  <p class="fm-code-annotation"><a id="pgfId-1060161"></a><span class="fm-combinumeral">❽</span> Retrieve the predictions for the last timestep, and use them to sample a new word.</p>

  <p class="fm-code-annotation"><a id="pgfId-1060190"></a><span class="fm-combinumeral">❾</span> Append the new word to the current sequence and repeat.</p>

  <p class="fm-code-annotation"><a id="pgfId-1060207"></a><span class="fm-combinumeral">❿</span> We’ll use a diverse range of temperatures to sample text, to demonstrate the effect of temperature on text generation.</p>

  <p class="body"><a id="pgfId-1014623"></a>Let’s <code class="fm-code-in-text">fit()</code> this thing.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1014689"></a>Listing 12.8 Fitting the language model</p>
  <pre class="programlisting"><a id="pgfId-1014638"></a>model.fit(lm_dataset, epochs=<span class="fm-codeblue">200</span>, callbacks=[text_gen_callback])</pre>

  <p class="body"><a id="pgfId-1014733"></a>Here are some cherrypicked examples of what we’re able to generate after 200 epochs of training. Note that punctuation isn’t part of our vocabulary, so none of our generated text has any punctuation:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1014739"></a>With <code class="fm-code-in-text">temperature=0.2</code></p>

      <ul class="calibre16">
        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014759"></a>“this movie is a [UNK] of the original movie and the first half hour of the movie is pretty good but it is a very good movie it is a good movie for the time period”</p>
        </li>

        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014777"></a>“this movie is a [UNK] of the movie it is a movie that is so bad that it is a [UNK] movie it is a movie that is so bad that it makes you laugh and cry at the same time it is not a movie i dont think ive ever seen”</p>
        </li>
      </ul>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1014787"></a>With <code class="fm-code-in-text">temperature=0.5</code></p>

      <ul class="calibre16">
        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014803"></a>“this movie is a [UNK] of the best genre movies of all time and it is not a good movie it is the only good thing about this movie i have seen it for the first time and i still remember it being a [UNK] movie i saw a lot of years”</p>
        </li>

        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014821"></a>“this movie is a waste of time and money i have to say that this movie was a complete waste of time i was surprised to see that the movie was made up of a good movie and the movie was not very good but it was a waste of time and”</p>
        </li>
      </ul>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1014831"></a>With <code class="fm-code-in-text">temperature=0.7</code></p>

      <ul class="calibre16">
        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014847"></a>“this movie is fun to watch and it is really funny to watch all the characters are extremely hilarious also the cat is a bit like a [UNK] [UNK] and a hat [UNK] the rules of the movie can be told in another scene saves it from being in the back of”</p>
        </li>

        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014865"></a>“this movie is about [UNK] and a couple of young people up on a small boat in the middle of nowhere one might find themselves being exposed to a [UNK] dentist they are killed by [UNK] i was a huge fan of the book and i havent seen the original so it”</p>
        </li>
      </ul>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1014875"></a>With <code class="fm-code-in-text">temperature=1.0</code></p>

      <ul class="calibre16">
        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014891"></a>“this movie was entertaining i felt the plot line was loud and touching but on a whole watch a stark contrast to the artistic of the original we watched the original version of england however whereas arc was a bit of a little too ordinary the [UNK] were the present parent [UNK]”</p>
        </li>

        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014909"></a>“this movie was a masterpiece away from the storyline but this movie was simply exciting and frustrating it really entertains friends like this the actors in this movie try to go straight from the sub thats image and they make it a really good tv show”</p>
        </li>
      </ul>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1014919"></a>With <code class="fm-code-in-text">temperature=1.5</code></p>

      <ul class="calibre16">
        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014935"></a>“this movie was possibly the worst film about that 80 women its as weird insightful actors like barker movies but in great buddies yes no decorated shield even [UNK] land dinosaur ralph ian was must make a play happened falls after miscast [UNK] bach not really not wrestlemania seriously sam didnt exist”</p>
        </li>

        <li class="fm-list-bullet1">
          <p class="list"><a class="calibre11" id="pgfId-1014953"></a>“this movie could be so unbelievably lucas himself bringing our country wildly funny things has is for the garish serious and strong performances colin writing more detailed dominated but before and that images gears burning the plate patriotism we you expected dyan bosses devotion to must do your own duty and another”</p>
        </li>
      </ul>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1014963"></a>As you can see, a low temperature value results in very boring and repetitive text and can sometimes cause the generation process to get stuck in a loop. With higher temperatures, the generated text becomes more interesting, surprising, even creative. With a very high temperature, the local structure starts to break down, and the output looks largely random. Here, a good generation temperature would seem to be about 0.7. Always experiment with multiple sampling strategies! A clever balance between learned structure and randomness is what makes generation interesting.</p>

  <p class="body"><a id="pgfId-1014969"></a>Note that by training a bigger model, longer, on more data, you can achieve generated samples that look far more coherent and realistic than this one—the output of a model like GPT-3 is a good example of what can be done with language models (GPT-3 is effectively the same thing as what we trained in this example, but with a deep stack of Transformer decoders, and a much bigger training corpus). But don’t expect to ever generate any meaningful text, other than through random chance and the magic of your own interpretation: all you’re doing is sampling data from a statistical model of which words come after which words. Language models are all form and no substance.</p>

  <p class="body"><a id="pgfId-1014975"></a>Natural language is many things: a communication channel, a way to act on the world, a social lubricant, a way to formulate, store, and retrieve your own thoughts . . . These uses of languages are where its meaning originates. A deep learning “language model,” despite its name, captures effectively none of these fundamental aspects of language. It cannot communicate (it has nothing to communicate about and no one to communicate with), it cannot act on the world (it has no agency and no intent), it cannot be social, and it doesn’t have any thoughts to process with the help of words. Language is the operating system of the mind, and so, for language to be meaningful, it needs a mind to leverage it.</p>

  <p class="body"><a id="pgfId-1014981"></a>What a language model does is capture the statistical structure of the observable artifacts—books, online movie reviews, tweets—that we generate as we use language to live our lives. The fact that these artifacts have a statistical structure at all is a side effect of how humans implement language. Here’s a thought experiment: what if our languages did a better job of compressing communications, much like computers do with most digital communications? Language would be no less meaningful and could still fulfill its many purposes, but it would lack any intrinsic statistical structure, thus making it impossible to model as you just did. <a id="marker-1014983"></a><a id="marker-1014986"></a><a id="marker-1014988"></a><a id="marker-1014990"></a></p>

  <h3 class="fm-head1" id="heading_id_9"><a id="pgfId-1014996"></a>12.1.6 Wrapping up</h3>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1015006"></a>You can generate discrete sequence data by training a model to predict the next token(s), given previous tokens.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1015020"></a>In the case of text, such a model is called a <i class="fm-italics1">language model</i>. It can be based on either words or characters.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1015039"></a>Sampling the next token requires a balance between adhering to what the model judges likely, and introducing randomness.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1015049"></a>One way to handle this is the notion of softmax temperature. Always experiment with different temperatures to find the right one. <a class="calibre11" id="marker-1015058"></a><a class="calibre11" id="marker-1015060"></a></p>
    </li>
  </ul>

  <h2 class="fm-head" id="heading_id_10"><a id="pgfId-1015066"></a>12.2 DeepDream</h2>

  <p class="body"><a id="pgfId-1015087"></a><a id="marker-1015079"></a><a id="marker-1015081"></a><i class="fm-italics">DeepDream</i> is an artistic image-modification technique that uses the representations learned by convolutional neural networks. It was first released by Google in the summer of 2015 as an implementation written using the Caffe deep learning library (this was several months before the first public release of TensorFlow).<a id="Id-1015093"></a><a href="#pgfId-1015093"><sup class="footnotenumber">3</sup></a> It quickly became an internet sensation thanks to the trippy pictures it could generate (see, for example, figure 12.4), full of algorithmic pareidolia artifacts, bird feathers, and dog eyes—a byproduct of the fact that the DeepDream convnet was trained on ImageNet, where dog breeds and bird species are vastly overrepresented.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/12-04.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1063027"></a>Figure 12.4 Example of a DeepDream output image</p>

  <p class="body"><a id="pgfId-1015120"></a>The DeepDream algorithm is almost identical to the convnet filter-visualization technique introduced in chapter 9, consisting of running a convnet in reverse: doing gradient ascent on the input to the convnet in order to maximize the activation of a specific filter in an upper layer of the convnet. DeepDream uses this same idea, with a few simple differences:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1015140"></a>With DeepDream, you try to maximize the activation of entire layers rather than that of a specific filter, thus mixing together visualizations of large numbers of features at once.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1015154"></a>You start not from blank, slightly noisy input, but rather from an existing image—thus the resulting effects latch on to preexisting visual patterns, distorting elements of the image in a somewhat artistic fashion.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1015164"></a>The input images are processed at different scales (called <i class="fm-italics1">octaves</i>), which improves the quality of the visualizations.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1015183"></a>Let’s make some DeepDreams.</p>

  <h3 class="fm-head1" id="heading_id_11"><a id="pgfId-1015189"></a>12.2.1 Implementing DeepDream in Keras</h3>

  <p class="body"><a id="pgfId-1015199"></a><a id="marker-1015200"></a>Let’s start by retrieving a test image to dream with. We’ll use a view of the rugged Northern California coast in the winter (figure 12.5).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1015259"></a>Listing 12.9 Fetching the test image</p>
  <pre class="programlisting"><a id="pgfId-1015352"></a><b class="fm-codebrown">from</b> tensorflow <b class="fm-codebrown">import</b> keras 
<a id="pgfId-1037057"></a><b class="fm-codebrown">import</b> matplotlib.pyplot <b class="fm-codebrown">as</b> plt
<a id="pgfId-1037064"></a>  
<a id="pgfId-1037071"></a>base_image_path = keras.utils.get_file(
<a id="pgfId-1037078"></a>    <span class="fm-codegreen">"coast.jpg"</span>, origin=<span class="fm-codegreen">"https:/ /img-datasets.s3.amazonaws.com/coast.jpg"</span>)
<a id="pgfId-1037085"></a>  
<a id="pgfId-1037092"></a>plt.axis(<span class="fm-codegreen">"off"</span>)
<a id="pgfId-1037099"></a>plt.imshow(keras.utils.load_img(base_image_path))</pre>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/12-05.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1063076"></a>Figure 12.5 Our test image</p>

  <p class="body"><a id="pgfId-1015371"></a>Next, we need a pretrained convnet. In Keras, many such convnets are available: VGG16, VGG19, Xception, ResNet50, and so on, all available with weights pretrained on ImageNet. You can implement DeepDream with any of them, but your base model of choice will naturally affect your visualizations, because different architectures result in different learned features. The convnet used in the original DeepDream release was an Inception model, and in practice, Inception is known to produce nice-looking DeepDreams, so we’ll use the Inception V3 model that comes with Keras.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1015442"></a>Listing 12.10 Instantiating a pretrained <code class="fm-code-in-text">InceptionV3</code> model</p>
  <pre class="programlisting"><a id="pgfId-1015508"></a><b class="fm-codebrown">from</b> tensorflow.keras.applications <b class="fm-codebrown">import</b> inception_v3
<a id="pgfId-1037106"></a>model = inception_v3.InceptionV3(weights=<span class="fm-codegreen">"imagenet"</span>, include_top=<code class="fm-codegreen">False</code>)</pre>

  <p class="body"><a id="pgfId-1015530"></a>We’ll use our pretrained convnet to create a feature exactor model that returns the activations of the various intermediate layers, listed in the following code. For each layer, we pick a scalar score that weights the contribution of the layer to the loss we will seek to maximize during the gradient ascent process. If you want a complete list of layer names that you can use to pick new layers to play <a id="marker-1015519"></a>with, just use <code class="fm-code-in-text">model.summary()</code>.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1015590"></a>Listing 12.11 Configuring the contribution of each layer to the DeepDream loss</p>
  <pre class="programlisting"><a id="pgfId-1015765"></a>layer_settings = {                                                         <span class="fm-combinumeral">❶</span>
<a id="pgfId-1037113"></a>    <span class="fm-codegreen">"mixed4"</span>: <span class="fm-codeblue">1.0</span>,
<a id="pgfId-1037120"></a>    <span class="fm-codegreen">"mixed5"</span>: <span class="fm-codeblue">1.5</span>,
<a id="pgfId-1037127"></a>    <span class="fm-codegreen">"mixed6"</span>: <span class="fm-codeblue">2.0</span>,
<a id="pgfId-1037134"></a>    <span class="fm-codegreen">"mixed7"</span>: <span class="fm-codeblue">2.5</span>,
<a id="pgfId-1037141"></a>}
<a id="pgfId-1037148"></a>outputs_dict = dict(                                                       <span class="fm-combinumeral">❷</span>
<a id="pgfId-1037155"></a>    [
<a id="pgfId-1037162"></a>        (layer.name, layer.output)
<a id="pgfId-1037169"></a>        <b class="fm-codebrown">for</b> layer <b class="fm-codebrown">in</b> [model.get_layer(name)
<a id="pgfId-1051083"></a>                     <b class="fm-codebrown">for</b> name <b class="fm-codebrown">in</b> layer_settings.keys()]
<a id="pgfId-1037176"></a>    ]
<a id="pgfId-1037183"></a>)
<a id="pgfId-1037190"></a>feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict) <span class="fm-combinumeral">❸</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1059780"></a><span class="fm-combinumeral">❶</span> Layers for which we try to maximize activation, as well as their weight in the total loss. You can tweak these setting to obtain new visual effects.</p>

  <p class="fm-code-annotation"><a id="pgfId-1059801"></a><span class="fm-combinumeral">❷</span> Symbolic outputs of each layer</p>

  <p class="fm-code-annotation"><a id="pgfId-1059818"></a><span class="fm-combinumeral">❸</span> Model that returns the activation values for every target layer (as a dict)</p>

  <p class="body"><a id="pgfId-1015823"></a>Next, we’ll compute the <i class="fm-italics">loss</i>: the quantity we’ll seek to maximize during the gradient-ascent process at each processing scale. In chapter 9, for filter visualization, we tried to maximize the value of a specific filter in a specific layer. Here, we’ll simultaneously maximize the activation of all filters in a number of layers. Specifically, we’ll maximize a weighted mean of the L2 norm of the activations of a set of high-level layers. The exact set of layers we choose (as well as their contribution to the final loss) has a major influence on the visuals we’ll be able to produce, so we want to make these parameters easily configurable. Lower layers result in geometric patterns, whereas higher layers result in visuals in which you can recognize some classes from ImageNet (for example, birds or dogs). We’ll start from a somewhat arbitrary configuration involving four layers—but you’ll definitely want to explore many different configurations later.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1015889"></a>Listing 12.12 The DeepDream loss</p>
  <pre class="programlisting"><a id="pgfId-1016022"></a><b class="fm-codebrown">def</b> compute_loss(input_image):
<a id="pgfId-1037197"></a>    features = feature_extractor(input_image)                                  <span class="fm-combinumeral">❶</span>
<a id="pgfId-1037204"></a>    loss = tf.zeros(shape=())                                                  <span class="fm-combinumeral">❷</span>
<a id="pgfId-1037211"></a>    <b class="fm-codebrown">for</b> name <b class="fm-codebrown">in</b> features.keys():
<a id="pgfId-1037218"></a>        coeff = layer_settings[name]
<a id="pgfId-1037225"></a>        activation = features[name]
<a id="pgfId-1037232"></a>        loss += coeff * tf.reduce_mean(tf.square(activation[:, <span class="fm-codeblue">2</span>:-<span class="fm-codeblue">2</span>, <span class="fm-codeblue">2</span>:-<span class="fm-codeblue">2</span>, :]))<span class="fm-combinumeral">❸</span>
<a id="pgfId-1037239"></a>    <b class="fm-codebrown">return</b> loss</pre>

  <p class="fm-code-annotation"><a id="pgfId-1059512"></a><span class="fm-combinumeral">❶</span> Extract activations.</p>

  <p class="fm-code-annotation"><a id="pgfId-1059533"></a><span class="fm-combinumeral">❷</span> Initialize the loss to 0.</p>

  <p class="fm-code-annotation"><a id="pgfId-1059550"></a><span class="fm-combinumeral">❸</span> We avoid border artifacts by only involving non-border pixels in the loss.</p>

  <p class="body"><a id="pgfId-1016083"></a>Now let’s set up the gradient ascent process that we will run at each octave. You’ll recognize that it’s the same thing as the filter-visualization technique from chapter 9! The DeepDream algorithm is simply a multiscale form of filter visualization.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1016140"></a>The DeepDream gradient ascent process</p>
  <pre class="programlisting"><a id="pgfId-1016453"></a><b class="fm-codebrown">import</b> tensorflow <b class="fm-codebrown">as</b> tf
<a id="pgfId-1037246"></a>  
<a id="pgfId-1037253"></a><code class="fm-codeblue">@tf.function</code>                                                               <span class="fm-combinumeral">❶</span>
<a id="pgfId-1037260"></a><b class="fm-codebrown">def</b> gradient_ascent_step(image, learning_rate):
<a id="pgfId-1037267"></a>    <b class="fm-codebrown">with</b> tf.GradientTape() <b class="fm-codebrown">as</b> tape:                                        <span class="fm-combinumeral">❷</span>
<a id="pgfId-1037274"></a>        tape.watch(image)                                                  <span class="fm-combinumeral">❷</span>
<a id="pgfId-1037281"></a>        loss = compute_loss(image)                                         <span class="fm-combinumeral">❷</span>
<a id="pgfId-1037288"></a>    grads = tape.gradient(loss, image)                                     <span class="fm-combinumeral">❷</span>
<a id="pgfId-1037295"></a>    grads = tf.math.l2_normalize(grads)                                    <span class="fm-combinumeral">❸</span>
<a id="pgfId-1037302"></a>    image += learning_rate * grads
<a id="pgfId-1037309"></a>    <b class="fm-codebrown">return</b> loss, image
<a id="pgfId-1037316"></a>  
<a id="pgfId-1037323"></a>  
<a id="pgfId-1037330"></a><b class="fm-codebrown">def</b> gradient_ascent_loop(image, iterations, learning_rate, max_loss=<code class="fm-codegreen">None</code>): <span class="fm-combinumeral">❹</span>
<a id="pgfId-1037337"></a>    <b class="fm-codebrown">for</b> i <b class="fm-codebrown">in</b> range(iterations):                                            <span class="fm-combinumeral">❺</span>
<a id="pgfId-1037344"></a>        loss, image = gradient_ascent_step(image, learning_rate)           <span class="fm-combinumeral">❺</span>
<a id="pgfId-1037351"></a>        <b class="fm-codebrown">if</b> max_loss <b class="fm-codebrown">is</b> <b class="fm-codebrown">not</b> <code class="fm-codegreen">None</code> <b class="fm-codebrown">and</b> loss &gt; max_loss:                       <span class="fm-combinumeral">❻</span>
<a id="pgfId-1037358"></a>            <b class="fm-codebrown">break</b>                                                          <span class="fm-combinumeral">❻</span>
<a id="pgfId-1037365"></a>        <b class="fm-codebrown">print</b>(f<span class="fm-codegreen">"... Loss value at step {i}: {loss:.2f}"</span>)
<a id="pgfId-1037372"></a>    <b class="fm-codebrown">return</b> image</pre>

  <p class="fm-code-annotation"><a id="pgfId-1058888"></a><span class="fm-combinumeral">❶</span> We make the training step fast by compiling it as a tf.function.</p>

  <p class="fm-code-annotation"><a id="pgfId-1058912"></a><span class="fm-combinumeral">❷</span> Compute gradients of DeepDream loss with respect to the current image.</p>

  <p class="fm-code-annotation"><a id="pgfId-1058929"></a><span class="fm-combinumeral">❸</span> Normalize gradients (the same trick we used in chapter 9).</p>

  <p class="fm-code-annotation"><a id="pgfId-1058946"></a><span class="fm-combinumeral">❹</span> This runs gradient ascent for a given image scale (octave).</p>

  <p class="fm-code-annotation"><a id="pgfId-1058963"></a><span class="fm-combinumeral">❺</span> Repeatedly update the image in a way that increases the DeepDream loss.</p>

  <p class="fm-code-annotation"><a id="pgfId-1058980"></a><span class="fm-combinumeral">❻</span> Break out if the loss crosses a certain threshold (over-optimizing would create unwanted image artifacts).</p>

  <p class="body"><a id="pgfId-1016595"></a>Finally, the outer loop of the DeepDream algorithm. First, we’ll define a <a id="marker-1016564"></a>list of <i class="fm-italics">scales</i> (also called <i class="fm-italics">octaves</i>) at which to process the images. We’ll process our image over three different such “octaves.” For each successive octave, from the smallest to the largest, we’ll run 20 gradient ascent steps via <code class="fm-code-in-text">gradient_ascent_loop()</code> to maximize the loss we previously defined. Between each octave, we’ll upscale the image by 40% (1.4x): we’ll start by processing a small image and then increasingly scale it up (see figure 12.6).</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/12-06.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1063118"></a>Figure 12.6 The DeepDream process: successive scales of spatial processing (octaves) and detail re-injection upon upscaling</p>

  <p class="body"><a id="pgfId-1016620"></a>We define the parameters of this process in the following code. Tweaking these parameters will allow you to achieve new effects!</p>
  <pre class="programlisting"><a id="pgfId-1016740"></a>step = <span class="fm-codeblue">20.</span>           <span class="fm-combinumeral">❶</span>
<a id="pgfId-1037379"></a>num_octave = <span class="fm-codeblue">3</span>       <span class="fm-combinumeral">❷</span>
<a id="pgfId-1037386"></a>octave_scale = <span class="fm-codeblue">1.4</span>   <span class="fm-combinumeral">❸</span>
<a id="pgfId-1037393"></a>iterations = <span class="fm-codeblue">30</span>      <span class="fm-combinumeral">❹</span>
<a id="pgfId-1037400"></a>max_loss = <span class="fm-codeblue">15.</span>       <span class="fm-combinumeral">❺</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1058619"></a><span class="fm-combinumeral">❶</span> Gradient ascent step size</p>

  <p class="fm-code-annotation"><a id="pgfId-1058640"></a><span class="fm-combinumeral">❷</span> Number of scales at which to run gradient ascent</p>

  <p class="fm-code-annotation"><a id="pgfId-1058657"></a><span class="fm-combinumeral">❸</span> Size ratio between successive scales</p>

  <p class="fm-code-annotation"><a id="pgfId-1058677"></a><span class="fm-combinumeral">❹</span> Number of gradient ascent steps per scale</p>

  <p class="fm-code-annotation"><a id="pgfId-1058694"></a><span class="fm-combinumeral">❺</span> We’ll stop the gradient ascent process for a scale if the loss gets higher than this.</p>

  <p class="body"><a id="pgfId-1016833"></a>We’re also going to need a couple of utility functions to load and save images.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1016890"></a>Listing 12.14 Image processing utilities</p>
  <pre class="programlisting"><a id="pgfId-1017133"></a><b class="fm-codebrown">import</b> numpy <b class="fm-codebrown">as</b> np
<a id="pgfId-1037407"></a>  
<a id="pgfId-1037414"></a><b class="fm-codebrown">def</b> preprocess_image(image_path):                <span class="fm-combinumeral">❶</span>
<a id="pgfId-1037421"></a>    img = keras.utils.load_img(image_path)
<a id="pgfId-1037428"></a>    img = keras.utils.img_to_array(img)
<a id="pgfId-1037435"></a>    img = np.expand_dims(img, axis=<span class="fm-codeblue">0</span>)
<a id="pgfId-1037442"></a>    img = keras.applications.inception_v3.preprocess_input(img)
<a id="pgfId-1037449"></a>    <b class="fm-codebrown">return</b> img
<a id="pgfId-1037456"></a>  
<a id="pgfId-1037463"></a><b class="fm-codebrown">def</b> deprocess_image(img):                        <span class="fm-combinumeral">❷</span>
<a id="pgfId-1037470"></a>    img = img.reshape((img.shape[<span class="fm-codeblue">1</span>], img.shape[<span class="fm-codeblue">2</span>], <span class="fm-codeblue">3</span>))
<a id="pgfId-1037477"></a>    img /= <span class="fm-codeblue">2.0</span>                                   <span class="fm-combinumeral">❸</span>
<a id="pgfId-1037484"></a>    img += <span class="fm-codeblue">0.5</span>                                   <span class="fm-combinumeral">❸</span>
<a id="pgfId-1037491"></a>    img *= <span class="fm-codeblue">255.</span>                                  <span class="fm-combinumeral">❸</span>
<a id="pgfId-1037498"></a>    img = np.clip(img, <span class="fm-codeblue">0</span>, <span class="fm-codeblue">255</span>).astype(<span class="fm-codegreen">"uint8"</span>)   <span class="fm-combinumeral">❹</span>
<a id="pgfId-1037505"></a>    <b class="fm-codebrown">return</b> img</pre>

  <p class="fm-code-annotation"><a id="pgfId-1058309"></a><span class="fm-combinumeral">❶</span> Util function to open, resize, and format pictures into appropriate arrays</p>

  <p class="fm-code-annotation"><a id="pgfId-1058330"></a><span class="fm-combinumeral">❷</span> Util function to convert a NumPy array into a valid image</p>

  <p class="fm-code-annotation"><a id="pgfId-1058347"></a><span class="fm-combinumeral">❸</span> Undo inception v3 preprocessing.</p>

  <p class="fm-code-annotation"><a id="pgfId-1058364"></a><span class="fm-combinumeral">❹</span> Convert to uint8 and clip to the valid range [0, 255].</p>

  <p class="body"><a id="pgfId-1017266"></a>This is the outer loop. To avoid losing a lot of image detail after each successive scale-up (resulting in increasingly blurry or pixelated images), we can use a simple trick: after each scale-up, we’ll re-inject the lost details back into the image, which is possible because we know what the original image should look like at the larger scale. Given a small image size <i class="fm-italics">S</i> and a larger image size <i class="fm-italics">L</i>, we can compute the difference between the original image resized to size <i class="fm-italics">L</i> and the original resized to size <i class="fm-italics">S</i>—this difference quantifies the details lost when going from <i class="fm-italics">S</i> to <i class="fm-italics">L</i>.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1017326"></a>Listing 12.15 Running gradient ascent over multiple successive "octaves"</p>
  <pre class="programlisting"><a id="pgfId-1017641"></a>original_img = preprocess_image(base_image_path)                              <span class="fm-combinumeral">❶</span>
<a id="pgfId-1037512"></a>original_shape = original_img.shape[<span class="fm-codeblue">1</span>:<span class="fm-codeblue">3</span>]
<a id="pgfId-1037519"></a>  
<a id="pgfId-1037526"></a>successive_shapes = [original_shape]                                          <span class="fm-combinumeral">❷</span>
<a id="pgfId-1037533"></a><b class="fm-codebrown">for</b> i <b class="fm-codebrown">in</b> range(<span class="fm-codeblue">1</span>, num_octave):                                                <span class="fm-combinumeral">❷</span>
<a id="pgfId-1037540"></a>    shape = tuple([int(dim / (octave_scale ** i)) <b class="fm-codebrown">for</b> dim <b class="fm-codebrown">in</b> original_shape]) <span class="fm-combinumeral">❷</span>
<a id="pgfId-1037547"></a>    successive_shapes.append(shape)                                           <span class="fm-combinumeral">❷</span>
<a id="pgfId-1037554"></a>successive_shapes = successive_shapes[::-<span class="fm-codeblue">1</span>]                                   <span class="fm-combinumeral">❷</span>
<a id="pgfId-1037561"></a>  
<a id="pgfId-1037568"></a>shrunk_original_img = tf.image.resize(original_img, successive_shapes[<span class="fm-codeblue">0</span>])
<a id="pgfId-1037575"></a>  
<a id="pgfId-1037582"></a>img = tf.identity(original_img)                                               <span class="fm-combinumeral">❸</span>
<a id="pgfId-1037589"></a><b class="fm-codebrown">for</b> i, shape <b class="fm-codebrown">in</b> enumerate(successive_shapes):                                 <span class="fm-combinumeral">❹</span>
<a id="pgfId-1037596"></a>    <b class="fm-codebrown">print</b>(f<span class="fm-codegreen">"Processing octave {i} with shape {shape}"</span>)
<a id="pgfId-1037603"></a>    img = tf.image.resize(img, shape)                                         <span class="fm-combinumeral">❺</span>
<a id="pgfId-1037610"></a>    img = gradient_ascent_loop(                                               <span class="fm-combinumeral">❻</span>
<a id="pgfId-1037617"></a>        img, iterations=iterations, learning_rate=step, max_loss=max_loss
<a id="pgfId-1037624"></a>    )
<a id="pgfId-1037631"></a>    upscaled_shrunk_original_img = tf.image.resize(shrunk_original_img, shape)<span class="fm-combinumeral">❼</span>
<a id="pgfId-1037638"></a>    same_size_original = tf.image.resize(original_img, shape)                 <span class="fm-combinumeral">❽</span>
<a id="pgfId-1037645"></a>    lost_detail = same_size_original - upscaled_shrunk_original_img           <span class="fm-combinumeral">❾</span>
<a id="pgfId-1037652"></a>    img += lost_detail                                                        <span class="fm-combinumeral">❿</span>
<a id="pgfId-1037659"></a>    shrunk_original_img = tf.image.resize(original_img, shape)
<a id="pgfId-1037666"></a>  
<a id="pgfId-1037673"></a>keras.utils.save_img(<span class="fm-codegreen">"dream.png"</span>, deprocess_image(img.numpy()))               <span class="fm-combinumeral">⓫</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1057477"></a><span class="fm-combinumeral">❶</span> Load the test image.</p>

  <p class="fm-code-annotation"><a id="pgfId-1057501"></a><span class="fm-combinumeral">❷</span> Compute the target shape of the image at different octaves.</p>

  <p class="fm-code-annotation"><a id="pgfId-1057518"></a><span class="fm-combinumeral">❸</span> Make a copy of the image (we need to keep the original around).</p>

  <p class="fm-code-annotation"><a id="pgfId-1057535"></a><span class="fm-combinumeral">❹</span> Iterate over the different octaves.</p>

  <p class="fm-code-annotation"><a id="pgfId-1057552"></a><span class="fm-combinumeral">❺</span> Scale up the dream image.</p>

  <p class="fm-code-annotation"><a id="pgfId-1057569"></a><span class="fm-combinumeral">❻</span> Run gradient ascent, altering the dream.</p>

  <p class="fm-code-annotation"><a id="pgfId-1057586"></a><span class="fm-combinumeral">❼</span> Scale up the smaller version of the original image: it will be pixellated.</p>

  <p class="fm-code-annotation"><a id="pgfId-1057603"></a><span class="fm-combinumeral">❽</span> Compute the high-quality version of the original image at this size.</p>

  <p class="fm-code-annotation"><a id="pgfId-1057620"></a><span class="fm-combinumeral">❾</span> The difference between the two is the detail that was lost when scaling up.</p>

  <p class="fm-code-annotation"><a id="pgfId-1057637"></a><span class="fm-combinumeral">❿</span> Re-inject lost detail into the dream.</p>

  <p class="fm-code-annotation"><a id="pgfId-1057654"></a><span class="fm-combinumeral">⓫</span> Save the final result.</p>

  <p class="fm-callout"><a id="pgfId-1017827"></a><span class="fm-callout-head">Note</span> Because the original Inception V3 network was trained to recognize concepts in images of size 299 × 299, and given that the process involves scaling the images down by a reasonable factor, the DeepDream implementation produces much better results on images that are somewhere between 300 × 300 and 400 × 400. Regardless, you can run the same code on images of any size and any ratio.</p>

  <p class="body"><a id="pgfId-1017843"></a>On a GPU, it only takes a few seconds to run the whole thing. Figure 12.7 shows the result of our dream configuration on the test image.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/12-07.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1063167"></a>Figure 12.7 Running the DeepDream code on the test image</p>

  <p class="body"><a id="pgfId-1017859"></a>I strongly suggest that you explore what you can do by adjusting which layers you use in your loss. Layers that are lower in the network contain more-local, less-abstract representations and lead to dream patterns that look more geometric. Layers that are higher up lead to more-recognizable visual patterns based on the most common objects found in ImageNet, such as dog eyes, bird feathers, and so on. You can use random generation of the parameters in the <code class="fm-code-in-text">layer_settings</code> dictionary to quickly explore many different layer combinations. Figure 12.8 shows a range of results obtained on an image of a delicious homemade pastry using different layer configurations. <a id="marker-1017890"></a></p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/12-08.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1063287"></a>Figure 12.8 Trying a range of DeepDream configurations on an example image</p>

  <h3 class="fm-head1" id="heading_id_12"><a id="pgfId-1017907"></a>12.2.2 Wrapping up</h3>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1017931"></a>DeepDream consists of running a convnet in reverse to generate inputs based on the representations learned by the network.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1017945"></a>The results produced are fun and somewhat similar to the visual artifacts induced in humans by the disruption of the visual cortex via psychedelics.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1017955"></a>Note that the process isn’t specific to image models or even to convnets. It can be done for speech, music, and more. <a class="calibre11" id="marker-1017964"></a><a class="calibre11" id="marker-1017966"></a></p>
    </li>
  </ul>

  <h2 class="fm-head" id="heading_id_13"><a id="pgfId-1017972"></a>12.3 Neural style transfer</h2>

  <p class="body"><a id="pgfId-1017997"></a><a id="marker-1017985"></a><a id="marker-1017987"></a>In addition to DeepDream, another major development in deep-learning-driven image modification is <i class="fm-italics">neural style transfer</i>, introduced by Leon Gatys et al. in the summer of 2015.<a id="Id-1018003"></a><a href="#pgfId-1018003"><sup class="footnotenumber">4</sup></a> The neural style transfer algorithm has undergone many refinements and spawned many variations since its original introduction, and it has made its way into many smartphone photo apps. For simplicity, this section focuses on the formulation described in the original paper.</p>

  <p class="body"><a id="pgfId-1018020"></a>Neural style transfer consists of applying the style of a reference image to a target image while conserving the content of the target image. Figure 12.9 shows an example.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/12-09.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1063329"></a>Figure 12.9 A style transfer example</p>

  <p class="body"><a id="pgfId-1018066"></a>In this context, <i class="fm-italics">style</i> essentially means textures, colors, and visual patterns in the image, at various spatial scales, and the content is the higher-level macrostructure of the image. For instance, blue-and-yellow circular brushstrokes are considered to be the style in figure 12.9 (using <i class="fm-italics">Starry Night</i> by Vincent Van Gogh), and the buildings in the Tübingen photograph are considered to be the content.</p>

  <p class="body"><a id="pgfId-1018075"></a>The idea of style transfer, which is tightly related to that of texture generation, has had a long history in the image-processing community prior to the development of neural style transfer in 2015. But as it turns out, the deep-learning-based implementations of style transfer offer results unparalleled by what had been previously achieved with classical computer vision techniques, and they triggered an amazing renaissance in creative applications of computer vision.</p>

  <p class="body"><a id="pgfId-1018097"></a>The key notion behind implementing style transfer is the same idea that’s central to all deep learning algorithms: you define a loss function to specify what you want to achieve, and you minimize this loss. We know what we want to achieve: conserving the content of the original image while adopting the style of the reference image. If we were able to mathematically define <i class="fm-italics">content</i> and <i class="fm-italics">style</i>, then an appropriate loss function to minimize would be the following:</p>
  <pre class="programlisting"><a id="pgfId-1018106"></a>loss = (distance(style(reference_image) - style(combination_image)) +
<a id="pgfId-1037680"></a>        distance(content(original_image) - content(combination_image)))</pre>

  <p class="body"><a id="pgfId-1018200"></a>Here, <code class="fm-code-in-text">distance</code> is a norm <a id="marker-1018127"></a>function such as the L2 norm, <code class="fm-code-in-text">content</code> is a <a id="marker-1018143"></a>function that takes an image and computes a representation of its content, and <code class="fm-code-in-text">style</code> is a <a id="marker-1018159"></a>function that takes an image and computes a representation of its style. Minimizing this loss causes <code class="fm-code-in-text">style(combination_image)</code> to be close to <code class="fm-code-in-text">style(reference_image)</code>, and <code class="fm-code-in-text">content(combination_image)</code> is close to <code class="fm-code-in-text">content(original_image)</code>, thus achieving style transfer as we defined it.</p>

  <p class="body"><a id="pgfId-1018225"></a>A fundamental observation made by Gatys et al. was that deep convolutional neural networks offer a way to mathematically define the <code class="fm-code-in-text">style</code> and <code class="fm-code-in-text">content</code> functions. Let’s see how.</p>

  <h3 class="fm-head1" id="heading_id_14"><a id="pgfId-1018234"></a>12.3.1 The content loss</h3>

  <p class="body"><a id="pgfId-1018257"></a><a id="marker-1018245"></a><a id="marker-1018247"></a>As you already know, activations from earlier layers in a network contain <i class="fm-italics">local</i> information about the image, whereas activations from higher layers contain increasingly global, abstract information. Formulated in a different way, the activations of the different layers of a convnet provide a decomposition of the contents of an image over different spatial scales. Therefore, you’d expect the content of an image, which is more global and abstract, to be captured by the representations of the upper layers in a convnet.</p>

  <p class="body"><a id="pgfId-1018266"></a>A good candidate for content loss is thus the L2 norm between the activations of an upper layer in a pretrained convnet, computed over the target image, and the activations of the same layer computed over the generated image. This guarantees that, as seen from the upper layer, the generated image will look similar to the original target image. Assuming that what the upper layers of a convnet see is really the content of their input images, this works as a way to preserve image content. <a id="marker-1018268"></a><a id="marker-1018271"></a></p>

  <h3 class="fm-head1" id="heading_id_15"><a id="pgfId-1018277"></a>12.3.2 The style loss</h3>

  <p class="body"><a id="pgfId-1018300"></a><a id="marker-1018288"></a><a id="marker-1018290"></a>The content loss only uses a single upper layer, but the style loss as defined by Gatys et al. uses multiple layers of a convnet: you try to capture the appearance of the style-reference image at all spatial scales extracted by the convnet, not just a single scale. For the style loss, Gatys et al. use the <i class="fm-italics">Gram matrix</i> of a <a id="marker-1018305"></a>layer’s activations: the inner product of the feature maps of a given layer. This inner product can be understood as representing a map of the correlations between the layer’s features. These feature correlations capture the statistics of the patterns of a particular spatial scale, which empirically correspond to the appearance of the textures found at this scale.</p>

  <p class="body"><a id="pgfId-1018315"></a>Hence, the style loss aims to preserve similar internal correlations within the activations of different layers, across the style-reference image and the generated image. In turn, this guarantees that the textures found at different spatial scales look similar across the style-reference image and the generated image.</p>

  <p class="body"><a id="pgfId-1018321"></a>In short, you can use a pretrained convnet to define a loss that will do the following:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018327"></a>Preserve content by maintaining similar high-level layer activations between the original image and the generated image. The convnet should “see” both the original image and the generated image as containing the same things.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018367"></a>Preserve style by maintaining similar <i class="fm-italics1">correlations</i> within activations <a class="calibre11" id="marker-1018356"></a>for both low-level layers and high-level layers. Feature correlations capture <i class="fm-italics1">textures</i>: the generated <a class="calibre11" id="marker-1018372"></a>image and the style-reference image should share the same textures at different spatial scales.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1018382"></a>Now let’s look at a Keras implementation of the original 2015 neural style transfer algorithm. As you’ll see, it shares many similarities with the DeepDream implementation we developed in the previous section. <a id="marker-1061760"></a><a id="marker-1061761"></a></p>

  <h3 class="fm-head1" id="heading_id_16"><a id="pgfId-1018393"></a>12.3.3 Neural style transfer in Keras</h3>

  <p class="body"><a id="pgfId-1018412"></a><a id="marker-1061763"></a><a id="marker-1061764"></a><a id="marker-1061765"></a>Neural style transfer can be implemented using any pretrained convnet. Here, we’ll use the VGG19 network used by Gatys et al. VGG19 is a simple variant of the VGG16 network introduced in chapter 5, with three more convolutional layers.</p>

  <p class="body"><a id="pgfId-1018417"></a>Here’s the general process:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018423"></a>Set up a network that computes VGG19 layer activations for the style-reference image, the base image, and the generated image at the same time.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018437"></a>Use the layer activations computed over these three images to define the loss function described earlier, which we’ll minimize in order to achieve style transfer.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1018447"></a>Set up a gradient-descent process to minimize this loss function.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1018457"></a>Let’s start by defining the paths to the style-reference image and the base image. To make sure that the processed images are a similar size (widely different sizes make style transfer more difficult), we’ll later resize them all to a shared height of 400 px.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1018514"></a>Listing 12.16 Getting the style and content images</p>
  <pre class="programlisting"><a id="pgfId-1018648"></a><b class="fm-codebrown">from</b> tensorflow <b class="fm-codebrown">import</b> keras
<a id="pgfId-1037687"></a>  
<a id="pgfId-1037694"></a>base_image_path = keras.utils.get_file(                                 <span class="fm-combinumeral">❶</span>
<a id="pgfId-1037701"></a>    <span class="fm-codegreen">"sf.jpg"</span>, origin=<span class="fm-codegreen">"https:/ /img-datasets.s3.amazonaws.com/sf.jpg"</span>)
<a id="pgfId-1037708"></a>style_reference_image_path = keras.utils.get_file(                      <span class="fm-combinumeral">❷</span>
<a id="pgfId-1037715"></a>    <span class="fm-codegreen">"starry_night.jpg"</span>, 
<a id="pgfId-1051107"></a>    origin=<span class="fm-codegreen">"https:/ /img-datasets.s3.amazonaws.com/starry_night.jpg"</span>)
<a id="pgfId-1037722"></a>  
<a id="pgfId-1037729"></a>original_width, original_height = keras.utils.load_img(base_image_path).size
<a id="pgfId-1037736"></a>img_height = <span class="fm-codeblue">400</span>                                                       <span class="fm-combinumeral">❸</span>
<a id="pgfId-1037743"></a>img_width = round(original_width * img_height / original_height)       <span class="fm-combinumeral">❸</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1057250"></a><span class="fm-combinumeral">❶</span> Path to the image we want to transform</p>

  <p class="fm-code-annotation"><a id="pgfId-1057271"></a><span class="fm-combinumeral">❷</span> Path to the style image</p>

  <p class="fm-code-annotation"><a id="pgfId-1057288"></a><span class="fm-combinumeral">❸</span> Dimensions of the generated picture</p>

  <p class="body"><a id="pgfId-1018706"></a>Our content image is shown in figure 12.10, and figure 12.11 shows our style image.</p>

  <p class="body"><a id="pgfId-1018746"></a>We also need some auxiliary functions for loading, preprocessing, and postprocessing the images that go in and out of the VGG19 convnet.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/12-10.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1063387"></a>Figure 12.10 Content image: San Francisco from Nob Hill</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/12-11.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1063422"></a>Figure 12.11 Style image: Starry Night by Van Gogh</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1018817"></a>Listing 12.17 Auxiliary functions</p>
  <pre class="programlisting"><a id="pgfId-1019080"></a><b class="fm-codebrown">import</b> numpy <b class="fm-codebrown">as</b> np
<a id="pgfId-1037750"></a>  
<a id="pgfId-1037757"></a><b class="fm-codebrown">def</b> preprocess_image(image_path):                   <span class="fm-combinumeral">❶</span>
<a id="pgfId-1037764"></a>    img = keras.utils.load_img(
<a id="pgfId-1037771"></a>        image_path, target_size=(img_height, img_width))
<a id="pgfId-1037778"></a>    img = keras.utils.img_to_array(img)
<a id="pgfId-1037785"></a>    img = np.expand_dims(img, axis=<span class="fm-codeblue">0</span>)
<a id="pgfId-1037792"></a>    img = keras.applications.vgg19.preprocess_input(img)
<a id="pgfId-1037799"></a>    <b class="fm-codebrown">return</b> img
<a id="pgfId-1037806"></a>  
<a id="pgfId-1037813"></a><b class="fm-codebrown">def</b> deprocess_image(img):                           <span class="fm-combinumeral">❷</span>
<a id="pgfId-1037820"></a>    img = img.reshape((img_height, img_width, <span class="fm-codeblue">3</span>))
<a id="pgfId-1037827"></a>    img[:, :, <span class="fm-codeblue">0</span>] += <span class="fm-codeblue">103.939</span>                         <span class="fm-combinumeral">❸</span>
<a id="pgfId-1037834"></a>    img[:, :, <span class="fm-codeblue">1</span>] += <span class="fm-codeblue">116.779</span>                         <span class="fm-combinumeral">❸</span>
<a id="pgfId-1037841"></a>    img[:, :, <span class="fm-codeblue">2</span>] += <span class="fm-codeblue">123.68</span>                          <span class="fm-combinumeral">❸</span>
<a id="pgfId-1037848"></a>    img = img[:, :, ::-<span class="fm-codeblue">1</span>]                           <span class="fm-combinumeral">❹</span>
<a id="pgfId-1037855"></a>    img = np.clip(img, <span class="fm-codeblue">0</span>, <span class="fm-codeblue">255</span>).astype(<span class="fm-codegreen">"uint8"</span>)
<a id="pgfId-1037862"></a>    <b class="fm-codebrown">return</b> img</pre>

  <p class="fm-code-annotation"><a id="pgfId-1056902"></a><span class="fm-combinumeral">❶</span> Util function to open, resize, and format pictures into appropriate arrays</p>

  <p class="fm-code-annotation"><a id="pgfId-1056923"></a><span class="fm-combinumeral">❷</span> Util function to convert a NumPy array into a valid image</p>

  <p class="fm-code-annotation"><a id="pgfId-1056940"></a><span class="fm-combinumeral">❸</span> Zero-centering by removing the mean pixel value from ImageNet. This reverses a transformation done by vgg19.preprocess_input.</p>

  <p class="fm-code-annotation"><a id="pgfId-1056957"></a><span class="fm-combinumeral">❹</span> Converts images from 'BGR' to 'RGB'. This is also part of the reversal of vgg19.preprocess_input.</p>

  <p class="body"><a id="pgfId-1019157"></a>Let’s set up the VGG19 network. Like in the DeepDream example, we’ll use the pretrained convnet to create a feature exactor model that returns the activations of intermediate layers—all layers in the model this time.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1019214"></a>Listing 12.18 Using a pretrained VGG19 model to create a feature extractor</p>
  <pre class="programlisting"><a id="pgfId-1019299"></a>model = keras.applications.vgg19.VGG19(weights=<span class="fm-codegreen">"imagenet"</span>, include_top=<code class="fm-codegreen">False</code>)<span class="fm-combinumeral">❶</span>
<a id="pgfId-1037869"></a>  
<a id="pgfId-1037876"></a>outputs_dict = dict([(layer.name, layer.output) <b class="fm-codebrown">for</b> layer <b class="fm-codebrown">in</b> model.layers])
<a id="pgfId-1037883"></a>feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)   <span class="fm-combinumeral">❷</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1056791"></a><span class="fm-combinumeral">❶</span> Build a VGG19 model loaded with pretrained ImageNet weights.</p>

  <p class="fm-code-annotation"><a id="pgfId-1056812"></a><span class="fm-combinumeral">❷</span> Model that returns the activation values for every target layer (as a dict)</p>

  <p class="body"><a id="pgfId-1019344"></a>Let’s define the content loss, which will make sure the top layer of the VGG19 convnet has a similar view of the style image and the combination image.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1019401"></a>Listing 12.19 Content loss</p>
  <pre class="programlisting"><a id="pgfId-1019444"></a><b class="fm-codebrown">def</b> content_loss(base_img, combination_img):
<a id="pgfId-1037890"></a>    <b class="fm-codebrown">return</b> tf.reduce_sum(tf.square(combination_img - base_img))</pre>

  <p class="body"><a id="pgfId-1019453"></a>Next is the style loss. It uses an auxiliary function to compute the Gram matrix of an input matrix: a map of the correlations found in the original feature matrix.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1019510"></a>Listing 12.20 Style loss</p>
  <pre class="programlisting"><a id="pgfId-1019663"></a><b class="fm-codebrown">def</b> gram_matrix(x):
<a id="pgfId-1037897"></a>    x = tf.transpose(x, (<span class="fm-codeblue">2</span>, <span class="fm-codeblue">0</span>, <span class="fm-codeblue">1</span>))
<a id="pgfId-1037904"></a>    features = tf.reshape(x, (tf.shape(x)[<span class="fm-codeblue">0</span>], -<span class="fm-codeblue">1</span>))
<a id="pgfId-1037911"></a>    gram = tf.matmul(features, tf.transpose(features))
<a id="pgfId-1037918"></a>    <b class="fm-codebrown">return</b> gram
<a id="pgfId-1037925"></a>  
<a id="pgfId-1037932"></a><b class="fm-codebrown">def</b> style_loss(style_img, combination_img):
<a id="pgfId-1037939"></a>    S = gram_matrix(style_img)
<a id="pgfId-1037946"></a>    C = gram_matrix(combination_img)
<a id="pgfId-1037953"></a>    channels = <span class="fm-codeblue">3</span>
<a id="pgfId-1051117"></a>    size = img_height * img_width
<a id="pgfId-1037960"></a>    <b class="fm-codebrown">return</b> tf.reduce_sum(tf.square(S - C)) / (<span class="fm-codeblue">4.0</span> * (channels ** <span class="fm-codeblue">2</span>) * (size ** <span class="fm-codeblue">2</span>))</pre>

  <p class="body"><a id="pgfId-1019672"></a>To these two loss components, you add a third: the <i class="fm-italics">total variation loss</i>, which operates <a id="marker-1019683"></a>on the pixels of the generated combination image. It encourages spatial continuity in the generated image, thus avoiding overly pixelated results. You can interpret it as a regularization loss.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1019744"></a>Listing 12.21 Total variation loss</p>
  <pre class="programlisting"><a id="pgfId-1019877"></a><b class="fm-codebrown">def</b> total_variation_loss(x):
<a id="pgfId-1037967"></a>    a = tf.square(
<a id="pgfId-1037974"></a>        x[:, : img_height - <span class="fm-codeblue">1</span>, : img_width - <span class="fm-codeblue">1</span>, :] - x[:, <span class="fm-codeblue">1</span>:, : img_width - <span class="fm-codeblue">1</span>, :]
<a id="pgfId-1037981"></a>    )
<a id="pgfId-1037988"></a>    b = tf.square(
<a id="pgfId-1037995"></a>        x[:, : img_height - <span class="fm-codeblue">1</span>, : img_width - <span class="fm-codeblue">1</span>, :] - x[:, : img_height - <span class="fm-codeblue">1</span>, <span class="fm-codeblue">1</span>:, :]
<a id="pgfId-1038002"></a>    )
<a id="pgfId-1038009"></a>    <b class="fm-codebrown">return</b> tf.reduce_sum(tf.pow(a + b, <span class="fm-codeblue">1.25</span>))</pre>

  <p class="body"><a id="pgfId-1019886"></a>The loss that you minimize is a weighted average of these three losses. To compute the content loss, you use only one upper layer—the <code class="fm-code-in-text">block5_conv2</code> layer—whereas for the style loss, you use a list of layers that spans both low-level and high-level layers. You add the total variation loss at the end.</p>

  <p class="body"><a id="pgfId-1019917"></a>Depending on the style-reference image and content image you’re using, you’ll likely want to tune the <code class="fm-code-in-text">content_weight</code> coefficient (the contribution of the content loss to the total loss). A higher <code class="fm-code-in-text">content_weight</code> means the target content will be more recognizable in the generated image.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1019977"></a>Listing 12.22 Defining the final loss that you’ll minimize</p>
  <pre class="programlisting"><a id="pgfId-1020362"></a>style_layer_names = [                                                       <span class="fm-combinumeral">❶</span>
<a id="pgfId-1038016"></a>    <span class="fm-codegreen">"block1_conv1"</span>,
<a id="pgfId-1038023"></a>    <span class="fm-codegreen">"block2_conv1"</span>,
<a id="pgfId-1038030"></a>    <span class="fm-codegreen">"block3_conv1"</span>,
<a id="pgfId-1038037"></a>    <span class="fm-codegreen">"block4_conv1"</span>,
<a id="pgfId-1038044"></a>    <span class="fm-codegreen">"block5_conv1"</span>,
<a id="pgfId-1038051"></a>]
<a id="pgfId-1038058"></a>content_layer_name = <span class="fm-codegreen">"block5_conv2"</span>                                         <span class="fm-combinumeral">❷</span>
<a id="pgfId-1038065"></a>total_variation_weight = <span class="fm-codeblue">1e-6</span>                                               <span class="fm-combinumeral">❸</span>
<a id="pgfId-1038072"></a>style_weight = <span class="fm-codeblue">1e-6</span>                                                         <span class="fm-combinumeral">❹</span>
<a id="pgfId-1038079"></a>content_weight = <span class="fm-codeblue">2.5e-8</span>                                                     <span class="fm-combinumeral">❺</span>
<a id="pgfId-1038086"></a>  
<a id="pgfId-1038093"></a><b class="fm-codebrown">def</b> compute_loss(combination_image, base_image, style_reference_image):
<a id="pgfId-1038100"></a>    input_tensor = tf.concat(
<a id="pgfId-1038107"></a>        [base_image, style_reference_image, combination_image], axis=<span class="fm-codeblue">0</span>)
<a id="pgfId-1038114"></a>    features = feature_extractor(input_tensor)
<a id="pgfId-1038121"></a>    loss = tf.zeros(shape=())                                               <span class="fm-combinumeral">❻</span>
<a id="pgfId-1038128"></a>    layer_features = features[content_layer_name]                           <span class="fm-combinumeral">❼</span>
<a id="pgfId-1038135"></a>    base_image_features = layer_features[<span class="fm-codeblue">0</span>, :, :, :]                        <span class="fm-combinumeral">❼</span>
<a id="pgfId-1038142"></a>    combination_features = layer_features[<span class="fm-codeblue">2</span>, :, :, :]                       <span class="fm-combinumeral">❼</span>
<a id="pgfId-1038149"></a>    loss = loss + content_weight * content_loss(                            <span class="fm-combinumeral">❼</span>
<a id="pgfId-1038156"></a>        base_image_features, combination_features                           <span class="fm-combinumeral">❼</span>
<a id="pgfId-1038163"></a>    )
<a id="pgfId-1038170"></a>    <b class="fm-codebrown">for</b> layer_name <b class="fm-codebrown">in</b> style_layer_names:                                    <span class="fm-combinumeral">❽</span>
<a id="pgfId-1038177"></a>        layer_features = features[layer_name]                               <span class="fm-combinumeral">❽</span>
<a id="pgfId-1038184"></a>        style_reference_features = layer_features[<span class="fm-codeblue">1</span>, :, :, :]               <span class="fm-combinumeral">❽</span>
<a id="pgfId-1038191"></a>        combination_features = layer_features[<span class="fm-codeblue">2</span>, :, :, :]                   <span class="fm-combinumeral">❽</span>
<a id="pgfId-1038198"></a>        style_loss_value = style_loss(                                      <span class="fm-combinumeral">❽</span>
<a id="pgfId-1038205"></a>            style_reference_features, combination_features)                 <span class="fm-combinumeral">❽</span>
<a id="pgfId-1038212"></a>        loss += (style_weight / len(style_layer_names)) * style_loss_value  <span class="fm-combinumeral">❽</span>
<a id="pgfId-1038219"></a>  
<a id="pgfId-1038226"></a>    loss += total_variation_weight * total_variation_loss(combination_image)<span class="fm-combinumeral">❾</span>
<a id="pgfId-1038233"></a>    <b class="fm-codebrown">return</b> loss</pre>

  <p class="fm-code-annotation"><a id="pgfId-1055833"></a><span class="fm-combinumeral">❶</span> List of layers to use for the style loss</p>

  <p class="fm-code-annotation"><a id="pgfId-1055854"></a><span class="fm-combinumeral">❷</span> The layer to use for the content loss</p>

  <p class="fm-code-annotation"><a id="pgfId-1055871"></a><span class="fm-combinumeral">❸</span> Contribution weight of the total variation loss</p>

  <p class="fm-code-annotation"><a id="pgfId-1055888"></a><span class="fm-combinumeral">❹</span> Contribution weight of the style loss</p>

  <p class="fm-code-annotation"><a id="pgfId-1055905"></a><span class="fm-combinumeral">❺</span> Contribution weight of the content loss</p>

  <p class="fm-code-annotation"><a id="pgfId-1055925"></a><span class="fm-combinumeral">❻</span> Initialize the loss to 0.</p>

  <p class="fm-code-annotation"><a id="pgfId-1055942"></a><span class="fm-combinumeral">❼</span> Add the content loss.</p>

  <p class="fm-code-annotation"><a id="pgfId-1055959"></a><span class="fm-combinumeral">❽</span> Add the style loss.</p>

  <p class="fm-code-annotation"><a id="pgfId-1055976"></a><span class="fm-combinumeral">❾</span> Add the total variation loss.</p>

  <p class="body"><a id="pgfId-1020532"></a>Finally, let’s set up the gradient-descent process. In the original Gatys et al. paper, optimization is performed using the L-BFGS algorithm, but that’s not available in TensorFlow, so we’ll just do mini-batch gradient descent with <a id="marker-1020521"></a>the <code class="fm-code-in-text">SGD</code> optimizer instead. We’ll leverage an optimizer feature you haven’t seen before: a learning-rate schedule. We’ll use it to gradually decrease the learning rate from a very high value (100) to a much smaller final value (about 20). That way, we’ll make fast progress in the early stages of training and then proceed more cautiously as we get closer to the loss minimum.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1020592"></a>Listing 12.23 Setting up the gradient-descent process</p>
  <pre class="programlisting"><a id="pgfId-1020875"></a><b class="fm-codebrown">import</b> tensorflow <b class="fm-codebrown">as</b> tf
<a id="pgfId-1038240"></a>  
<a id="pgfId-1038247"></a><code class="fm-codeblue">@tf.function</code>                                                           <span class="fm-combinumeral">❶</span>
<a id="pgfId-1038254"></a><b class="fm-codebrown">def</b> compute_loss_and_grads(
<a id="pgfId-1051126"></a>    combination_image, base_image, style_reference_image):
<a id="pgfId-1038261"></a>    <b class="fm-codebrown">with</b> tf.GradientTape() <b class="fm-codebrown">as</b> tape:
<a id="pgfId-1038268"></a>        loss = compute_loss(
<a id="pgfId-1051137"></a>            combination_image, base_image, style_reference_image)
<a id="pgfId-1038275"></a>    grads = tape.gradient(loss, combination_image)
<a id="pgfId-1038282"></a>    <b class="fm-codebrown">return</b> loss, grads
<a id="pgfId-1038289"></a>  
<a id="pgfId-1038296"></a>optimizer = keras.optimizers.SGD(
<a id="pgfId-1038303"></a>    keras.optimizers.schedules.ExponentialDecay(                       <span class="fm-combinumeral">❷</span>
<a id="pgfId-1038310"></a>        initial_learning_rate=<span class="fm-codeblue">100.0</span>, decay_steps=<span class="fm-codeblue">100</span>, decay_rate=<span class="fm-codeblue">0.96</span> 
<a id="pgfId-1038317"></a>    )
<a id="pgfId-1038324"></a>)
<a id="pgfId-1038331"></a>  
<a id="pgfId-1038338"></a>base_image = preprocess_image(base_image_path)
<a id="pgfId-1038345"></a>style_reference_image = preprocess_image(style_reference_image_path)
<a id="pgfId-1038352"></a>combination_image = tf.Variable(preprocess_image(base_image_path))     <span class="fm-combinumeral">❸</span>
<a id="pgfId-1038359"></a>  
<a id="pgfId-1038366"></a>iterations = <span class="fm-codeblue">4000</span> 
<a id="pgfId-1038373"></a><b class="fm-codebrown">for</b> i <b class="fm-codebrown">in</b> range(<span class="fm-codeblue">1</span>, iterations + <span class="fm-codeblue">1</span>): 
<a id="pgfId-1038380"></a>    loss, grads = compute_loss_and_grads(
<a id="pgfId-1038387"></a>        combination_image, base_image, style_reference_image
<a id="pgfId-1038394"></a>    )
<a id="pgfId-1038401"></a>    optimizer.apply_gradients([(grads, combination_image)])            <span class="fm-combinumeral">❹</span>
<a id="pgfId-1038408"></a>    <b class="fm-codebrown">if</b> i % <span class="fm-codeblue">100</span> == <span class="fm-codeblue">0</span>:
<a id="pgfId-1038415"></a>        <b class="fm-codebrown">print</b>(f<span class="fm-codegreen">"Iteration {i}: loss={loss:.2f}"</span>)
<a id="pgfId-1038422"></a>        img = deprocess_image(combination_image.numpy())
<a id="pgfId-1038429"></a>        fname = f<span class="fm-codegreen">"combination_image_at_iteration_{i}.png"</span> 
<a id="pgfId-1038436"></a>        keras.utils.save_img(fname, img)                                <span class="fm-combinumeral">❺</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1055478"></a><span class="fm-combinumeral">❶</span> We make the training step fast by compiling it as a tf.function.</p>

  <p class="fm-code-annotation"><a id="pgfId-1055499"></a><span class="fm-combinumeral">❷</span> We’ll start with a learning rate of 100 and decrease it by 4% every 100 steps.</p>

  <p class="fm-code-annotation"><a id="pgfId-1055516"></a><span class="fm-combinumeral">❸</span> Use a Variable to store the combination image since we’ll be updating it during training.</p>

  <p class="fm-code-annotation"><a id="pgfId-1055533"></a><span class="fm-combinumeral">❹</span> Update the combination image in a direction that reduces the style transfer loss.</p>

  <p class="fm-code-annotation"><a id="pgfId-1055550"></a><span class="fm-combinumeral">❺</span> Save the combination image at regular intervals.</p>

  <p class="body"><a id="pgfId-1020965"></a>Figure 12.12 shows what you get. Keep in mind that what this technique achieves is merely a form of image retexturing, or texture transfer. It works best with style-reference images that are strongly textured and highly self-similar, and with content targets that don’t require high levels of detail in order to be recognizable. It typically can’t achieve fairly abstract feats such as transferring the style of one portrait to another. The algorithm is closer to classical signal processing than to AI, so don’t expect it to work like magic!</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/12-12.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1063466"></a>Figure 12.12 Style transfer result</p>

  <p class="body"><a id="pgfId-1020981"></a>Additionally, note that this style-transfer algorithm is slow to run. But the transformation operated by the setup is simple enough that it can be learned by a small, fast feedforward convnet as well—as long as you have appropriate training data available. Fast style transfer can thus be achieved by first spending a lot of compute cycles to generate input-output training examples for a fixed style-reference image, using the method outlined here, and then training a simple convnet to learn this style-specific transformation. Once that’s done, stylizing a given image is instantaneous: it’s just a forward pass of this small convnet. <a id="marker-1020997"></a><a id="marker-1021000"></a><a id="marker-1021002"></a></p>

  <h3 class="fm-head1" id="heading_id_17"><a id="pgfId-1021008"></a>12.3.4 Wrapping up</h3>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1021018"></a>Style transfer consists of creating a new image that preserves the contents of a target image while also capturing the style of a reference image.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1021032"></a>Content can be captured by the high-level activations of a convnet.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1021042"></a>Style can be captured by the internal correlations of the activations of different layers of a convnet.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1021052"></a>Hence, deep learning allows style transfer to be formulated as an optimization process using a loss defined with a pretrained convnet.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1021062"></a>Starting from this basic idea, many variants and refinements are possible. <a class="calibre11" id="marker-1021071"></a><a class="calibre11" id="marker-1021073"></a></p>
    </li>
  </ul>

  <h2 class="fm-head" id="heading_id_18"><a id="pgfId-1021079"></a>12.4 Generating images with variational autoencoders</h2>

  <p class="body"><a id="pgfId-1021098"></a><a id="marker-1021092"></a><a id="marker-1021094"></a>The most popular and successful application of creative AI today is image generation: learning latent visual spaces and sampling from them to create entirely new pictures interpolated from real ones—pictures of imaginary people, imaginary places, imaginary cats and dogs, and so on.</p>

  <p class="body"><a id="pgfId-1021119"></a>In this section and the next, we’ll review some high-level concepts pertaining to image generation, alongside implementation details relative to the two main techniques in this domain: <i class="fm-italics">variational autoencoders</i> (VAEs) and <i class="fm-italics">generative adversarial networks</i> (GANs). Note that <a id="marker-1021124"></a>the techniques I’ll present here aren’t specific to images—you could develop latent spaces of sound, music, or even text, using GANs and VAEs—but in practice, the most interesting results have been obtained with pictures, and that’s what we’ll focus on here.</p>

  <h3 class="fm-head1" id="heading_id_19"><a id="pgfId-1021134"></a>12.4.1 Sampling from latent spaces of images</h3>

  <p class="body"><a id="pgfId-1021177"></a><a id="marker-1021145"></a><a id="marker-1021147"></a>The key idea of image generation is to develop a low-dimensional <i class="fm-italics">latent space</i> of representations (which, like everything else in deep learning, is a vector space), where any point can be mapped to a “valid” image: an image that looks like the real thing. The module capable of realizing this mapping, taking as input a latent point and outputting an image (a grid of pixels), is called a <i class="fm-italics">generator</i> (in the case of GANs) or a <i class="fm-italics">decoder</i> (in the case of VAEs). Once such a latent space has been learned, you can sample points from it, and, by mapping them back to image space, generate images that have never been seen before (see figure 12.13). These new images are the in-betweens of the training images.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/12-13.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1063521"></a>Figure 12.13 Learning a latent vector space of images and using it to sample new images</p>

  <p class="body"><a id="pgfId-1021196"></a>GANs and VAEs are two different strategies for learning such latent spaces of image representations, each with its own characteristics. VAEs are great for learning latent spaces that are well structured, where specific directions encode a meaningful axis of variation in the data (see figure 12.14). GANs generate images that can potentially be highly realistic, but the latent space they come from may not have as much structure and continuity. <a id="marker-1021212"></a><a id="marker-1021215"></a></p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/12-14.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1063563"></a>Figure 12.14 A continuous space of faces generated by Tom White using VAEs</p>

  <h3 class="fm-head1" id="heading_id_20"><a id="pgfId-1021231"></a>12.4.2 Concept vectors for image editing</h3>

  <p class="body"><a id="pgfId-1021308"></a><a id="marker-1021256"></a><a id="marker-1021258"></a><a id="marker-1021260"></a><a id="marker-1021262"></a>We already hinted at the idea of a <i class="fm-italics">concept vector</i> when we covered word embeddings in chapter 11. The idea is still the same: given a latent space of representations, or an embedding space, certain directions in the space may encode interesting axes of variation in the original data. In a latent space of images of faces, for instance, there may be a <i class="fm-italics">smile vector</i>, such that <a id="marker-1021287"></a>if latent point <code class="fm-code-in-text">z</code> is the embedded representation of a certain face, then latent point <code class="fm-code-in-text">z</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">s</code> is the embedded representation of the same face, smiling. Once you’ve identified such a vector, it then becomes possible to edit images by projecting them into the latent space, moving their representation in a meaningful way, and then decoding them back to image space. There are concept vectors for essentially any independent dimension of variation in image space—in the case of faces, you may discover vectors for adding sunglasses to a face, removing glasses, turning a male face into a female face, and so on. Figure 12.15 is an example of a smile vector, a concept vector discovered by Tom White, from the Victoria University School of Design in New Zealand, using VAEs trained on a dataset of faces of celebrities (the CelebA dataset). <a id="marker-1021313"></a><a id="marker-1021316"></a><a id="marker-1021318"></a><a id="marker-1021320"></a></p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/12-15.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1063605"></a>Figure 12.15 The smile vector</p>

  <h3 class="fm-head1" id="heading_id_21"><a id="pgfId-1021336"></a>12.4.3 Variational autoencoders</h3>

  <p class="body"><a id="pgfId-1021367"></a><a id="marker-1021361"></a><a id="marker-1021363"></a>Variational autoencoders, simultaneously discovered by Kingma and Welling in December 2013<a id="Id-1021369"></a><a href="#pgfId-1021369"><sup class="footnotenumber">5</sup></a> and Rezende, Mohamed, and Wierstra in January 2014,<a id="Id-1021383"></a><a href="#pgfId-1021383"><sup class="footnotenumber">6</sup></a> are a kind of generative model that’s especially appropriate for the task of image editing via concept vectors. They’re a modern take on autoencoders (a type of network that aims to encode an input to a low-dimensional latent space and then decode it back) that mixes ideas from deep learning with Bayesian inference.</p>

  <p class="body"><a id="pgfId-1021400"></a>A classical image autoencoder takes an image, maps it to a latent vector space via an encoder module, and then decodes it back to an output with the same dimensions as the original image, via a decoder module (see figure 12.16). It’s then trained by using as target data the <i class="fm-italics">same images</i> as the input images, meaning the autoencoder learns to reconstruct the original inputs. By imposing various constraints on the code (the output of the encoder), you can get the autoencoder to learn more- or less-interesting latent representations of the data. Most commonly, you’ll constrain the code to be low-dimensional and sparse (mostly zeros), in which case the encoder acts as a way to compress the input data into fewer bits of information.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/12-16.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1063647"></a>Figure 12.16 An autoencoder mapping an input <i class="fm-italics">x</i> to a compressed representation and then decoding it back as <i class="fm-italics">x</i>'</p>

  <p class="body"><a id="pgfId-1021444"></a>In practice, such classical autoencoders don’t lead to particularly useful or nicely structured latent spaces. They’re not much good at compression, either. For these reasons, they have largely fallen out of fashion. VAEs, however, augment autoencoders with a little bit of statistical magic that forces them to learn continuous, highly structured latent spaces. They have turned out to be a powerful tool for image generation.</p>

  <p class="body"><a id="pgfId-1021464"></a>A VAE, instead of compressing its input image into a fixed code in the latent space, turns the image into the parameters of a statistical distribution: a mean and a variance. Essentially, this means we’re assuming the input image has been generated by a statistical process, and that the randomness of this process should be taken into account during encoding and decoding. The VAE then uses the mean and variance parameters to randomly sample one element of the distribution, and decodes that element back to the original input (see figure 12.17). The stochasticity of this process improves robustness and forces the latent space to encode meaningful representations everywhere: every point sampled in the latent space is decoded to a valid output.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/12-17.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1063689"></a>Figure 12.17 A VAE maps an image to two vectors, <code class="fm-code-in-text">z_mean</code> and <code class="fm-code-in-text">z_log_sigma</code>, which define a probability distribution over the latent space, used to sample a latent point to decode.</p>

  <p class="body"><a id="pgfId-1021499"></a>In technical terms, here’s how a VAE works:</p>

  <ol class="calibre14">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1021571"></a>An encoder module turns the input sample, <code class="fm-code-in-text">input_img</code>, into two parameters in a latent space of representations, <code class="fm-code-in-text">z_mean</code> and <code class="fm-code-in-text">z_log_variance</code>.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1021610"></a>You randomly sample a point <code class="fm-code-in-text">z</code> from the latent normal distribution that’s assumed to generate the input image, via <code class="fm-code-in-text">z</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">z_mean</code> <code class="fm-code-in-text">+</code> <code class="fm-code-in-text">exp(z_log_variance)</code> <code class="fm-code-in-text">*</code> <code class="fm-code-in-text">epsilon</code>, where <code class="fm-code-in-text">epsilon</code> is a random tensor of small values.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1021619"></a>A decoder module maps this point in the latent space back to the original input image.</p>
    </li>
  </ol>

  <p class="body"><a id="pgfId-1021665"></a>Because <code class="fm-code-in-text">epsilon</code> is random, the process ensures that every point that’s close to the latent location where you encoded <code class="fm-code-in-text">input_img</code> (<code class="fm-code-in-text">z-mean</code>) can be decoded to something similar to <code class="fm-code-in-text">input_img</code>, thus forcing the latent space to be continuously meaningful. Any two close points in the latent space will decode to highly similar images. Continuity, combined with the low dimensionality of the latent space, forces every direction in the latent space to encode a meaningful axis of variation of the data, making the latent space very structured and thus highly suitable to manipulation via concept vectors.</p>

  <p class="body"><a id="pgfId-1021696"></a>The parameters of a VAE are trained via two loss functions: a <i class="fm-italics">reconstruction loss</i> that forces <a id="marker-1021685"></a>the decoded samples to match the initial inputs, and a <i class="fm-italics">regularization loss</i> that helps <a id="marker-1021701"></a>learn well-rounded latent distributions and reduces overfitting to the training data. Schematically, the process looks like this:</p>
  <pre class="programlisting"><a id="pgfId-1021751"></a>z_mean, z_log_variance = encoder(input_img)   <span class="fm-combinumeral">❶</span>
<a id="pgfId-1038443"></a>z = z_mean + exp(z_log_variance) * epsilon    <span class="fm-combinumeral">❷</span>
<a id="pgfId-1038450"></a>reconstructed_img = decoder(z)                <span class="fm-combinumeral">❸</span>
<a id="pgfId-1038457"></a>model = Model(input_img, reconstructed_img)   <span class="fm-combinumeral">❹</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1055278"></a><span class="fm-combinumeral">❶</span> Encodes the input into mean and variance parameters</p>

  <p class="fm-code-annotation"><a id="pgfId-1055299"></a><span class="fm-combinumeral">❷</span> Draws a latent point using a small random epsilon</p>

  <p class="fm-code-annotation"><a id="pgfId-1055319"></a><span class="fm-combinumeral">❸</span> Decodes z back to an image</p>

  <p class="fm-code-annotation"><a id="pgfId-1055336"></a><span class="fm-combinumeral">❹</span> Instantiates the autoencoder model, which maps an input image to its reconstruction</p>

  <p class="body"><a id="pgfId-1021828"></a>You can then train the model using the reconstruction loss and the regularization loss. For the regularization loss, we typically use an expression (the Kullback–Leibler divergence) meant to nudge the distribution of the encoder output toward a well-rounded normal distribution centered around 0. This provides the encoder with a sensible assumption about the structure of the latent space it’s modeling.</p>

  <p class="body"><a id="pgfId-1021834"></a>Now let’s see what implementing a VAE looks like in practice!<a id="marker-1021836"></a><a id="marker-1021839"></a></p>

  <h3 class="fm-head1" id="heading_id_22"><a id="pgfId-1021845"></a>12.4.4 Implementing a VAE with Keras</h3>

  <p class="body"><a id="pgfId-1021864"></a><a id="marker-1021856"></a><a id="marker-1021858"></a><a id="marker-1021860"></a>We’re going to be implementing a VAE that can generate MNIST digits. It’s going to have three parts:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1021869"></a>An encoder network that turns a real image into a mean and a variance in the latent space</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1021883"></a>A sampling layer that takes such a mean and variance, and uses them to sample a random point from the latent space</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1021893"></a>A decoder network that turns points from the latent space back into images</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1021955"></a>The following listing shows the encoder network we’ll use, mapping images to the parameters of a probability distribution over the latent space. It’s a simple convnet that maps the input image <code class="fm-code-in-text">x</code> to two vectors, <code class="fm-code-in-text">z_mean</code> and <code class="fm-code-in-text">z_log_var</code>. One important detail is that we use strides for downsampling feature maps instead of max pooling. The last time we did this was in the image segmentation example in chapter 9. Recall that, in general, strides are preferable to max pooling for any model that cares about <i class="fm-italics">information location</i>—that is <a id="marker-1021944"></a>to say, <i class="fm-italics">where</i> stuff is in the image—and this one does, since it will have to produce an image encoding that can be used to reconstruct a valid image.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1022015"></a>Listing 12.24 VAE encoder network</p>
  <pre class="programlisting"><a id="pgfId-1022298"></a><b class="fm-codebrown">from</b> tensorflow <b class="fm-codebrown">import</b> keras 
<a id="pgfId-1038464"></a><b class="fm-codebrown">from</b> tensorflow.keras <b class="fm-codebrown">import</b> layers
<a id="pgfId-1038471"></a>  
<a id="pgfId-1038478"></a>latent_dim = <span class="fm-codeblue">2</span>                                              <span class="fm-combinumeral">❶</span>
<a id="pgfId-1038485"></a>  
<a id="pgfId-1038492"></a>encoder_inputs = keras.Input(shape=(<span class="fm-codeblue">28</span>, <span class="fm-codeblue">28</span>, <span class="fm-codeblue">1</span>))
<a id="pgfId-1038499"></a>x = layers.Conv2D(
<a id="pgfId-1050557"></a><span class="fm-codeblue">    32</span>, <span class="fm-codeblue">3</span>, activation=<span class="fm-codegreen">"relu"</span>, strides=<span class="fm-codeblue">2</span>, padding=<span class="fm-codegreen">"same"</span>)(encoder_inputs)
<a id="pgfId-1038506"></a>x = layers.Conv2D(<span class="fm-codeblue">64</span>, <span class="fm-codeblue">3</span>, activation=<span class="fm-codegreen">"relu"</span>, strides=<span class="fm-codeblue">2</span>, padding=<span class="fm-codegreen">"same"</span>)(x)
<a id="pgfId-1038513"></a>x = layers.Flatten()(x)
<a id="pgfId-1038520"></a>x = layers.Dense(<span class="fm-codeblue">16</span>, activation=<span class="fm-codegreen">"relu"</span>)(x)
<a id="pgfId-1038527"></a>z_mean = layers.Dense(latent_dim, name=<span class="fm-codegreen">"z_mean"</span>)(x)         <span class="fm-combinumeral">❷</span>
<a id="pgfId-1038534"></a>z_log_var = layers.Dense(latent_dim, name=<span class="fm-codegreen">"z_log_var"</span>)(x)   <span class="fm-combinumeral">❷</span>
<a id="pgfId-1038541"></a>encoder = keras.Model(encoder_inputs, [z_mean, z_log_var], name=<span class="fm-codegreen">"encoder"</span>)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1055118"></a><span class="fm-combinumeral">❶</span> Dimensionality of the latent space: a 2D plane</p>

  <p class="fm-code-annotation"><a id="pgfId-1055135"></a><span class="fm-combinumeral">❷</span> The input image ends up being encoded into these two parameters.</p>

  <p class="body"><a id="pgfId-1022343"></a>Its summary looks like this:</p>
  <pre class="programlisting"><a id="pgfId-1022769"></a>&gt;&gt;&gt; encoder.summary()
<a id="pgfId-1038548"></a>Model: "encoder" 
<a id="pgfId-1038555"></a>__________________________________________________________________________________________________
<a id="pgfId-1038562"></a>Layer (type)                    Output Shape         Param #     Connected to
<a id="pgfId-1038569"></a>==================================================================================================
<a id="pgfId-1038576"></a>input_1 (InputLayer)            [(None, 28, 28, 1)]  0 
<a id="pgfId-1038583"></a>__________________________________________________________________________________________________
<a id="pgfId-1038590"></a>conv2d (Conv2D)                 (None, 14, 14, 32)   320         input_1[0][0]
<a id="pgfId-1038597"></a>__________________________________________________________________________________________________
<a id="pgfId-1038604"></a>conv2d_1 (Conv2D)               (None, 7, 7, 64)     18496       conv2d[0][0]
<a id="pgfId-1038611"></a>__________________________________________________________________________________________________
<a id="pgfId-1038618"></a>flatten (Flatten)               (None, 3136)         0           conv2d_1[0][0]
<a id="pgfId-1038625"></a>__________________________________________________________________________________________________
<a id="pgfId-1038632"></a>dense (Dense)                   (None, 16)           50192       flatten[0][0]
<a id="pgfId-1038639"></a>__________________________________________________________________________________________________
<a id="pgfId-1038646"></a>z_mean (Dense)                  (None, 2)            34          dense[0][0]
<a id="pgfId-1038653"></a>__________________________________________________________________________________________________
<a id="pgfId-1038660"></a>z_log_var (Dense)               (None, 2)            34          dense[0][0]
<a id="pgfId-1038667"></a>==================================================================================================
<a id="pgfId-1038674"></a>Total params: 69,076 
<a id="pgfId-1038681"></a>Trainable params: 69,076 
<a id="pgfId-1038688"></a>Non-trainable params: 0 
<a id="pgfId-1038695"></a>__________________________________________________________________________________________________</pre>

  <p class="body"><a id="pgfId-1022820"></a>Next is the code for using <code class="fm-code-in-text">z_mean</code> and <code class="fm-code-in-text">z_log_var</code>, the parameters of the statistical distribution assumed to have produced <code class="fm-code-in-text">input_img</code>, to generate a latent space point <code class="fm-code-in-text">z</code>.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1022880"></a>Listing 12.25 Latent-space-sampling layer</p>
  <pre class="programlisting"><a id="pgfId-1023003"></a><b class="fm-codebrown">import</b> tensorflow <b class="fm-codebrown">as</b> tf
<a id="pgfId-1038702"></a>  
<a id="pgfId-1038709"></a><b class="fm-codebrown">class</b> Sampler(layers.Layer):
<a id="pgfId-1038716"></a>    <b class="fm-codebrown">def</b> call(self, z_mean, z_log_var):
<a id="pgfId-1038723"></a>        batch_size = tf.shape(z_mean)[<span class="fm-codeblue">0</span>]
<a id="pgfId-1038730"></a>        z_size = tf.shape(z_mean)[<span class="fm-codeblue">1</span>]
<a id="pgfId-1038737"></a>        epsilon = tf.random.normal(shape=(batch_size, z_size))  <span class="fm-combinumeral">❶</span>
<a id="pgfId-1038744"></a>        <b class="fm-codebrown">return</b> z_mean + tf.exp(<span class="fm-codeblue">0.5</span> * z_log_var) * epsilon       <span class="fm-combinumeral">❷</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1054989"></a><span class="fm-combinumeral">❶</span> Draw a batch of random normal vectors.</p>

  <p class="fm-code-annotation"><a id="pgfId-1055010"></a><span class="fm-combinumeral">❷</span> Apply the VAE sampling formula.</p>

  <p class="body"><a id="pgfId-1023061"></a>The following listing shows the decoder implementation. We reshape the vector <code class="fm-code-in-text">z</code> to the dimensions of an image and then use a few convolution layers to obtain a final image output that has the same dimensions as the original <code class="fm-code-in-text">input_img</code>.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1023121"></a>Listing 12.26 VAE decoder network, mapping latent space points to images</p>
  <pre class="programlisting"><a id="pgfId-1023426"></a>latent_inputs = keras.Input(shape=(latent_dim,))                                  <span class="fm-combinumeral">❶</span>
<a id="pgfId-1038751"></a>x = layers.Dense(<span class="fm-codeblue">7</span> * <span class="fm-codeblue">7</span> * <span class="fm-codeblue">64</span>, activation=<span class="fm-codegreen">"relu"</span>)(latent_inputs)                    <span class="fm-combinumeral">❷</span>
<a id="pgfId-1038758"></a>x = layers.Reshape((<span class="fm-codeblue">7</span>, <span class="fm-codeblue">7</span>, <span class="fm-codeblue">64</span>))(x)                                                 <span class="fm-combinumeral">❸</span>
<a id="pgfId-1038765"></a>x = layers.Conv2DTranspose(<span class="fm-codeblue">64</span>, <span class="fm-codeblue">3</span>, activation=<span class="fm-codegreen">"relu"</span>, strides=<span class="fm-codeblue">2</span>, padding=<span class="fm-codegreen">"same"</span>)(x)<span class="fm-combinumeral">❹</span>
<a id="pgfId-1038772"></a>x = layers.Conv2DTranspose(<span class="fm-codeblue">32</span>, <span class="fm-codeblue">3</span>, activation=<span class="fm-codegreen">"relu"</span>, strides=<span class="fm-codeblue">2</span>, padding=<span class="fm-codegreen">"same"</span>)(x)<span class="fm-combinumeral">❹</span>
<a id="pgfId-1038779"></a>decoder_outputs = layers.Conv2D(<span class="fm-codeblue">1</span>, <span class="fm-codeblue">3</span>, activation=<span class="fm-codegreen">"sigmoid"</span>, padding=<span class="fm-codegreen">"same"</span>)(x)    <span class="fm-combinumeral">❺</span>
<a id="pgfId-1038786"></a>decoder = keras.Model(latent_inputs, decoder_outputs, name=<span class="fm-codegreen">"decoder"</span>)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1054622"></a><span class="fm-combinumeral">❶</span> Input where we’ll feed z</p>

  <p class="fm-code-annotation"><a id="pgfId-1054639"></a><span class="fm-combinumeral">❷</span> Produce the same number of coefficients that we had at the level of the Flatten layer in the encoder.</p>

  <p class="fm-code-annotation"><a id="pgfId-1054656"></a><span class="fm-combinumeral">❸</span> Revert the Flatten layer of the encoder.</p>

  <p class="fm-code-annotation"><a id="pgfId-1054673"></a><span class="fm-combinumeral">❹</span> Revert the Conv2D layers of the encoder.</p>

  <p class="fm-code-annotation"><a id="pgfId-1054690"></a><span class="fm-combinumeral">❺</span> The output ends up with shape (28, 28, 1).</p>

  <p class="body"><a id="pgfId-1023519"></a>Its summary looks like this:</p>
  <pre class="programlisting"><a id="pgfId-1023835"></a>&gt;&gt;&gt; decoder.summary()
<a id="pgfId-1038793"></a>Model: "decoder" 
<a id="pgfId-1038800"></a>_________________________________________________________________
<a id="pgfId-1038807"></a>Layer (type)                 Output Shape              Param # 
<a id="pgfId-1038814"></a>=================================================================
<a id="pgfId-1038821"></a>input_2 (InputLayer)         [(None, 2)]               0 
<a id="pgfId-1038828"></a>_________________________________________________________________
<a id="pgfId-1038835"></a>dense_1 (Dense)              (None, 3136)              9408 
<a id="pgfId-1038842"></a>_________________________________________________________________
<a id="pgfId-1038849"></a>reshape (Reshape)            (None, 7, 7, 64)          0 
<a id="pgfId-1038856"></a>_________________________________________________________________
<a id="pgfId-1038863"></a>conv2d_transpose (Conv2DTran (None, 14, 14, 64)        36928 
<a id="pgfId-1038870"></a>_________________________________________________________________
<a id="pgfId-1038877"></a>conv2d_transpose_1 (Conv2DTr (None, 28, 28, 32)        18464 
<a id="pgfId-1038884"></a>_________________________________________________________________
<a id="pgfId-1038891"></a>conv2d_2 (Conv2D)            (None, 28, 28, 1)         289 
<a id="pgfId-1038898"></a>=================================================================
<a id="pgfId-1038905"></a>Total params: 65,089 
<a id="pgfId-1038912"></a>Trainable params: 65,089 
<a id="pgfId-1038919"></a>Non-trainable params: 0 
<a id="pgfId-1038926"></a>_________________________________________________________________</pre>

  <p class="body"><a id="pgfId-1023882"></a>Now let’s create the VAE model itself. This is your first example of a model that isn’t doing supervised learning (an autoencoder is an example of <i class="fm-italics">self-supervised</i> learning, because it <a id="marker-1023855"></a>uses its inputs as targets). Whenever you depart from classic supervised learning, it’s common to subclass the <code class="fm-code-in-text">Model</code> class and <a id="marker-1023871"></a>implement a custom <code class="fm-code-in-text">train_ step()</code> to specify <a id="marker-1023887"></a>the new training logic, a workflow you learned about in chapter 7. That’s what we’ll do here.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1023948"></a>Listing 12.27 VAE model with custom <code class="fm-code-in-text">train_step()</code></p>
  <pre class="programlisting"><a id="pgfId-1024249"></a><b class="fm-codebrown">class</b> VAE(keras.Model):
<a id="pgfId-1038933"></a>    <b class="fm-codebrown">def</b> __init__(self, encoder, decoder, **kwargs):
<a id="pgfId-1038940"></a>        super().__init__(**kwargs)
<a id="pgfId-1038947"></a>        self.encoder = encoder
<a id="pgfId-1038954"></a>        self.decoder = decoder
<a id="pgfId-1038961"></a>        self.sampler = Sampler()
<a id="pgfId-1038968"></a>        self.total_loss_tracker = keras.metrics.Mean(name=<span class="fm-codegreen">"total_loss"</span>) <span class="fm-combinumeral">❶</span>
<a id="pgfId-1038975"></a>        self.reconstruction_loss_tracker = keras.metrics.Mean(          <span class="fm-combinumeral">❶</span>
<a id="pgfId-1038982"></a>            name=<span class="fm-codegreen">"reconstruction_loss"</span>)
<a id="pgfId-1038989"></a>        self.kl_loss_tracker = keras.metrics.Mean(name=<span class="fm-codegreen">"kl_loss"</span>)       <span class="fm-combinumeral">❶</span>
<a id="pgfId-1038996"></a>  
<a id="pgfId-1039003"></a>    <code class="fm-codeblue">@property</code>
<a id="pgfId-1050917"></a>    <b class="fm-codebrown">def</b> metrics(self):                                                  <span class="fm-combinumeral">❷</span>
<a id="pgfId-1039010"></a>        <b class="fm-codebrown">return</b> [self.total_loss_tracker,
<a id="pgfId-1039017"></a>                self.reconstruction_loss_tracker,
<a id="pgfId-1039024"></a>                self.kl_loss_tracker]
<a id="pgfId-1039031"></a>  
<a id="pgfId-1039038"></a>    <b class="fm-codebrown">def</b> train_step(self, data):
<a id="pgfId-1039045"></a>        <b class="fm-codebrown">with</b> tf.GradientTape() <b class="fm-codebrown">as</b> tape:
<a id="pgfId-1039052"></a>            z_mean, z_log_var = self.encoder(data)
<a id="pgfId-1039059"></a>            z = self.sampler(z_mean, z_log_var)
<a id="pgfId-1039066"></a>            reconstruction = decoder(z)
<a id="pgfId-1039073"></a>            reconstruction_loss = tf.reduce_mean(                       <span class="fm-combinumeral">❸</span>
<a id="pgfId-1039080"></a>                tf.reduce_sum(                      
<a id="pgfId-1039087"></a>                    keras.losses.binary_crossentropy(data, reconstruction),
<a id="pgfId-1039094"></a>                    axis=(<span class="fm-codeblue">1</span>, <span class="fm-codeblue">2</span>)
<a id="pgfId-1039101"></a>                )
<a id="pgfId-1039108"></a>            )
<a id="pgfId-1039115"></a>            kl_loss = -<span class="fm-codeblue">0.5</span> * (<span class="fm-codeblue">1</span> + z_log_var - tf.square(z_mean) -       <span class="fm-combinumeral">❹</span> tf.exp(z_log_var))                                       <span class="fm-combinumeral">❹</span>
<a id="pgfId-1039122"></a>            total_loss = reconstruction_loss + tf.reduce_mean(kl_loss)  <span class="fm-combinumeral">❹</span>
<a id="pgfId-1039129"></a>        grads = tape.gradient(total_loss, self.trainable_weights)
<a id="pgfId-1039136"></a>        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))
<a id="pgfId-1039143"></a>        self.total_loss_tracker.update_state(total_loss)
<a id="pgfId-1039150"></a>        self.reconstruction_loss_tracker.update_state(reconstruction_loss)
<a id="pgfId-1039157"></a>        self.kl_loss_tracker.update_state(kl_loss)
<a id="pgfId-1039164"></a>        <b class="fm-codebrown">return</b> {
<a id="pgfId-1039171"></a>            <span class="fm-codegreen">"total_loss"</span>: self.total_loss_tracker.result(),
<a id="pgfId-1039178"></a>            <span class="fm-codegreen">"reconstruction_loss"</span>: self.reconstruction_loss_tracker.result(),
<a id="pgfId-1039185"></a>            <span class="fm-codegreen">"kl_loss"</span>: self.kl_loss_tracker.result(),
<a id="pgfId-1039192"></a>        }</pre>

  <p class="fm-code-annotation"><a id="pgfId-1054260"></a><span class="fm-combinumeral">❶</span> We use these metrics to keep track of the loss averages over each epoch.</p>

  <p class="fm-code-annotation"><a id="pgfId-1054277"></a><span class="fm-combinumeral">❷</span> We list the metrics in the metrics property to enable the model to reset them after each epoch (or between multiple calls to fit()/evaluate()).</p>

  <p class="fm-code-annotation"><a id="pgfId-1054294"></a><span class="fm-combinumeral">❸</span> We sum the reconstruction loss over the spatial dimensions (axes 1 and 2) and take its mean over the batch dimension.</p>

  <p class="fm-code-annotation"><a id="pgfId-1054311"></a><span class="fm-combinumeral">❹</span> Add the regularization term (Kullback–Leibler divergence).</p>

  <p class="body"><a id="pgfId-1024352"></a>Finally, we’re ready to instantiate and train the model on MNIST digits. Because the loss is taken care of in the custom layer, we don’t specify an external loss at compile time (<code class="fm-code-in-text">loss=None</code>), which in turn means we won’t pass target data during training (as you can see, we only pass <code class="fm-code-in-text">x_train</code> to the model in <code class="fm-code-in-text">fit()</code>).</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1024412"></a>Listing 12.28 Training the VAE</p>
  <pre class="programlisting"><a id="pgfId-1024545"></a><b class="fm-codebrown">import</b> numpy <b class="fm-codebrown">as</b> np
<a id="pgfId-1039199"></a>  
<a id="pgfId-1039206"></a>(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()
<a id="pgfId-1039213"></a>mnist_digits = np.concatenate([x_train, x_test], axis=<span class="fm-codeblue">0</span>)               <span class="fm-combinumeral">❶</span>
<a id="pgfId-1039220"></a>mnist_digits = np.expand_dims(mnist_digits, -<span class="fm-codeblue">1</span>).astype(<span class="fm-codegreen">"float32"</span>) / <span class="fm-codeblue">255</span> 
<a id="pgfId-1039227"></a>  
<a id="pgfId-1039234"></a>vae = VAE(encoder, decoder)
<a id="pgfId-1039241"></a>vae.compile(optimizer=keras.optimizers.Adam(), run_eagerly=<code class="fm-codegreen">True</code>)       <span class="fm-combinumeral">❷</span>
<a id="pgfId-1039248"></a>vae.fit(mnist_digits, epochs=<span class="fm-codeblue">30</span>, batch_size=<span class="fm-codeblue">128</span>)                       <span class="fm-combinumeral">❸</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1054074"></a><span class="fm-combinumeral">❶</span> We train on all MNIST digits, so we concatenate the training and test samples.</p>

  <p class="fm-code-annotation"><a id="pgfId-1054095"></a><span class="fm-combinumeral">❷</span> Note that we don’t pass a loss argument in compile(), since the loss is already part of the train_step().</p>

  <p class="fm-code-annotation"><a id="pgfId-1054112"></a><span class="fm-combinumeral">❸</span> Note that we don’t pass targets in fit(), since train_step() doesn’t expect any.</p>

  <p class="body"><a id="pgfId-1024606"></a>Once the model is trained, we can use the <code class="fm-code-in-text">decoder</code> network to <a id="marker-1024617"></a>turn arbitrary latent space vectors into images.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1024678"></a>Listing 12.29 Sampling a grid of images from the 2D latent space</p>
  <pre class="programlisting"><a id="pgfId-1025031"></a><b class="fm-codebrown">import</b> matplotlib.pyplot <b class="fm-codebrown">as</b> plt
<a id="pgfId-1039255"></a>  
<a id="pgfId-1039262"></a>n = <span class="fm-codeblue">30</span>                                                        <span class="fm-combinumeral">❶</span>
<a id="pgfId-1039269"></a>digit_size = <span class="fm-codeblue">28</span> 
<a id="pgfId-1039276"></a>figure = np.zeros((digit_size * n, digit_size * n))
<a id="pgfId-1039283"></a>  
<a id="pgfId-1039290"></a>grid_x = np.linspace(-<span class="fm-codeblue">1</span>, <span class="fm-codeblue">1</span>, n)                                <span class="fm-combinumeral">❷</span>
<a id="pgfId-1039297"></a>grid_y = np.linspace(-<span class="fm-codeblue">1</span>, <span class="fm-codeblue">1</span>, n)[::-<span class="fm-codeblue">1</span>]                          <span class="fm-combinumeral">❷</span>
<a id="pgfId-1039304"></a>  
<a id="pgfId-1039311"></a><b class="fm-codebrown">for</b> i, yi <b class="fm-codebrown">in</b> enumerate(grid_y):                               <span class="fm-combinumeral">❸</span>
<a id="pgfId-1039318"></a>    <b class="fm-codebrown">for</b> j, xi <b class="fm-codebrown">in</b> enumerate(grid_x):                           <span class="fm-combinumeral">❸</span>
<a id="pgfId-1039325"></a>        z_sample = np.array([[xi, yi]])                       <span class="fm-combinumeral">❹</span>
<a id="pgfId-1039332"></a>        x_decoded = vae.decoder.predict(z_sample)             <span class="fm-combinumeral">❹</span>
<a id="pgfId-1039339"></a>        digit = x_decoded[<span class="fm-codeblue">0</span>].reshape(digit_size, digit_size)  <span class="fm-combinumeral">❹</span>
<a id="pgfId-1039346"></a>        figure[
<a id="pgfId-1039353"></a>            i * digit_size : (i + <span class="fm-codeblue">1</span>) * digit_size,
<a id="pgfId-1039360"></a>            j * digit_size : (j + <span class="fm-codeblue">1</span>) * digit_size,
<a id="pgfId-1039367"></a>        ] = digit
<a id="pgfId-1039374"></a>  
<a id="pgfId-1039381"></a>plt.figure(figsize=(<span class="fm-codeblue">15</span>, <span class="fm-codeblue">15</span>))
<a id="pgfId-1039388"></a>start_range = digit_size // <span class="fm-codeblue">2</span> 
<a id="pgfId-1039395"></a>end_range = n * digit_size + start_range
<a id="pgfId-1039402"></a>pixel_range = np.arange(start_range, end_range, digit_size)
<a id="pgfId-1039409"></a>sample_range_x = np.round(grid_x, <span class="fm-codeblue">1</span>)
<a id="pgfId-1039416"></a>sample_range_y = np.round(grid_y, <span class="fm-codeblue">1</span>)
<a id="pgfId-1039423"></a>plt.xticks(pixel_range, sample_range_x)
<a id="pgfId-1039430"></a>plt.yticks(pixel_range, sample_range_y)
<a id="pgfId-1039437"></a>plt.xlabel(<span class="fm-codegreen">"z[0]"</span>)
<a id="pgfId-1039444"></a>plt.ylabel(<span class="fm-codegreen">"z[1]"</span>)
<a id="pgfId-1039451"></a>plt.axis(<span class="fm-codegreen">"off"</span>)
<a id="pgfId-1039458"></a>plt.imshow(figure, cmap=<span class="fm-codegreen">"Greys_r"</span>)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1053674"></a><span class="fm-combinumeral">❶</span> We’ll display a grid of 30 × 30 digits (900 digits total).</p>

  <p class="fm-code-annotation"><a id="pgfId-1053695"></a><span class="fm-combinumeral">❷</span> Sample points linearly on a 2D grid.</p>

  <p class="fm-code-annotation"><a id="pgfId-1053712"></a><span class="fm-combinumeral">❸</span> Iterate over grid locations.</p>

  <p class="fm-code-annotation"><a id="pgfId-1053729"></a><span class="fm-combinumeral">❹</span> For each location, sample a digit and add it to our figure.</p>

  <p class="body"><a id="pgfId-1025108"></a>The grid of sampled digits (see figure 12.18) shows a completely continuous distribution of the different digit classes, with one digit morphing into another as you follow a path through latent space. Specific directions in this space have a meaning: for example, there are directions for “five-ness,” “one-ness,” and so on.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/12-18.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1063731"></a>Figure 12.18 Grid of digits decoded from the latent space</p>

  <p class="body"><a id="pgfId-1025124"></a>In the next section, we’ll cover in detail the other major tool for generating artificial images: generative adversarial networks (GANs). <a id="marker-1025140"></a><a id="marker-1025143"></a><a id="marker-1025145"></a></p>

  <h3 class="fm-head1" id="heading_id_23"><a id="pgfId-1025151"></a>12.4.5 Wrapping up</h3>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1025161"></a>Image generation with deep learning is done by learning latent spaces that capture statistical information about a dataset of images. By sampling and decoding points from the latent space, you can generate never-before-seen images. There are two major tools to do this: VAEs and GANs.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1025175"></a>VAEs result in highly structured, continuous latent representations. For this reason, they work well for doing all sorts of image editing in latent space: face swapping, turning a frowning face into a smiling face, and so on. They also work nicely for doing latent-space-based animations, such as animating a walk along a cross section of the latent space or showing a starting image slowly morphing into different images in a continuous way.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1025185"></a>GANs enable the generation of realistic single-frame images but may not induce latent spaces with solid structure and high continuity.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1025195"></a>Most successful practical applications I have seen with images rely on VAEs, but GANs have enjoyed enduring popularity in the world of academic research. You’ll find out how they work and how to implement one in the next section. <a id="marker-1025200"></a><a id="marker-1025202"></a></p>

  <h2 class="fm-head" id="heading_id_24"><a id="pgfId-1025208"></a>12.5 Introduction to generative adversarial networks</h2>

  <p class="body"><a id="pgfId-1025231"></a><a id="marker-1025221"></a><a id="marker-1025223"></a><a id="marker-1025225"></a><a id="marker-1025227"></a>Generative adversarial networks (GANs), introduced in 2014 by Goodfellow et al.,<a id="Id-1025233"></a><a href="#pgfId-1025233"><sup class="footnotenumber">7</sup></a> are an alternative to VAEs for learning latent spaces of images. They enable the generation of fairly realistic synthetic images by forcing the generated images to be statistically almost indistinguishable from real ones.</p>

  <p class="body"><a id="pgfId-1025250"></a>An intuitive way to understand GANs is to imagine a forger trying to create a fake Picasso painting. At first, the forger is pretty bad at the task. He mixes some of his fakes with authentic Picassos and shows them all to an art dealer. The art dealer makes an authenticity assessment for each painting and gives the forger feedback about what makes a Picasso look like a Picasso. The forger goes back to his studio to prepare some new fakes. As time goes on, the forger becomes increasingly competent at imitating the style of Picasso, and the art dealer becomes increasingly expert at spotting fakes. In the end, they have on their hands some excellent fake Picassos.</p>

  <p class="body"><a id="pgfId-1025256"></a>That’s what a GAN is: a forger network and an expert network, each being trained to best the other. As such, a GAN is made of two parts:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1025262"></a><i class="fm-italics1">Generator network</i>—Takes as <a class="calibre11" id="marker-1025279"></a>input a random vector (a random point in the latent space), and decodes it into a synthetic image</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1025289"></a><i class="fm-italics1">Discriminator network (or adversary)</i>—Takes as <a class="calibre11" id="marker-1025302"></a>input an image (real or synthetic), and predicts whether the image came from the training set or was created by the generator network</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1025312"></a>The generator network is trained to be able to fool the discriminator network, and thus it evolves toward generating increasingly realistic images as training goes on: artificial images that look indistinguishable from real ones, to the extent that it’s impossible for the discriminator network to tell the two apart (see figure 12.19). Meanwhile, the discriminator is constantly adapting to the gradually improving capabilities of the generator, setting a high bar of realism for the generated images. Once training is over, the generator is capable of turning any point in its input space into a believable image. Unlike VAEs, this latent space has fewer explicit guarantees of meaningful structure; in particular, it isn’t continuous.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/12-19.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1063773"></a>Figure 12.19 A generator transforms random latent vectors into images, and a discriminator seeks to tell real images from generated ones. The generator is trained to fool the discriminator.</p>

  <p class="body"><a id="pgfId-1025328"></a>Remarkably, a GAN is a system where the optimization minimum isn’t fixed, unlike in any other training setup you’ve encountered in this book. Normally, gradient descent consists of rolling down hills in a static loss landscape. But with a GAN, every step taken down the hill changes the entire landscape a little. It’s a dynamic system where the optimization process is seeking not a minimum, but an equilibrium between two forces. For this reason, GANs are notoriously difficult to train—getting a GAN to work requires lots of careful tuning of the model architecture and training parameters.</p>

  <h3 class="fm-head1" id="heading_id_25"><a id="pgfId-1025358"></a>12.5.1 A schematic GAN implementation</h3>

  <p class="body"><a id="pgfId-1025401"></a><a id="marker-1025383"></a><a id="marker-1025385"></a>In this section, we’ll explain how to implement a GAN in Keras in its barest form. GANs are advanced, so diving deeply into the technical details of architectures like that of the StyleGAN2 that generated the images in figure 12.20 would be out of scope for this book. The specific implementation we’ll use in this demonstration <a id="marker-1025390"></a>is a <i class="fm-italics">deep convolutional GAN</i> (DCGAN): a very basic GAN where the generator and discriminator are deep convnets.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/12-20.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1063818"></a>Figure 12.20 Latent space dwellers. Images generated by <span class="fm-hyperlink"><a class="url" href="https://thispersondoesnotexist.com">https://thispersondoesnotexist.com</a></span> using a StyleGAN2 model. (Image credit: Phillip Wang is the website author. The model used is the StyleGAN2 model from Karras et al., <span class="fm-hyperlink"><a class="url" href="https://arxiv.org/abs/1912.04958">https://arxiv.org/abs/1912.04958</a></span>.)</p>

  <p class="body"><a id="pgfId-1032128"></a>We’ll train our GAN on images from the Large-scale CelebFaces Attributes dataset (known as CelebA), a dataset of 200,000 faces of celebrities (<span class="fm-hyperlink"><a class="url" href="https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html</a></span>) To speed up training, we’ll resize the images to 64 × 64, so we’ll be learning to generate 64 × 64 images of human faces.</p>

  <p class="body"><a id="pgfId-1026246"></a>Schematically, the GAN looks like this:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1026292"></a>A <code class="fm-code-in-text">generator</code> network maps <a class="calibre11" id="marker-1026271"></a>vectors of shape <code class="fm-code-in-text">(latent_dim,)</code> to images of shape <code class="fm-code-in-text">(64,</code> <code class="fm-code-in-text">64,</code> <code class="fm-code-in-text">3)</code>.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1026321"></a>A <code class="fm-code-in-text">discriminator</code> network maps images of shape <code class="fm-code-in-text">(64,</code> <code class="fm-code-in-text">64,</code> <code class="fm-code-in-text">3)</code> to a binary score estimating the probability that the image is real.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1026360"></a>A <code class="fm-code-in-text">gan</code> network chains the generator and the discriminator together: <code class="fm-code-in-text">gan(x)</code> <code class="fm-code-in-text">=</code> <code class="fm-code-in-text">discriminator(generator(x))</code>. Thus, this <code class="fm-code-in-text">gan</code> network maps latent space vectors to the discriminator’s assessment of the realism of these latent vectors as decoded by the generator.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1026369"></a>We train the discriminator using examples of real and fake images along with “real”/“fake” labels, just as we train any regular image-classification model.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1026400"></a>To train the generator, we use the gradients of the generator’s weights with regard to the loss <a class="calibre11" id="marker-1026389"></a>of the <code class="fm-code-in-text">gan</code> model. This means that at every step, we move the weights of the generator in a direction that makes the discriminator more likely to classify as “real” the images decoded by the generator. In other words, we train the generator to fool the discriminator. <a class="calibre11" id="marker-1026405"></a><a class="calibre11" id="marker-1026408"></a></p>
    </li>
  </ul>

  <h3 class="fm-head1" id="heading_id_26"><a id="pgfId-1026414"></a>12.5.2 A bag of tricks</h3>

  <p class="body"><a id="pgfId-1026431"></a><a id="marker-1026425"></a><a id="marker-1026427"></a>The process of training GANs and tuning GAN implementations is notoriously difficult. There are a number of known tricks you should keep in mind. Like most things in deep learning, it’s more alchemy than science: these tricks are heuristics, not theory-backed guidelines. They’re supported by a level of intuitive understanding of the phenomenon at hand, and they’re known to work well empirically, although not necessarily in every context.</p>

  <p class="body"><a id="pgfId-1026436"></a>Here are a few of the tricks used in the implementation of the GAN generator and discriminator in this section. It isn’t an exhaustive list of GAN-related tips; you’ll find many more across the GAN literature:</p>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1026442"></a>We use strides instead of pooling for downsampling feature maps in the discriminator, just like we did in our VAE encoder.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1026456"></a>We sample points from the latent space using a <i class="fm-italics1">normal distribution</i> (Gaussian distribution), not a uniform distribution.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1026475"></a>Stochasticity is good for inducing robustness. Because GAN training results in a dynamic equilibrium, GANs are likely to get stuck in all sorts of ways. Introducing randomness during training helps prevent this. We introduce randomness by adding random noise to the labels for the discriminator.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1026538"></a>Sparse gradients can hinder GAN training. In deep learning, sparsity is often a desirable property, but not in GANs. Two things can induce gradient sparsity: max pooling operations <a class="calibre11" id="marker-1026491"></a>and <code class="fm-code-in-text">relu</code> activations. Instead of max pooling, we recommend using strided convolutions for downsampling, and we recommend using a <code class="fm-code-in-text">LeakyReLU</code> layer instead <a class="calibre11" id="marker-1026517"></a>of a <code class="fm-code-in-text">relu</code> activation. It’s similar to <code class="fm-code-in-text">relu</code>, but it relaxes sparsity constraints by allowing small negative activation values.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1026567"></a>In generated images, it’s common to see checkerboard artifacts caused by unequal coverage of the pixel space in the generator (see figure 12.21). To fix this, we use a kernel size that’s divisible by the stride size whenever we use a strided <code class="fm-code-in-text">Conv2DTranspose</code> or <code class="fm-code-in-text">Conv2D</code> in both the generator and the discriminator. <a class="calibre11" id="marker-1026572"></a><a class="calibre11" id="marker-1026575"></a></p>
    </li>
  </ul>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/12-21.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1063872"></a>Figure 12.21 Checkerboard artifacts caused by mismatching strides and kernel sizes, resulting in unequal pixel-space coverage: one of the many gotchas of GANs</p>

  <h3 class="fm-head1" id="heading_id_27"><a id="pgfId-1026591"></a>12.5.3 Getting our hands on the CelebA dataset</h3>

  <p class="body"><a id="pgfId-1032366"></a><a id="marker-1048082"></a><a id="marker-1048083"></a><a id="marker-1048084"></a>You can download the dataset manually from the website: <span class="fm-hyperlink"><a class="url" href="https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html</a></span>. If you’re using Colab, you can run the following to download the data from Google Drive and uncompress it.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1027000"></a>Listing 12.30 Getting the CelebA data</p>
  <pre class="programlisting"><a id="pgfId-1027065"></a>!mkdir celeba_gan                                                      <span class="fm-combinumeral">❶</span>
<a id="pgfId-1039465"></a>!gdown --id 1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684 -O celeba_gan/data.zip   <span class="fm-combinumeral">❷</span>
<a id="pgfId-1039472"></a>!unzip -qq celeba_gan/data.zip -d celeba_gan                           <span class="fm-combinumeral">❸</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1053441"></a><span class="fm-combinumeral">❶</span> Create a working directory.</p>

  <p class="fm-code-annotation"><a id="pgfId-1053462"></a><span class="fm-combinumeral">❷</span> Download the compressed data using gdown (available by default in Colab; install it otherwise).</p>

  <p class="fm-code-annotation"><a id="pgfId-1053479"></a><span class="fm-combinumeral">❸</span> Uncompress the data.</p>

  <p class="body"><a id="pgfId-1027142"></a>Once you’ve got the uncompressed images in a directory, you can use <code class="fm-code-in-text">image_dataset_from_directory</code> to turn it into a dataset. Since we just need the images—there are no labels—we’ll specify <code class="fm-code-in-text">label_mode=None</code>.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1027202"></a>Listing 12.31 Creating a dataset from a directory of images</p>
  <pre class="programlisting"><a id="pgfId-1027305"></a><b class="fm-codebrown">from</b> tensorflow <b class="fm-codebrown">import</b> keras
<a id="pgfId-1039479"></a>dataset = keras.utils_dataset_from_directory(
<a id="pgfId-1039486"></a>    <span class="fm-codegreen">"celeba_gan"</span>,
<a id="pgfId-1039493"></a>    label_mode=<code class="fm-codegreen">None</code>,      <span class="fm-combinumeral">❶</span>
<a id="pgfId-1039500"></a>    image_size=(<span class="fm-codeblue">64</span>, <span class="fm-codeblue">64</span>),
<a id="pgfId-1039507"></a>    batch_size=<span class="fm-codeblue">32</span>,
<a id="pgfId-1039514"></a>    smart_resize=<code class="fm-codegreen">True</code>)    <span class="fm-combinumeral">❷</span></pre>

  <p class="fm-code-annotation"><a id="pgfId-1053335"></a><span class="fm-combinumeral">❶</span> Only the images will be returned—no labels.</p>

  <p class="fm-code-annotation"><a id="pgfId-1053356"></a><span class="fm-combinumeral">❷</span> We will resize the images to 64 × 64 by using a smart combination of cropping and resizing to preserve aspect ratio. We don’t want face proportions to get distorted!</p>

  <p class="body"><a id="pgfId-1027347"></a>Finally, let’s rescale the images to the <code class="fm-code-in-text">[0-1]</code> range.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1027413"></a>Listing 12.32 Rescaling the images</p>
  <pre class="programlisting"><a id="pgfId-1027458"></a>dataset = dataset.map(<b class="fm-codebrown">lambda</b> x: x / <span class="fm-codeblue">255.</span>)</pre>

  <p class="body"><a id="pgfId-1027467"></a>You can use the following code to display a sample image. <a id="marker-1027469"></a><a id="marker-1027472"></a><a id="marker-1027474"></a></p>

  <p class="fm-code-listing-caption"><a id="pgfId-1027531"></a>Listing 12.33 Displaying the first image</p>
  <pre class="programlisting"><a id="pgfId-1027644"></a><b class="fm-codebrown">import</b> matplotlib.pyplot <b class="fm-codebrown">as</b> plt 
<a id="pgfId-1039521"></a><b class="fm-codebrown">for</b> x <b class="fm-codebrown">in</b> dataset:
<a id="pgfId-1039528"></a>    plt.axis(<span class="fm-codegreen">"off"</span>)
<a id="pgfId-1039535"></a>    plt.imshow((x.numpy() * <span class="fm-codeblue">255</span>).astype(<span class="fm-codegreen">"int32"</span>)[<span class="fm-codeblue">0</span>])
<a id="pgfId-1039542"></a>    <b class="fm-codebrown">break</b></pre>

  <h3 class="fm-head1" id="heading_id_28"><a id="pgfId-1027650"></a>12.5.4 The discriminator</h3>

  <p class="body"><a id="pgfId-1027675"></a><a id="marker-1027661"></a><a id="marker-1027663"></a><a id="marker-1027665"></a>First, we’ll develop a <code class="fm-code-in-text">discriminator</code> model that takes as input a candidate image (real or synthetic) and classifies it into one of two classes: “generated image” or “real image that comes from the training set.” One of the many issues that commonly arise with GANs is that the generator gets stuck with generated images that look like noise. A possible solution is to use dropout in the discriminator, so that’s what we will do here.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1027735"></a>Listing 12.34 The GAN discriminator network</p>
  <pre class="programlisting"><a id="pgfId-1028008"></a><b class="fm-codebrown">from</b> tensorflow.keras <b class="fm-codebrown">import</b> layers
<a id="pgfId-1039549"></a>  
<a id="pgfId-1039556"></a>discriminator = keras.Sequential(
<a id="pgfId-1039563"></a>    [
<a id="pgfId-1039570"></a>        keras.Input(shape=(<span class="fm-codeblue">64</span>, <span class="fm-codeblue">64</span>, <span class="fm-codeblue">3</span>)),
<a id="pgfId-1039577"></a>        layers.Conv2D(<span class="fm-codeblue">64</span>, kernel_size=<span class="fm-codeblue">4</span>, strides=<span class="fm-codeblue">2</span>, padding=<span class="fm-codegreen">"same"</span>),
<a id="pgfId-1039584"></a>        layers.LeakyReLU(alpha=<span class="fm-codeblue">0.2</span>),
<a id="pgfId-1039591"></a>        layers.Conv2D(<span class="fm-codeblue">128</span>, kernel_size=<span class="fm-codeblue">4</span>, strides=<span class="fm-codeblue">2</span>, padding=<span class="fm-codegreen">"same"</span>),
<a id="pgfId-1039598"></a>        layers.LeakyReLU(alpha=<span class="fm-codeblue">0.2</span>),
<a id="pgfId-1039605"></a>        layers.Conv2D(<span class="fm-codeblue">128</span>, kernel_size=<span class="fm-codeblue">4</span>, strides=<span class="fm-codeblue">2</span>, padding=<span class="fm-codegreen">"same"</span>),
<a id="pgfId-1039612"></a>        layers.LeakyReLU(alpha=<span class="fm-codeblue">0.2</span>),
<a id="pgfId-1039619"></a>        layers.Flatten(),
<a id="pgfId-1039626"></a>        layers.Dropout(<span class="fm-codeblue">0.2</span>),                   <span class="fm-combinumeral">❶</span>
<a id="pgfId-1039633"></a>        layers.Dense(<span class="fm-codeblue">1</span>, activation=<span class="fm-codegreen">"sigmoid"</span>),
<a id="pgfId-1039640"></a>    ],
<a id="pgfId-1039647"></a>    name=<span class="fm-codegreen">"discriminator"</span>,
<a id="pgfId-1039654"></a>)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1053267"></a><span class="fm-combinumeral">❶</span> One dropout layer: an important trick!</p>

  <p class="body"><a id="pgfId-1028037"></a>Here’s the discriminator model summary:<a id="marker-1028039"></a><a id="marker-1028042"></a><a id="marker-1028044"></a></p>
  <pre class="programlisting"><a id="pgfId-1028460"></a>&gt;&gt;&gt; discriminator.summary()
<a id="pgfId-1039661"></a>Model: "discriminator" 
<a id="pgfId-1039668"></a>_________________________________________________________________
<a id="pgfId-1039675"></a>Layer (type)                 Output Shape              Param # 
<a id="pgfId-1039682"></a>=================================================================
<a id="pgfId-1039689"></a>conv2d (Conv2D)              (None, 32, 32, 64)        3136 
<a id="pgfId-1039696"></a>_________________________________________________________________
<a id="pgfId-1039703"></a>leaky_re_lu (LeakyReLU)      (None, 32, 32, 64)        0 
<a id="pgfId-1039710"></a>_________________________________________________________________
<a id="pgfId-1039717"></a>conv2d_1 (Conv2D)            (None, 16, 16, 128)       131200 
<a id="pgfId-1039724"></a>_________________________________________________________________
<a id="pgfId-1039731"></a>leaky_re_lu_1 (LeakyReLU)    (None, 16, 16, 128)       0 
<a id="pgfId-1039738"></a>_________________________________________________________________
<a id="pgfId-1039745"></a>conv2d_2 (Conv2D)            (None, 8, 8, 128)         262272 
<a id="pgfId-1039752"></a>_________________________________________________________________
<a id="pgfId-1039759"></a>leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 128)         0 
<a id="pgfId-1039766"></a>_________________________________________________________________
<a id="pgfId-1039773"></a>flatten (Flatten)            (None, 8192)              0 
<a id="pgfId-1039780"></a>_________________________________________________________________
<a id="pgfId-1039787"></a>dropout (Dropout)            (None, 8192)              0 
<a id="pgfId-1039794"></a>_________________________________________________________________
<a id="pgfId-1039801"></a>dense (Dense)                (None, 1)                 8193 
<a id="pgfId-1039808"></a>=================================================================
<a id="pgfId-1039815"></a>Total params: 404,801 
<a id="pgfId-1039822"></a>Trainable params: 404,801 
<a id="pgfId-1039829"></a>Non-trainable params: 0 
<a id="pgfId-1039836"></a>_________________________________________________________________</pre>

  <h3 class="fm-head1" id="heading_id_29"><a id="pgfId-1028469"></a>12.5.5 The generator</h3>

  <p class="body"><a id="pgfId-1028494"></a><a id="marker-1028480"></a><a id="marker-1028482"></a><a id="marker-1028484"></a>Next, let’s develop a <code class="fm-code-in-text">generator</code> model that turns a vector (from the latent space—during training it will be sampled at random) into a candidate image.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1028554"></a>Listing 12.35 GAN generator network</p>
  <pre class="programlisting"><a id="pgfId-1028949"></a>latent_dim = <span class="fm-codeblue">128</span>                                                              <span class="fm-combinumeral">❶</span>
<a id="pgfId-1039843"></a>  
<a id="pgfId-1039850"></a>generator = keras.Sequential(
<a id="pgfId-1039857"></a>    [
<a id="pgfId-1039864"></a>        keras.Input(shape=(latent_dim,)),
<a id="pgfId-1039871"></a>        layers.Dense(<span class="fm-codeblue">8</span> * <span class="fm-codeblue">8</span> * <span class="fm-codeblue">128</span>),                                            <span class="fm-combinumeral">❷</span>
<a id="pgfId-1039878"></a>        layers.Reshape((<span class="fm-codeblue">8</span>, <span class="fm-codeblue">8</span>, <span class="fm-codeblue">128</span>)),                                          <span class="fm-combinumeral">❸</span>
<a id="pgfId-1039885"></a>        layers.Conv2DTranspose(<span class="fm-codeblue">128</span>, kernel_size=<span class="fm-codeblue">4</span>, strides=<span class="fm-codeblue">2</span>, padding=<span class="fm-codegreen">"same"</span>),<span class="fm-combinumeral">❹</span>
<a id="pgfId-1039892"></a>        layers.LeakyReLU(alpha=<span class="fm-codeblue">0.2</span>),                                          <span class="fm-combinumeral">❺</span>
<a id="pgfId-1039899"></a>        layers.Conv2DTranspose(<span class="fm-codeblue">256</span>, kernel_size=<span class="fm-codeblue">4</span>, strides=<span class="fm-codeblue">2</span>, padding=<span class="fm-codegreen">"same"</span>),<span class="fm-combinumeral">❹</span>
<a id="pgfId-1039906"></a>        layers.LeakyReLU(alpha=<span class="fm-codeblue">0.2</span>),                                          <span class="fm-combinumeral">❺</span>
<a id="pgfId-1039913"></a>        layers.Conv2DTranspose(<span class="fm-codeblue">512</span>, kernel_size=<span class="fm-codeblue">4</span>, strides=<span class="fm-codeblue">2</span>, padding=<span class="fm-codegreen">"same"</span>),<span class="fm-combinumeral">❹</span>
<a id="pgfId-1039920"></a>        layers.LeakyReLU(alpha=<span class="fm-codeblue">0.2</span>),                                          <span class="fm-combinumeral">❺</span>
<a id="pgfId-1039927"></a>        layers.Conv2D(<span class="fm-codeblue">3</span>, kernel_size=<span class="fm-codeblue">5</span>, padding=<span class="fm-codegreen">"same"</span>, activation=<span class="fm-codegreen">"sigmoid"</span>),<span class="fm-combinumeral">❻</span>
<a id="pgfId-1039934"></a>    ],
<a id="pgfId-1039941"></a>    name=<span class="fm-codegreen">"generator"</span>,
<a id="pgfId-1039948"></a>)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1052768"></a><span class="fm-combinumeral">❶</span> The latent space will be made of 128-dimensional vectors.</p>

  <p class="fm-code-annotation"><a id="pgfId-1052789"></a><span class="fm-combinumeral">❷</span> Produce the same number of coefficients we had at the level of the Flatten layer in the encoder.</p>

  <p class="fm-code-annotation"><a id="pgfId-1052813"></a><span class="fm-combinumeral">❸</span> Revert the Flatten layer of the encoder.</p>

  <p class="fm-code-annotation"><a id="pgfId-1052830"></a><span class="fm-combinumeral">❹</span> Revert the Conv2D layers of the encoder.</p>

  <p class="fm-code-annotation"><a id="pgfId-1052847"></a><span class="fm-combinumeral">❺</span> We use LeakyReLU as our activation.</p>

  <p class="fm-code-annotation"><a id="pgfId-1052864"></a><span class="fm-combinumeral">❻</span> The output ends up with shape (28, 28, 1).</p>

  <p class="body"><a id="pgfId-1029058"></a>This is the generator model summary:<a id="marker-1029060"></a><a id="marker-1029063"></a><a id="marker-1029065"></a></p>
  <pre class="programlisting"><a id="pgfId-1029571"></a>&gt;&gt;&gt; generator.summary()
<a id="pgfId-1039955"></a>Model: "generator" 
<a id="pgfId-1039962"></a>_________________________________________________________________
<a id="pgfId-1039969"></a>Layer (type)                 Output Shape              Param # 
<a id="pgfId-1039976"></a>=================================================================
<a id="pgfId-1039983"></a>dense_1 (Dense)              (None, 8192)              1056768 
<a id="pgfId-1039990"></a>_________________________________________________________________
<a id="pgfId-1039997"></a>reshape (Reshape)            (None, 8, 8, 128)         0 
<a id="pgfId-1040004"></a>_________________________________________________________________
<a id="pgfId-1040011"></a>conv2d_transpose (Conv2DTran (None, 16, 16, 128)       262272 
<a id="pgfId-1040018"></a>_________________________________________________________________
<a id="pgfId-1040025"></a>leaky_re_lu_3 (LeakyReLU)    (None, 16, 16, 128)       0 
<a id="pgfId-1040032"></a>_________________________________________________________________
<a id="pgfId-1040039"></a>conv2d_transpose_1 (Conv2DTr (None, 32, 32, 256)       524544 
<a id="pgfId-1040046"></a>_________________________________________________________________
<a id="pgfId-1040053"></a>leaky_re_lu_4 (LeakyReLU)    (None, 32, 32, 256)       0 
<a id="pgfId-1040060"></a>_________________________________________________________________
<a id="pgfId-1040067"></a>conv2d_transpose_2 (Conv2DTr (None, 64, 64, 512)       2097664 
<a id="pgfId-1040074"></a>_________________________________________________________________
<a id="pgfId-1040081"></a>leaky_re_lu_5 (LeakyReLU)    (None, 64, 64, 512)       0 
<a id="pgfId-1040088"></a>_________________________________________________________________
<a id="pgfId-1040095"></a>conv2d_3 (Conv2D)            (None, 64, 64, 3)         38403 
<a id="pgfId-1040102"></a>=================================================================
<a id="pgfId-1040109"></a>Total params: 3,979,651 
<a id="pgfId-1040116"></a>Trainable params: 3,979,651 
<a id="pgfId-1040123"></a>Non-trainable params: 0 
<a id="pgfId-1040130"></a>_________________________________________________________________</pre>

  <h3 class="fm-head1" id="heading_id_30"><a id="pgfId-1029580"></a>12.5.6 The adversarial network</h3>

  <p class="body"><a id="pgfId-1029623"></a><a id="marker-1029591"></a><a id="marker-1029593"></a>Finally, we’ll set up the GAN, which chains the generator and the discriminator. When trained, this model will move the generator in a direction that improves its ability to fool the discriminator. This model turns latent-space points into a classification decision—“fake” or “real”—and it’s meant to be trained with labels that are always “these are real images.” So training <code class="fm-code-in-text">gan</code> will update the weights of <code class="fm-code-in-text">generator</code> in a way that makes <code class="fm-code-in-text">discriminator</code> more likely to predict “real” when looking at fake images.</p>

  <p class="body"><a id="pgfId-1029632"></a>To recapitulate, this is what the training loop looks like schematically. For each epoch, you do the following:</p>

  <ol class="calibre14">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1029638"></a>Draw random points in the latent space (random noise).</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1029652"></a>Generate images with <code class="fm-code-in-text">generator</code> using this random noise.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1029671"></a>Mix the generated images with real ones.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1029681"></a>Train <code class="fm-code-in-text">discriminator</code> using these mixed images, with corresponding targets: either “real” (for the real images) or “fake” (for the generated images).</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1029700"></a>Draw new random points in the latent space.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1029710"></a>Train <code class="fm-code-in-text">generator</code> using these random vectors, with targets that all say “these are real images.” This updates the weights of the generator to move them toward getting the discriminator to predict “these are real images” for generated images: this trains the generator to fool the discriminator.</p>
    </li>
  </ol>

  <p class="body"><a id="pgfId-1029761"></a>Let’s implement it. Like in our VAE example, we’ll use a <code class="fm-code-in-text">Model</code> subclass with <a id="marker-1029740"></a>a custom <code class="fm-code-in-text">train_step()</code>. Note that we’ll use two optimizers (one for the generator and one for the discriminator), so we will also override <code class="fm-code-in-text">compile()</code> to allow <a id="marker-1029766"></a>for passing two optimizers.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1029827"></a>Listing 12.36 The GAN <code class="fm-code-in-text">Model</code></p>
  <pre class="programlisting"><a id="pgfId-1030378"></a><b class="fm-codebrown">import</b> tensorflow <b class="fm-codebrown">as</b> tf
<a id="pgfId-1040137"></a><b class="fm-codebrown">class</b> GAN(keras.Model):
<a id="pgfId-1040144"></a>    <b class="fm-codebrown">def</b> __init__(self, discriminator, generator, latent_dim):
<a id="pgfId-1040151"></a>        super().__init__()
<a id="pgfId-1040158"></a>        self.discriminator = discriminator
<a id="pgfId-1040165"></a>        self.generator = generator
<a id="pgfId-1040172"></a>        self.latent_dim = latent_dim
<a id="pgfId-1040179"></a>        self.d_loss_metric = keras.metrics.Mean(name=<span class="fm-codegreen">"d_loss"</span>)              <span class="fm-combinumeral">❶</span>
<a id="pgfId-1040186"></a>        self.g_loss_metric = keras.metrics.Mean(name=<span class="fm-codegreen">"g_loss"</span>)              <span class="fm-combinumeral">❶</span>
<a id="pgfId-1040200"></a> 
<a id="pgfId-1040193"></a>    <b class="fm-codebrown">def</b> compile(self, d_optimizer, g_optimizer, loss_fn):
<a id="pgfId-1040207"></a>        super(GAN, self).compile()
<a id="pgfId-1040214"></a>        self.d_optimizer = d_optimizer
<a id="pgfId-1040221"></a>        self.g_optimizer = g_optimizer
<a id="pgfId-1040228"></a>        self.loss_fn = loss_fn
<a id="pgfId-1040235"></a>  
<a id="pgfId-1040242"></a>    <code class="fm-codeblue">@property</code>
<a id="pgfId-1050990"></a>    <b class="fm-codebrown">def</b> metrics(self):                                                      <span class="fm-combinumeral">❶</span>
<a id="pgfId-1040249"></a>        <b class="fm-codebrown">return</b> [self.d_loss_metric, self.g_loss_metric]
<a id="pgfId-1040263"></a>    <b class="fm-codebrown">def</b> train_step(self, real_images):
<a id="pgfId-1040270"></a>        batch_size = tf.shape(real_images)[<span class="fm-codeblue">0</span>]                               <span class="fm-combinumeral">❷</span>
<a id="pgfId-1040277"></a>        random_latent_vectors = tf.random.normal(                           <span class="fm-combinumeral">❷</span>
<a id="pgfId-1040284"></a>            shape=(batch_size, self.latent_dim))                            <span class="fm-combinumeral">❷</span>
<a id="pgfId-1040291"></a>        generated_images = self.generator(random_latent_vectors)            <span class="fm-combinumeral">❸</span>
<a id="pgfId-1040298"></a>        combined_images = tf.concat([generated_images, real_images], axis=<span class="fm-codeblue">0</span>)<span class="fm-combinumeral">❹</span>
<a id="pgfId-1040305"></a>        labels = tf.concat(                                                 <span class="fm-combinumeral">❺</span>
<a id="pgfId-1040312"></a>            [tf.ones((batch_size, <span class="fm-codeblue">1</span>)), tf.zeros((batch_size, <span class="fm-codeblue">1</span>))],          <span class="fm-combinumeral">❺</span>
<a id="pgfId-1040319"></a>            axis=<span class="fm-codeblue">0</span>                                                          <span class="fm-combinumeral">❺</span>
<a id="pgfId-1040326"></a>        )
<a id="pgfId-1040333"></a>        labels += <span class="fm-codeblue">0.05</span> * tf.random.uniform(tf.shape(labels))                <span class="fm-combinumeral">❻</span>
<a id="pgfId-1040347"></a> 
<a id="pgfId-1040340"></a>        <b class="fm-codebrown">with</b> tf.GradientTape() <b class="fm-codebrown">as</b> tape:                                     <span class="fm-combinumeral">❼</span>
<a id="pgfId-1040354"></a>            predictions = self.discriminator(combined_images)               <span class="fm-combinumeral">❼</span>
<a id="pgfId-1040361"></a>            d_loss = self.loss_fn(labels, predictions)                      <span class="fm-combinumeral">❼</span>
<a id="pgfId-1040368"></a>        grads = tape.gradient(d_loss, self.discriminator.trainable_weights) <span class="fm-combinumeral">❼</span>
<a id="pgfId-1040375"></a>        self.d_optimizer.apply_gradients(                                   <span class="fm-combinumeral">❼</span>
<a id="pgfId-1040382"></a>            zip(grads, self.discriminator.trainable_weights)                <span class="fm-combinumeral">❼</span>
<a id="pgfId-1040389"></a>        )
<a id="pgfId-1040396"></a>  
<a id="pgfId-1040403"></a>        random_latent_vectors = tf.random.normal(
<a id="pgfId-1040410"></a>            shape=(batch_size, self.latent_dim))                            <span class="fm-combinumeral">❽</span>
<a id="pgfId-1040417"></a>  
<a id="pgfId-1040424"></a>        misleading_labels = tf.zeros((batch_size, <span class="fm-codeblue">1</span>))                       <span class="fm-combinumeral">❾</span>
<a id="pgfId-1040431"></a>  
<a id="pgfId-1040438"></a>        <b class="fm-codebrown">with</b> tf.GradientTape() <b class="fm-codebrown">as</b> tape:                                     <span class="fm-combinumeral">❿</span>
<a id="pgfId-1040445"></a>            predictions = self.discriminator(                               <span class="fm-combinumeral">❿</span>
<a id="pgfId-1040452"></a>                self.generator(random_latent_vectors))                      <span class="fm-combinumeral">❿</span>
<a id="pgfId-1040459"></a>            g_loss = self.loss_fn(misleading_labels, predictions)           <span class="fm-combinumeral">❿</span>
<a id="pgfId-1040466"></a>        grads = tape.gradient(g_loss, self.generator.trainable_weights)     <span class="fm-combinumeral">❿</span>
<a id="pgfId-1040473"></a>        self.g_optimizer.apply_gradients(                                   <span class="fm-combinumeral">❿</span>
<a id="pgfId-1040480"></a>            zip(grads, self.generator.trainable_weights))                   <span class="fm-combinumeral">❿</span>
<a id="pgfId-1040487"></a>  
<a id="pgfId-1040494"></a>        self.d_loss_metric.update_state(d_loss)
<a id="pgfId-1040501"></a>        self.g_loss_metric.update_state(g_loss)
<a id="pgfId-1040508"></a>        <b class="fm-codebrown">return</b> {<span class="fm-codegreen">"d_loss"</span>: self.d_loss_metric.result(),
<a id="pgfId-1040515"></a>                <span class="fm-codegreen">"g_loss"</span>: self.g_loss_metric.result()}</pre>

  <p class="fm-code-annotation"><a id="pgfId-1051710"></a><span class="fm-combinumeral">❶</span> Sets up metrics to track the two losses over each training epoch</p>

  <p class="fm-code-annotation"><a id="pgfId-1051731"></a><span class="fm-combinumeral">❷</span> Samples random points in the latent space</p>

  <p class="fm-code-annotation"><a id="pgfId-1051748"></a><span class="fm-combinumeral">❸</span> Decodes them to fake images</p>

  <p class="fm-code-annotation"><a id="pgfId-1051765"></a><span class="fm-combinumeral">❹</span> Combines them with real images</p>

  <p class="fm-code-annotation"><a id="pgfId-1051782"></a><span class="fm-combinumeral">❺</span> Assembles labels, discriminating real from fake images</p>

  <p class="fm-code-annotation"><a id="pgfId-1051799"></a><span class="fm-combinumeral">❻</span> Adds random noise to the labels—an important trick!</p>

  <p class="fm-code-annotation"><a id="pgfId-1051816"></a><span class="fm-combinumeral">❼</span> Trains the discriminator</p>

  <p class="fm-code-annotation"><a id="pgfId-1051833"></a><span class="fm-combinumeral">❽</span> Samples random points in the latent space</p>

  <p class="fm-code-annotation"><a id="pgfId-1051850"></a><span class="fm-combinumeral">❾</span> Assembles labels that say “these are all real images” (it’s a lie!)</p>

  <p class="fm-code-annotation"><a id="pgfId-1051867"></a><span class="fm-combinumeral">❿</span> Trains the generator</p>

  <p class="body"><a id="pgfId-1030551"></a>Before we start training, let’s also set up a callback to monitor our results: it will use the generator to create and save a number of fake images at the end of each epoch.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1030608"></a>Listing 12.37 A callback that samples generated images during training</p>
  <pre class="programlisting"><a id="pgfId-1030721"></a><b class="fm-codebrown">class</b> GANMonitor(keras.callbacks.Callback):
<a id="pgfId-1040522"></a>    <b class="fm-codebrown">def</b> __init__(self, num_img=<span class="fm-codeblue">3</span>, latent_dim=<span class="fm-codeblue">128</span>):
<a id="pgfId-1040529"></a>        self.num_img = num_img
<a id="pgfId-1040536"></a>        self.latent_dim = latent_dim
<a id="pgfId-1040543"></a>  
<a id="pgfId-1040550"></a>    <b class="fm-codebrown">def</b> on_epoch_end(self, epoch, logs=<code class="fm-codegreen">None</code>):
<a id="pgfId-1040557"></a>        random_latent_vectors = tf.random.normal(
<a id="pgfId-1051170"></a>            shape=(self.num_img, self.latent_dim))
<a id="pgfId-1040564"></a>        generated_images = self.model.generator(random_latent_vectors)
<a id="pgfId-1040571"></a>        generated_images *= <span class="fm-codeblue">255</span> 
<a id="pgfId-1040578"></a>        generated_images.numpy()
<a id="pgfId-1040585"></a>        <b class="fm-codebrown">for</b> i <b class="fm-codebrown">in</b> range(self.num_img):
<a id="pgfId-1040592"></a>            img = keras.utils.array_to_img(generated_images[i])
<a id="pgfId-1040599"></a>            img.save(f<span class="fm-codegreen">"generated_img_{epoch:03d}_{i}.png"</span>)</pre>

  <p class="body"><a id="pgfId-1030730"></a>Finally, we can start training.</p>

  <p class="fm-code-listing-caption"><a id="pgfId-1030787"></a>Listing 12.38 Compiling and training the GAN</p>
  <pre class="programlisting"><a id="pgfId-1030862"></a>epochs = <span class="fm-codeblue">100</span>       <span class="fm-combinumeral">❶</span>
<a id="pgfId-1040606"></a>  
<a id="pgfId-1040613"></a>gan = GAN(discriminator=discriminator, generator=generator,
<a id="pgfId-1051187"></a>          latent_dim=latent_dim)
<a id="pgfId-1040620"></a>gan.compile(
<a id="pgfId-1040627"></a>    d_optimizer=keras.optimizers.Adam(learning_rate=<span class="fm-codeblue">0.0001</span>),
<a id="pgfId-1040634"></a>    g_optimizer=keras.optimizers.Adam(learning_rate=<span class="fm-codeblue">0.0001</span>),
<a id="pgfId-1040641"></a>    loss_fn=keras.losses.BinaryCrossentropy(),
<a id="pgfId-1040648"></a>)
<a id="pgfId-1040655"></a>  
<a id="pgfId-1040662"></a>gan.fit(
<a id="pgfId-1040669"></a>    dataset, epochs=epochs,
<a id="pgfId-1051221"></a>    callbacks=[GANMonitor(num_img=<span class="fm-codeblue">10</span>, latent_dim=latent_dim)]
<a id="pgfId-1040676"></a>)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1051637"></a><span class="fm-combinumeral">❶</span> You’ll start getting interesting results after epoch 20.</p>

  <p class="body"><a id="pgfId-1030891"></a>When training, you may see the adversarial loss begin to increase considerably, while the discriminative loss tends to zero—the discriminator may end up dominating the generator. If that’s the case, try reducing the discriminator learning rate, and increase the dropout rate of the discriminator.</p>

  <p class="body"><a id="pgfId-1030897"></a>Figure 12.22 shows what our GAN is capable of generating after 30 epochs of training.</p>

  <p class="fm-figure"><img alt="" class="calibre13" src="../Images/12-22.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1063914"></a> <a id="marker-1030899"></a><a id="marker-1030902"></a>Figure 12.22 Some generated images around epoch 30</p>

  <h3 class="fm-head1" id="heading_id_31"><a id="pgfId-1030918"></a>12.5.7 Wrapping up</h3>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1030942"></a>A GAN consists of a generator network coupled with a discriminator network. The discriminator is trained to differentiate between the output of the generator and real images from a training dataset, and the generator is trained to fool the discriminator. Remarkably, the generator never sees images from the training set directly; the information it has about the data comes from the discriminator.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1030956"></a>GANs are difficult to train, because training a GAN is a dynamic process rather than a simple gradient descent process with a fixed loss landscape. Getting a GAN to train correctly requires using a number of heuristic tricks, as well as extensive tuning.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1030966"></a>GANs can potentially produce highly realistic images. But unlike VAEs, the latent space they learn doesn’t have a neat continuous structure and thus may not be suited for certain practical applications, such as image editing via latent-space concept vectors.</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1030976"></a>These few techniques cover only the basics of this fast-expanding field. There’s a lot more to discover out there—generative deep learning is deserving of an entire book of its own. <a id="marker-1030981"></a><a id="marker-1030983"></a></p>

  <h2 class="fm-head" id="heading_id_32"><a id="pgfId-1030989"></a>Summary</h2>

  <ul class="calibre10">
    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1030999"></a>You can use a sequence-to-sequence model to generate sequence data, one step at a time. This is applicable to text generation, but also to note-by-note music generation or any other type of timeseries data.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1031013"></a>DeepDream works by maximizing convnet layer activations through gradient ascent in input space.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1031023"></a>In the style-transfer algorithm, a content image and a style image are combined together via gradient descent to produce an image with the high-level features of the content image and the local characteristics of the style image.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><a class="calibre11" id="pgfId-1031033"></a>VAEs and GANs are models that learn a latent space of images and can then dream up entirely new images by sampling from the latent space. <i class="fm-italics1">Concept vectors</i> in the latent space can even be used for image editing.</p>
    </li>
  </ul>
  <hr class="calibre15"/>

  <p class="fm-footnote"><a href="#Id-1011892"><sup class="footnotenumber1">1</sup></a> <a id="pgfId-1011892"></a>Iannis Xenakis, “Musiques formelles: nouveaux principes formels de composition musicale,” special issue of <i class="fm-italics">La Revue musicale</i>, nos. 253–254 (1963).</p>

  <p class="fm-footnote"><a href="#Id-1011991"><sup class="footnotenumber1">2</sup></a> <a id="pgfId-1011991"></a>Alex Graves, “Generating Sequences With Recurrent Neural Networks,” arXiv (2013), <span class="fm-hyperlink"><a class="url" href="https://arxiv.org/abs/1308.0850">https://arxiv.org/abs/1308.0850</a></span>.</p>

  <p class="fm-footnote"><a href="#Id-1015093"><sup class="footnotenumber1">3</sup></a> <a id="pgfId-1015093"></a>Alexander Mordvintsev, Christopher Olah, and Mike Tyka, “DeepDream: A Code Example for Visualizing Neural Networks,” Google Research Blog, July 1, 2015, <span class="fm-hyperlink"><a class="url" href="https://mng.bz/xXlM">http://mng.bz/xXlM</a></span>.</p>

  <p class="fm-footnote"><a href="#Id-1018003"><sup class="footnotenumber1">4</sup></a> <a id="pgfId-1018003"></a>Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge, “A Neural Algorithm of Artistic Style,” arXiv (2015), <span class="fm-hyperlink"><a class="url" href="https://arxiv.org/abs/1508.06576">https://arxiv.org/abs/1508.06576</a></span>.</p>

  <p class="fm-footnote"><a href="#Id-1021369"><sup class="footnotenumber1">5</sup></a> <a id="pgfId-1021369"></a>Diederik P. Kingma and Max Welling, “Auto-Encoding Variational Bayes,” arXiv (2013), <span class="fm-hyperlink"><a class="url" href="https://arxiv.org/abs/131">https://arxiv.org/abs/1312.6114</a></span>.</p>

  <p class="fm-footnote"><a href="#Id-1021383"><sup class="footnotenumber1">6</sup></a> <a id="pgfId-1021383"></a>Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra, “Stochastic Backpropagation and Approximate Inference in Deep Generative Models,” arXiv (2014), <span class="fm-hyperlink"><a class="url" href="https://arxiv.org/abs/1401.4082">https://arxiv.org/abs/1401.4082</a></span>.</p>

  <p class="fm-footnote"><a href="#Id-1025233"><sup class="footnotenumber1">7</sup></a> <a id="pgfId-1025233"></a>Ian Goodfellow et al., “Generative Adversarial Networks,” arXiv (2014), <span class="fm-hyperlink"><a class="url" href="https://arxiv.org/abs/1406.2661">https://arxiv.org/abs/1406.2661</a></span>.</p>
</body>
</html>
